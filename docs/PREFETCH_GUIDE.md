# æ•°æ®é›†é¢„è¯»å–åŠŸèƒ½ä½¿ç”¨æŒ‡å—

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

é¢„è¯»å–ï¼ˆPrefetchingï¼‰åŠŸèƒ½ä½¿ç”¨**å•ç‹¬çš„çº¿ç¨‹**æå‰åŠ è½½æ•°æ®ï¼Œå½“æ¨¡å‹åœ¨å¤„ç†å½“å‰batchæ—¶ï¼Œé¢„è¯»å–çº¿ç¨‹å·²ç»åœ¨åå°åŠ è½½ä¸‹ä¸€ä¸ªbatchçš„æ•°æ®ï¼Œä»è€Œæ˜¾è‘—å‡å°‘è®­ç»ƒç­‰å¾…æ—¶é—´ã€‚

### æ ¸å¿ƒä¼˜åŠ¿

- âœ… **å¹¶è¡ŒåŠ è½½**: æ•°æ®åŠ è½½ä¸æ¨¡å‹è®­ç»ƒå¹¶è¡Œè¿›è¡Œ
- âœ… **å‡å°‘ç­‰å¾…**: è®­ç»ƒæ—¶æ•°æ®å·²ç»å‡†å¤‡å¥½
- âœ… **ç®€å•æ˜“ç”¨**: ä¸€ä¸ªå‚æ•°å³å¯å¯ç”¨
- âœ… **çµæ´»é…ç½®**: å¯è°ƒèŠ‚ç¼“å†²åŒºå¤§å°
- âœ… **çº¿ç¨‹å®‰å…¨**: ä½¿ç”¨ç‹¬ç«‹çº¿ç¨‹ï¼Œä¸å½±å“ä¸»è®­ç»ƒæµç¨‹

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1: åœ¨DataLoaderä¸­å¯ç”¨ï¼ˆæ¨èï¼‰

```python
from pathlib import Path
from src.dataset.mnist_dataset import MNISTDataset

# åˆ›å»ºæ•°æ®é›†
dataset = MNISTDataset(
    root_dir=Path("data/mnist"),
    split='train'
)

# åˆ›å»ºDataLoaderæ—¶å¯ç”¨é¢„è¯»å–
dataloader = dataset.dataloader(
    batch_size=32,
    shuffle=True,
    enable_prefetch=True,        # âœ¨ å¯ç”¨é¢„è¯»å–
    prefetch_buffer_size=4       # é¢„è¯»å–ç¼“å†²åŒºå¤§å°
)

# æ­£å¸¸è®­ç»ƒ
for batch in dataloader:
    # è®­ç»ƒä»£ç ...
    pass
```

**å°±è¿™ä¹ˆç®€å•ï¼** é¢„è¯»å–çº¿ç¨‹ä¼šè‡ªåŠ¨åœ¨åå°å·¥ä½œã€‚

---

## ğŸ“– è¯¦ç»†è¯´æ˜

### å·¥ä½œåŸç†

```
ä¸»çº¿ç¨‹ï¼ˆè®­ç»ƒï¼‰          é¢„è¯»å–çº¿ç¨‹
    â”‚                      â”‚
    â”œâ”€ å¤„ç†batch 0         â”œâ”€ åŠ è½½batch 1 â†’ ç¼“å†²åŒº
    â”‚                      â”‚
    â”œâ”€ å¤„ç†batch 1 â†â”€â”€â”€â”€â”€â”€â”€â”¤ (ä»ç¼“å†²åŒºå–)
    â”‚                      â”‚
    â”‚                      â”œâ”€ åŠ è½½batch 2 â†’ ç¼“å†²åŒº
    â”œâ”€ å¤„ç†batch 2 â†â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                      â”‚
    â””â”€ ...                 â””â”€ åŠ è½½batch 3 â†’ ç¼“å†²åŒº
```

### ä¸¤ç§é¢„è¯»å–æ¨¡å¼

#### 1. é€šç”¨æ¨¡å¼ï¼ˆGeneral Modeï¼‰

- é€‚ç”¨äº **shuffle=True** çš„åœºæ™¯
- ä½¿ç”¨é˜Ÿåˆ—æœºåˆ¶
- æ”¯æŒéšæœºè®¿é—®

```python
dataloader = dataset.dataloader(
    batch_size=32,
    shuffle=True,           # å¯ç”¨shuffle
    enable_prefetch=True
)
```

#### 2. é¡ºåºæ¨¡å¼ï¼ˆSequential Modeï¼‰

- é€‚ç”¨äº **shuffle=False** çš„åœºæ™¯
- ä½¿ç”¨å­—å…¸ç¼“å†²
- æ€§èƒ½æ›´ä¼˜

```python
dataloader = dataset.dataloader(
    batch_size=32,
    shuffle=False,          # ä¸shuffle
    enable_prefetch=True
)
```

ç³»ç»Ÿä¼š**è‡ªåŠ¨é€‰æ‹©**åˆé€‚çš„æ¨¡å¼ï¼

---

## âš™ï¸ é…ç½®å‚æ•°

### enable_prefetch

- **ç±»å‹**: bool
- **é»˜è®¤**: False
- **è¯´æ˜**: æ˜¯å¦å¯ç”¨é¢„è¯»å–

```python
dataloader = dataset.dataloader(
    batch_size=32,
    enable_prefetch=True  # å¯ç”¨é¢„è¯»å–
)
```

### prefetch_buffer_size

- **ç±»å‹**: int
- **é»˜è®¤**: 2
- **å»ºè®®**: 2-8
- **è¯´æ˜**: é¢„è¯»å–ç¼“å†²åŒºå¤§å°ï¼ˆæå‰åŠ è½½å¤šå°‘ä¸ªæ ·æœ¬ï¼‰

```python
dataloader = dataset.dataloader(
    batch_size=32,
    enable_prefetch=True,
    prefetch_buffer_size=4  # æå‰åŠ è½½4ä¸ªæ ·æœ¬
)
```

**ç¼“å†²åŒºå¤§å°é€‰æ‹©**:
- **å¤ªå°**ï¼ˆ1-2ï¼‰: æ•ˆæœæœ‰é™
- **é€‚ä¸­**ï¼ˆ4-6ï¼‰: å¹³è¡¡æ€§èƒ½å’Œå†…å­˜
- **å¤ªå¤§**ï¼ˆ>8ï¼‰: å ç”¨å†…å­˜å¤šï¼Œæ”¶ç›Šé€’å‡

---

## ğŸ“ ä½¿ç”¨åœºæ™¯

### âœ… é€‚åˆä½¿ç”¨é¢„è¯»å–

1. **æ•°æ®åŠ è½½æ˜¯ç“¶é¢ˆ**
   - IOå¯†é›†å‹ï¼šä»ç£ç›˜/ç½‘ç»œåŠ è½½
   - CPUå¯†é›†å‹ï¼šå¤æ‚çš„æ•°æ®é¢„å¤„ç†

2. **è®­ç»ƒé€Ÿåº¦è¾ƒå¿«**
   - æ¨¡å‹è¾ƒå°
   - batch sizeè¾ƒå°
   - GPUåˆ©ç”¨ç‡é«˜

3. **å†…å­˜å……è¶³**
   - é¢„è¯»å–ä¼šå ç”¨é¢å¤–å†…å­˜

### âŒ ä¸é€‚åˆä½¿ç”¨é¢„è¯»å–

1. **æ¨¡å‹è®­ç»ƒå¾ˆæ…¢**
   - æ•°æ®åŠ è½½å·²ç»å¾ˆå¿«
   - æ¨¡å‹è®¡ç®—æ—¶é—´è¿œå¤§äºæ•°æ®åŠ è½½

2. **å†…å­˜å—é™**
   - é¢„è¯»å–ç¼“å†²ä¼šå ç”¨å†…å­˜

3. **å·²ä½¿ç”¨å¤šè¿›ç¨‹**
   - `num_workers > 0` æ—¶æ•ˆæœå åŠ ä¸æ˜æ˜¾

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### å…¸å‹æ€§èƒ½æå‡

| åœºæ™¯ | ä¸ä½¿ç”¨é¢„è¯»å– | ä½¿ç”¨é¢„è¯»å– | æå‡ |
|------|-------------|-----------|------|
| IOå¯†é›† | 100% | 70-80% | **20-30%** |
| CPUå¯†é›†é¢„å¤„ç† | 100% | 75-85% | **15-25%** |
| ç®€å•æ•°æ®é›† | 100% | 95-98% | **2-5%** |

**æ³¨æ„**: å®é™…æå‡å–å†³äºå…·ä½“åœºæ™¯ã€‚

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### æ‰‹åŠ¨ä½¿ç”¨é¢„è¯»å–åŒ…è£…å™¨

```python
from src.dataset.prefetch_wrapper import PrefetchDataset, SequentialPrefetchDataset

# æ–¹å¼1: é€šç”¨é¢„è¯»å–
dataset = MNISTDataset(root_dir=Path("data/mnist"), split='train')
prefetch_dataset = PrefetchDataset(
    dataset,
    buffer_size=4,
    enable_prefetch=True
)

# æ–¹å¼2: é¡ºåºé¢„è¯»å–ï¼ˆshuffle=Falseæ—¶æ€§èƒ½æ›´å¥½ï¼‰
seq_prefetch_dataset = SequentialPrefetchDataset(
    dataset,
    buffer_size=4,
    enable_prefetch=True
)

# ä½¿ç”¨é¢„è¯»å–æ•°æ®é›†
for i in range(len(prefetch_dataset)):
    sample = prefetch_dataset[i]
    # å¤„ç†æ•°æ®...
```

### ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
from src.dataset.prefetch_wrapper import PrefetchDataset

with PrefetchDataset(dataset, buffer_size=4) as prefetch_ds:
    for i in range(100):
        sample = prefetch_ds[i]
        # å¤„ç†...
# é€€å‡ºæ—¶è‡ªåŠ¨åœæ­¢é¢„è¯»å–çº¿ç¨‹
```

### æ‰‹åŠ¨æ§åˆ¶é¢„è¯»å–çº¿ç¨‹

```python
prefetch_dataset = PrefetchDataset(dataset, buffer_size=4)

# ä½¿ç”¨æ•°æ®...
for sample in prefetch_dataset:
    pass

# æ‰‹åŠ¨åœæ­¢é¢„è¯»å–çº¿ç¨‹
prefetch_dataset.stop_prefetch()
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. ä¸ç¼“å­˜åŠŸèƒ½é…åˆ

```python
# å…ˆå¯ç”¨ç¼“å­˜åŠ å¿«æ•°æ®åŠ è½½
dataset = MNISTDataset(
    root_dir=Path("data/mnist"),
    split='train',
    enable_cache=True  # ç¼“å­˜åŠ å¿«åŠ è½½é€Ÿåº¦
)

# å†å¯ç”¨é¢„è¯»å–è¿›ä¸€æ­¥ä¼˜åŒ–
dataloader = dataset.dataloader(
    batch_size=32,
    enable_prefetch=True,  # é¢„è¯»å–å‡å°‘ç­‰å¾…
    prefetch_buffer_size=4
)

# åŒé‡ä¼˜åŒ–ï¼
```

### 2. ä¸å¤šè¿›ç¨‹DataLoaderé…åˆ

```python
dataloader = dataset.dataloader(
    batch_size=32,
    num_workers=2,           # å¤šè¿›ç¨‹åŠ è½½
    enable_prefetch=True,    # é¢„è¯»å–
    prefetch_buffer_size=4
)

# æ³¨æ„: num_workerså·²ç»å¾ˆé«˜æ—¶ï¼Œé¢„è¯»å–æ”¶ç›Šå¯èƒ½è¾ƒå°
```

### 3. æ ¹æ®åœºæ™¯è°ƒæ•´ç¼“å†²åŒº

```python
# åœºæ™¯1: æ•°æ®åŠ è½½å¾ˆæ…¢ï¼ˆIOå¯†é›†ï¼‰
dataloader = dataset.dataloader(
    batch_size=32,
    enable_prefetch=True,
    prefetch_buffer_size=8  # è¾ƒå¤§çš„ç¼“å†²åŒº
)

# åœºæ™¯2: æ•°æ®åŠ è½½è¾ƒå¿«
dataloader = dataset.dataloader(
    batch_size=32,
    enable_prefetch=True,
    prefetch_buffer_size=2  # è¾ƒå°çš„ç¼“å†²åŒºå³å¯
)
```

### 4. å†…å­˜ä¼˜åŒ–

```python
# å¦‚æœå†…å­˜ç´§å¼ 
dataloader = dataset.dataloader(
    batch_size=32,
    enable_prefetch=True,
    prefetch_buffer_size=2,  # å‡å°ç¼“å†²åŒº
    pin_memory=False         # ç¦ç”¨pin_memoryèŠ‚çœå†…å­˜
)
```

---

## ğŸ¯ å®Œæ•´è®­ç»ƒç¤ºä¾‹

```python
import torch
from pathlib import Path
from src.dataset.mnist_dataset import MNISTDataset

# 1. åˆ›å»ºæ•°æ®é›†ï¼ˆå¯ç”¨ç¼“å­˜ï¼‰
train_dataset = MNISTDataset(
    root_dir=Path("data/mnist"),
    split='train',
    enable_cache=True
)

# 2. åˆ›å»ºDataLoaderï¼ˆå¯ç”¨é¢„è¯»å–ï¼‰
train_loader = train_dataset.dataloader(
    batch_size=32,
    shuffle=True,
    num_workers=2,
    enable_prefetch=True,
    prefetch_buffer_size=4
)

# 3. è®­ç»ƒ
model = YourModel()
optimizer = torch.optim.Adam(model.parameters())

for epoch in range(num_epochs):
    for batch in train_loader:
        # æ•°æ®å·²ç»é¢„è¯»å–å¥½äº†ï¼Œç›´æ¥ä½¿ç”¨
        images = batch['image']
        labels = batch['mask']
        
        # å‰å‘ä¼ æ’­
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. çº¿ç¨‹å®‰å…¨

é¢„è¯»å–ä½¿ç”¨ç‹¬ç«‹çº¿ç¨‹ï¼Œä½†PyTorchçš„æŸäº›æ“ä½œä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼š

```python
# âœ… å®‰å…¨: åªè¯»æ“ä½œ
for batch in dataloader:
    images = batch['image']
    # ...

# âŒ ä¸å®‰å…¨: ä¿®æ”¹å…±äº«çŠ¶æ€
# é¿å…åœ¨é¢„è¯»å–çº¿ç¨‹å’Œä¸»çº¿ç¨‹é—´å…±äº«å¯å˜å¯¹è±¡
```

### 2. å†…å­˜å ç”¨

```python
# ç¼“å†²åŒºå ç”¨å†…å­˜ä¼°ç®—
memory_per_sample = sample_size  # å•ä¸ªæ ·æœ¬å¤§å°
buffer_memory = memory_per_sample * prefetch_buffer_size

# ä¾‹å¦‚: å›¾åƒ(3, 224, 224), float32
memory_per_sample = 3 * 224 * 224 * 4 = 602KB
buffer_memory = 602KB * 4 = 2.4MB  # ç¼“å†²åŒº4æ—¶
```

### 3. é¢„è¯»å–å¤±æ•ˆåœºæ™¯

é¢„è¯»å–åœ¨ä»¥ä¸‹æƒ…å†µå¯èƒ½å¤±æ•ˆï¼š

- **éšæœºè·³è·ƒè®¿é—®**: ç´¢å¼•ä¸è¿ç»­
- **DataLoader shuffle**: æ¯ä¸ªepoché‡æ–°æ‰“ä¹±ï¼ˆä½†ä»æœ‰æ•ˆï¼‰
- **num_workerså¾ˆé«˜**: å¤šè¿›ç¨‹å·²ç»è¶³å¤Ÿå¿«

### 4. èµ„æºæ¸…ç†

```python
# é¢„è¯»å–çº¿ç¨‹ä¼šåœ¨å¯¹è±¡é”€æ¯æ—¶è‡ªåŠ¨åœæ­¢
# ä½†æœ€å¥½æ˜¾å¼åœæ­¢

prefetch_dataset = PrefetchDataset(dataset, buffer_size=4)
try:
    # ä½¿ç”¨æ•°æ®é›†...
    pass
finally:
    prefetch_dataset.stop_prefetch()  # æ˜¾å¼åœæ­¢
```

---

## ğŸ” è°ƒè¯•å’Œç›‘æ§

### æŸ¥çœ‹é¢„è¯»å–çŠ¶æ€

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# å¯ç”¨é¢„è¯»å–ï¼Œä¼šçœ‹åˆ°è¯¦ç»†æ—¥å¿—
dataloader = dataset.dataloader(
    batch_size=32,
    enable_prefetch=True
)

# æ—¥å¿—è¾“å‡ºç¤ºä¾‹:
# INFO - é¢„è¯»å–å·²å¯ç”¨ï¼Œç¼“å†²åŒºå¤§å°: 4
# INFO - DataLoaderå¯ç”¨é¢„è¯»å–ï¼Œæ¨¡å¼: general, ç¼“å†²åŒº: 4
# DEBUG - é¢„è¯»å–é˜Ÿåˆ—ä¸ºç©ºï¼Œç›´æ¥åŠ è½½ index=100
```

### æ€§èƒ½åˆ†æ

```python
import time

# æµ‹è¯•ä¸ä½¿ç”¨é¢„è¯»å–
start = time.time()
dataloader1 = dataset.dataloader(batch_size=32, enable_prefetch=False)
for batch in dataloader1:
    pass
time1 = time.time() - start

# æµ‹è¯•ä½¿ç”¨é¢„è¯»å–
start = time.time()
dataloader2 = dataset.dataloader(batch_size=32, enable_prefetch=True)
for batch in dataloader2:
    pass
time2 = time.time() - start

print(f"ä¸ä½¿ç”¨é¢„è¯»å–: {time1:.2f}s")
print(f"ä½¿ç”¨é¢„è¯»å–: {time2:.2f}s")
print(f"æå‡: {(time1-time2)/time1*100:.1f}%")
```

---

## ğŸ“š ç¤ºä¾‹ç¨‹åº

è¿è¡Œå®Œæ•´çš„æ¼”ç¤ºç¨‹åºï¼š

```bash
conda run -n ntrain python examples/prefetch_demo.py
```

è¯¥ç¨‹åºåŒ…å«ï¼š
- âœ… æ€§èƒ½å¯¹æ¯”æµ‹è¯•
- âœ… ä¸åŒç¼“å†²åŒºå¤§å°æµ‹è¯•
- âœ… shuffleåœºæ™¯æµ‹è¯•
- âœ… æ‰‹åŠ¨ä½¿ç”¨ç¤ºä¾‹

---

## ğŸ†š é¢„è¯»å– vs å¤šè¿›ç¨‹

| ç‰¹æ€§ | é¢„è¯»å–ï¼ˆçº¿ç¨‹ï¼‰ | å¤šè¿›ç¨‹ï¼ˆnum_workersï¼‰ |
|------|---------------|---------------------|
| å®ç° | å•çº¿ç¨‹ | å¤šè¿›ç¨‹ |
| å¼€é”€ | ä½ | é«˜ï¼ˆè¿›ç¨‹åˆ›å»ºï¼‰ |
| å†…å­˜ | å…±äº«å†…å­˜ | ç‹¬ç«‹å†…å­˜ |
| é€‚ç”¨ | IOå¯†é›† | CPUå¯†é›†é¢„å¤„ç† |
| é…ç½® | enable_prefetch | num_workers |

**å»ºè®®**: å¯ä»¥åŒæ—¶ä½¿ç”¨ï¼

```python
dataloader = dataset.dataloader(
    batch_size=32,
    num_workers=2,           # å¤šè¿›ç¨‹å¤„ç†é¢„å¤„ç†
    enable_prefetch=True,    # çº¿ç¨‹é¢„è¯»å–
    prefetch_buffer_size=4
)
```

---

## ğŸŠ æ€»ç»“

### æ ¸å¿ƒä¼˜åŠ¿

1. âœ¨ **ç®€å•**: ä¸€ä¸ªå‚æ•°å¯ç”¨
2. âœ¨ **é«˜æ•ˆ**: å‡å°‘20-30%è®­ç»ƒç­‰å¾…
3. âœ¨ **çµæ´»**: å¯é…ç½®ç¼“å†²åŒºå¤§å°
4. âœ¨ **å®‰å…¨**: çº¿ç¨‹å®‰å…¨ï¼Œè‡ªåŠ¨èµ„æºç®¡ç†
5. âœ¨ **é€šç”¨**: é€‚ç”¨äºæ‰€æœ‰æ•°æ®é›†

### ä½¿ç”¨å»ºè®®

- âœ… **æ¨èåœºæ™¯**: IOå¯†é›†ã€æ•°æ®é¢„å¤„ç†å¤æ‚
- âœ… **é…ç½®å»ºè®®**: buffer_size=4-6
- âœ… **ä¸ç¼“å­˜é…åˆ**: å…ˆç¼“å­˜åé¢„è¯»å–
- âœ… **ç›‘æ§æ€§èƒ½**: æµ‹è¯•å®é™…æå‡

### å¿«é€Ÿå¼€å§‹

```python
# ä»…éœ€ä¸€è¡Œä»£ç ï¼
dataloader = dataset.dataloader(
    batch_size=32,
    enable_prefetch=True
)
```

---

**ç‰ˆæœ¬**: 1.0.0  
**æ›´æ–°æ—¥æœŸ**: 2025-10-29  
**ç›¸å…³åŠŸèƒ½**: [æ•°æ®é›†ç¼“å­˜](AUTO_CACHE_GUIDE.md)
