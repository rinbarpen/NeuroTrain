# Models æ¨¡å—æ–‡æ¡£

## æ¦‚è¿°

Modelsæ¨¡å—æ˜¯NeuroTrainçš„æ ¸å¿ƒç»„ä»¶ï¼Œæä¾›äº†ç»Ÿä¸€çš„æ¨¡å‹æ¥å£å’Œä¸°å¯Œçš„æ¨¡å‹é€‰æ‹©ã€‚æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹ã€TorchVisioné¢„è®­ç»ƒæ¨¡å‹ã€TIMMæ¨¡å‹åº“ã€ä»¥åŠå¤šæ¨¡æ€æ¨¡å‹ï¼ˆCLIPã€LLaVAç­‰ï¼‰ã€‚

## ä¸»è¦ç‰¹æ€§

- ğŸ¯ **ç»Ÿä¸€çš„æ¨¡å‹æ¥å£**: é€šè¿‡ `get_model()` å‡½æ•°ç»Ÿä¸€è·å–å„ç§æ¨¡å‹
- ğŸ”„ **å¤šç§æ¨¡å‹åç«¯**: æ”¯æŒè‡ªå®šä¹‰ã€TorchVisionã€TIMMã€Hugging Faceç­‰
- ğŸ¨ **æ¨¡å—åŒ–è®¾è®¡**: æä¾›å„ç§å¯ç»„åˆçš„ç½‘ç»œç»„ä»¶
- ğŸš€ **é¢„è®­ç»ƒæ”¯æŒ**: è½»æ¾åŠ è½½é¢„è®­ç»ƒæƒé‡
- ğŸ”§ **çµæ´»å®šåˆ¶**: æ–¹ä¾¿ä¿®æ”¹æ¨¡å‹ç»“æ„å’Œå‚æ•°
- ğŸ“Š **æ¨¡å‹åˆ†æ**: é›†æˆæ¨¡å‹æ‘˜è¦å’ŒFLOPsè®¡ç®—

## æ ¸å¿ƒå‡½æ•°

### get_model()

ç»Ÿä¸€çš„æ¨¡å‹è·å–æ¥å£ï¼Œæ ¹æ®é…ç½®è¿”å›ç›¸åº”çš„æ¨¡å‹å®ä¾‹ã€‚

```python
from src.models import get_model

# è·å–æ¨¡å‹
model = get_model(model_name, config)
```

**å‚æ•°ï¼š**
- `model_name` (str): æ¨¡å‹åç§°ï¼Œå¦‚ 'unet', 'torchvision', 'timm', 'clip'ç­‰
- `config` (dict): æ¨¡å‹é…ç½®å­—å…¸

**è¿”å›ï¼š**
- `nn.Module`: PyTorchæ¨¡å‹å®ä¾‹

## æ”¯æŒçš„æ¨¡å‹ç±»å‹

### 1. è‡ªå®šä¹‰æ¨¡å‹

#### UNet - åŒ»å­¦å›¾åƒåˆ†å‰²

UNetæ˜¯ç»å…¸çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œé‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚

```python
config = {
    'n_channels': 3,      # è¾“å…¥é€šé“æ•°
    'n_classes': 2,       # è¾“å‡ºç±»åˆ«æ•°
    'bilinear': False     # æ˜¯å¦ä½¿ç”¨åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·
}

model = get_model('unet', config)
```

**ç‰¹ç‚¹ï¼š**
- Uå‹æ¶æ„ï¼Œå…·æœ‰è·³è·ƒè¿æ¥
- é€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡
- æ”¯æŒå¤šå°ºåº¦ç‰¹å¾èåˆ

**åº”ç”¨åœºæ™¯ï¼š**
- è§†ç½‘è†œè¡€ç®¡åˆ†å‰²
- ç»†èƒæ ¸åˆ†å‰²
- å™¨å®˜åˆ†å‰²

#### SimpleNet - ç®€å•ç¤ºä¾‹ç½‘ç»œ

ç”¨äºæ¼”ç¤ºå’Œå¿«é€ŸåŸå‹å¼€å‘çš„ç®€å•ç½‘ç»œã€‚

```python
model = get_model('simple-net', {})
```

### 2. TorchVisionæ¨¡å‹

ä½¿ç”¨TorchVisionæä¾›çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒResNetã€VGGã€DenseNetã€EfficientNetç­‰ã€‚

```python
config = {
    'arch': 'resnet18',         # æ¨¡å‹æ¶æ„
    'pretrained': True,         # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
    'n_classes': 10,            # ç›®æ ‡ç±»åˆ«æ•°
    'n_channels': 3             # è¾“å…¥é€šé“æ•°ï¼ˆé»˜è®¤3ï¼‰
}

model = get_model('torchvision', config)
```

**æ”¯æŒçš„æ¶æ„ï¼š**

#### åˆ†ç±»æ¨¡å‹
- **ResNetç³»åˆ—**: resnet18, resnet34, resnet50, resnet101, resnet152
- **VGGç³»åˆ—**: vgg11, vgg13, vgg16, vgg19 (å¯å¸¦bn)
- **DenseNetç³»åˆ—**: densenet121, densenet161, densenet169, densenet201
- **EfficientNetç³»åˆ—**: efficientnet_b0 åˆ° efficientnet_b7
- **MobileNetç³»åˆ—**: mobilenet_v2, mobilenet_v3_small, mobilenet_v3_large
- **Vision Transformer**: vit_b_16, vit_b_32, vit_l_16, vit_l_32

**ç‰¹ç‚¹ï¼š**
- è‡ªåŠ¨åŠ è½½ImageNeté¢„è®­ç»ƒæƒé‡
- è‡ªåŠ¨è°ƒæ•´åˆ†ç±»å¤´é€‚åº”ç›®æ ‡ç±»åˆ«æ•°
- æ”¯æŒç°åº¦å›¾åƒï¼ˆè‡ªåŠ¨ä¿®æ”¹ç¬¬ä¸€å±‚ï¼‰

**ç¤ºä¾‹ï¼š**

```python
# ResNet18ç”¨äºCIFAR-10åˆ†ç±»
config = {
    'arch': 'resnet18',
    'pretrained': True,
    'n_classes': 10
}
model = get_model('torchvision', config)

# EfficientNetç”¨äºImageNetåˆ†ç±»
config = {
    'arch': 'efficientnet_b0',
    'pretrained': True,
    'n_classes': 1000
}
model = get_model('torchvision', config)

# Vision Transformer
config = {
    'arch': 'vit_b_16',
    'pretrained': True,
    'n_classes': 100  # CIFAR-100
}
model = get_model('torchvision', config)
```

### 3. TIMMæ¨¡å‹åº“

TIMM (PyTorch Image Models) æä¾›äº†æ•°ç™¾ç§é¢„è®­ç»ƒæ¨¡å‹ã€‚

```python
config = {
    'model_name': 'efficientnet_b0',  # TIMMæ¨¡å‹åç§°
    'pretrained': True,                # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
    'n_classes': 100,                  # ç›®æ ‡ç±»åˆ«æ•°
    'n_channels': 3                    # è¾“å…¥é€šé“æ•°
}

model = get_model('timm', config)
```

**çƒ­é—¨æ¨¡å‹ï¼š**
- EfficientNetç³»åˆ—
- RegNetç³»åˆ—
- NFNetç³»åˆ—
- ConvNeXtç³»åˆ—
- Swin Transformerç³»åˆ—
- å„ç§ViTå˜ä½“

**å®‰è£…ï¼š**
```bash
pip install timm
```

**æŸ¥çœ‹å¯ç”¨æ¨¡å‹ï¼š**
```python
import timm
available_models = timm.list_models()
print(f"Available models: {len(available_models)}")
```

### 4. CLIP - å¤šæ¨¡æ€æ¨¡å‹

CLIP (Contrastive Language-Image Pre-training) æ˜¯OpenAIçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å›¾åƒå’Œæ–‡æœ¬çš„å…³ç³»ã€‚

```python
config = {
    'model_name': 'openai/clip-vit-base-patch32',
    'cache_dir': 'cache/models/pretrained',
    'device': 'cuda',
    'dtype': torch.float16
}

model = get_model('clip', config)
```

**å¯ç”¨çš„CLIPæ¨¡å‹ï¼š**
- openai/clip-vit-base-patch32
- openai/clip-vit-base-patch16
- openai/clip-vit-large-patch14

**åŠŸèƒ½ï¼š**
- å›¾åƒç¼–ç 
- æ–‡æœ¬ç¼–ç 
- å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
- é›¶æ ·æœ¬åˆ†ç±»
- å›¾åƒæ£€ç´¢

**ç¤ºä¾‹ï¼š**
```python
from transformers import CLIPProcessor

processor = CLIPProcessor.from_pretrained(config['model_name'])
model = get_model('clip', config)

# å›¾åƒç¼–ç 
images = torch.randn(4, 3, 224, 224)
image_features = model.get_image_features(images)

# æ–‡æœ¬ç¼–ç 
texts = ["a photo of a cat", "a photo of a dog"]
text_inputs = processor(text=texts, return_tensors="pt", padding=True)
text_features = model.get_text_features(**text_inputs)

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = torch.matmul(image_features, text_features.T)
```

## æ¨¡å‹ç»„ä»¶

Modelsæ¨¡å—è¿˜æä¾›äº†ä¸°å¯Œçš„å¯ç»„åˆç»„ä»¶ï¼Œç”¨äºæ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚

### æ³¨æ„åŠ›æœºåˆ¶ (attention/)

```python
from src.models.attention import SelfAttention, MultiHeadAttention

# è‡ªæ³¨æ„åŠ›
self_attn = SelfAttention(embed_dim=512, num_heads=8)

# å¤šå¤´æ³¨æ„åŠ›
multi_head_attn = MultiHeadAttention(embed_dim=512, num_heads=8)
```

**å¯ç”¨çš„æ³¨æ„åŠ›æ¨¡å—ï¼š**
- SelfAttention: è‡ªæ³¨æ„åŠ›
- MultiHeadAttention: å¤šå¤´æ³¨æ„åŠ›
- CrossAttention: äº¤å‰æ³¨æ„åŠ›
- SpatialAttention: ç©ºé—´æ³¨æ„åŠ›
- ChannelAttention: é€šé“æ³¨æ„åŠ›
- CBAM: å·ç§¯å—æ³¨æ„åŠ›æ¨¡å—

### Transformerç»„ä»¶ (transformer/)

```python
from src.models.transformer import TransformerBlock, TransformerEncoder

# Transformerå—
transformer_block = TransformerBlock(
    embed_dim=512,
    num_heads=8,
    mlp_ratio=4.0,
    dropout=0.1
)

# Transformerç¼–ç å™¨
encoder = TransformerEncoder(
    num_layers=6,
    embed_dim=512,
    num_heads=8
)
```

### å·ç§¯å±‚å˜ä½“ (conv/)

```python
from src.models.conv import DepthwiseSeparableConv, InvertedResidual

# æ·±åº¦å¯åˆ†ç¦»å·ç§¯
dwconv = DepthwiseSeparableConv(in_channels=64, out_channels=128)

# å€’æ®‹å·®å—ï¼ˆMobileNetï¼‰
inverted_residual = InvertedResidual(
    in_channels=64,
    out_channels=128,
    stride=1,
    expand_ratio=6
)
```

### å½’ä¸€åŒ–å±‚ (norm/)

```python
from src.models.norm import LayerNorm, GroupNorm, BatchNorm2d

# Layer Normalization
ln = LayerNorm(normalized_shape=512)

# Group Normalization
gn = GroupNorm(num_groups=32, num_channels=512)
```

### ä½ç½®ç¼–ç  (position_encoding.py)

```python
from src.models.position_encoding import PositionalEncoding, LearnedPositionalEncoding

# å›ºå®šä½ç½®ç¼–ç 
pos_enc = PositionalEncoding(d_model=512, max_len=5000)

# å¯å­¦ä¹ ä½ç½®ç¼–ç 
learned_pos_enc = LearnedPositionalEncoding(d_model=512, max_len=5000)
```

### åµŒå…¥å±‚ (embedding.py)

```python
from src.models.embedding import PatchEmbedding, TokenEmbedding

# å›¾åƒå—åµŒå…¥ï¼ˆVision Transformerï¼‰
patch_emb = PatchEmbedding(
    img_size=224,
    patch_size=16,
    in_channels=3,
    embed_dim=768
)

# TokenåµŒå…¥
token_emb = TokenEmbedding(
    vocab_size=10000,
    embed_dim=512
)
```

## æ¨¡å‹å®šåˆ¶

### ä¿®æ”¹åˆ†ç±»å¤´

```python
import torch.nn as nn

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
config = {'arch': 'resnet18', 'pretrained': True, 'n_classes': 1000}
model = get_model('torchvision', config)

# æ›¿æ¢åˆ†ç±»å¤´
num_features = model.fc.in_features
model.fc = nn.Sequential(
    nn.Dropout(0.5),
    nn.Linear(num_features, 256),
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(256, 10)  # 10ä¸ªç±»åˆ«
)
```

### å†»ç»“éƒ¨åˆ†å±‚

```python
# å†»ç»“é™¤åˆ†ç±»å¤´å¤–çš„æ‰€æœ‰å±‚
for name, param in model.named_parameters():
    if 'fc' not in name:  # ä¸å†»ç»“fcå±‚
        param.requires_grad = False

# ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
total = sum(p.numel() for p in model.parameters())
print(f"Trainable: {trainable:,} / {total:,} ({trainable/total*100:.1f}%)")
```

### æ·»åŠ è‡ªå®šä¹‰å±‚

```python
class CustomModel(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base = base_model
        self.custom_head = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        # æå–ç‰¹å¾
        features = self.base.features(x)
        # è‡ªå®šä¹‰åˆ†ç±»å¤´
        output = self.custom_head(features)
        return output

# ä½¿ç”¨
base = get_model('torchvision', {'arch': 'resnet18', 'pretrained': True})
model = CustomModel(base)
```

## æ¨¡å‹åˆ†æ

### æ¨¡å‹æ‘˜è¦

```python
from torchinfo import summary

model = get_model('unet', {'n_channels': 3, 'n_classes': 2})

# æ‰“å°æ¨¡å‹æ‘˜è¦
summary(model, 
        input_size=(1, 3, 512, 512),  # (batch_size, channels, height, width)
        col_names=["input_size", "output_size", "num_params", "mult_adds"],
        depth=4)
```

### è®¡ç®—FLOPs

```python
from fvcore.nn import FlopCountAnalysis, parameter_count

model = get_model('resnet18', {'arch': 'resnet18', 'pretrained': False})
inputs = torch.randn(1, 3, 224, 224)

# è®¡ç®—FLOPs
flops = FlopCountAnalysis(model, inputs)
print(f"FLOPs: {flops.total() / 1e9:.2f} G")

# è®¡ç®—å‚æ•°é‡
params = parameter_count(model)
print(f"Parameters: {params[''] / 1e6:.2f} M")
```

## æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

### ä¿å­˜æ¨¡å‹

```python
import torch
from pathlib import Path

# ä¿å­˜å®Œæ•´æ¨¡å‹
save_path = Path('models/my_model.pth')
save_path.parent.mkdir(parents=True, exist_ok=True)

torch.save({
    'model_state_dict': model.state_dict(),
    'model_name': 'unet',
    'config': config,
    'epoch': epoch,
    'optimizer_state_dict': optimizer.state_dict(),
}, save_path)

# ä»…ä¿å­˜æƒé‡
torch.save(model.state_dict(), 'models/model_weights.pth')
```

### åŠ è½½æ¨¡å‹

```python
# åŠ è½½å®Œæ•´æ¨¡å‹
checkpoint = torch.load('models/my_model.pth', map_location='cpu')

# é‡å»ºæ¨¡å‹
model = get_model(checkpoint['model_name'], checkpoint['config'])
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# ä»…åŠ è½½æƒé‡
model.load_state_dict(torch.load('models/model_weights.pth'))
```

### éƒ¨åˆ†åŠ è½½

```python
# åŠ è½½éƒ¨åˆ†æƒé‡ï¼ˆå¦‚é¢„è®­ç»ƒçš„backboneï¼‰
pretrained_dict = torch.load('pretrained_backbone.pth')
model_dict = model.state_dict()

# è¿‡æ»¤ä¸åŒ¹é…çš„é”®
pretrained_dict = {k: v for k, v in pretrained_dict.items() 
                   if k in model_dict and model_dict[k].shape == v.shape}

# æ›´æ–°æ¨¡å‹å­—å…¸
model_dict.update(pretrained_dict)
model.load_state_dict(model_dict)
```

## æ¨¡å‹å¯¼å‡º

### ONNXå¯¼å‡º

```python
from pathlib import Path

model = get_model('resnet18', {'arch': 'resnet18', 'pretrained': True})
model.eval()

# å‡†å¤‡ç¤ºä¾‹è¾“å…¥
dummy_input = torch.randn(1, 3, 224, 224)

# å¯¼å‡ºä¸ºONNX
onnx_path = Path('models/resnet18.onnx')
torch.onnx.export(
    model,
    dummy_input,
    onnx_path,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={
        'input': {0: 'batch_size'},
        'output': {0: 'batch_size'}
    },
    opset_version=11
)

print(f"Model exported to {onnx_path}")
```

### TorchScriptå¯¼å‡º

```python
# Traceæ¨¡å¼
traced_model = torch.jit.trace(model, dummy_input)
traced_model.save('models/model_traced.pt')

# Scriptæ¨¡å¼
scripted_model = torch.jit.script(model)
scripted_model.save('models/model_scripted.pt')

# åŠ è½½TorchScriptæ¨¡å‹
loaded_model = torch.jit.load('models/model_traced.pt')
loaded_model.eval()
```

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„æ¨¡å‹

**å›¾åƒåˆ†ç±»ï¼š**
- å°æ•°æ®é›†ï¼šResNet18, MobileNetV2
- ä¸­ç­‰æ•°æ®é›†ï¼šResNet50, EfficientNet-B0
- å¤§æ•°æ®é›†ï¼šEfficientNet-B4, Vision Transformer

**åŒ»å­¦å›¾åƒåˆ†å‰²ï¼š**
- 2Dåˆ†å‰²ï¼šUNet, UNet++
- 3Dåˆ†å‰²ï¼š3D UNet, V-Net

**ç›®æ ‡æ£€æµ‹ï¼š**
- å®æ—¶æ£€æµ‹ï¼šYOLO, SSD
- é«˜ç²¾åº¦æ£€æµ‹ï¼šFaster R-CNN, Mask R-CNN

### 2. é¢„è®­ç»ƒç­–ç•¥

- å°½å¯èƒ½ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
- ImageNeté¢„è®­ç»ƒé€‚ç”¨äºå¤§å¤šæ•°è§†è§‰ä»»åŠ¡
- åŒ»å­¦å›¾åƒå¯èƒ½éœ€è¦é¢†åŸŸç‰¹å®šçš„é¢„è®­ç»ƒ

### 3. è¿ç§»å­¦ä¹ 

```python
# Step 1: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = get_model('torchvision', {
    'arch': 'resnet50',
    'pretrained': True,
    'n_classes': 1000
})

# Step 2: å†»ç»“æ—©æœŸå±‚
for param in list(model.parameters())[:-10]:
    param.requires_grad = False

# Step 3: ä¿®æ”¹åˆ†ç±»å¤´
model.fc = nn.Linear(model.fc.in_features, num_classes)

# Step 4: ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è®­ç»ƒ
optimizer = torch.optim.Adam([
    {'params': model.fc.parameters(), 'lr': 1e-3},
    {'params': filter(lambda p: p.requires_grad, model.parameters()), 'lr': 1e-4}
])
```

### 4. æ¨¡å‹æ€§èƒ½ä¼˜åŒ–

```python
# æ··åˆç²¾åº¦è®­ç»ƒ
model = model.half()  # è½¬æ¢ä¸ºfloat16

# æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœå†…å­˜ï¼‰
from torch.utils.checkpoint import checkpoint

class CheckpointedBlock(nn.Module):
    def __init__(self, block):
        super().__init__()
        self.block = block
    
    def forward(self, x):
        return checkpoint(self.block, x)

# æ¨¡å‹ç¼–è¯‘ï¼ˆPyTorch 2.0+ï¼‰
compiled_model = torch.compile(model)
```

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•æŸ¥çœ‹æ¨¡å‹ç»“æ„ï¼Ÿ

```python
# æ–¹æ³•1: æ‰“å°æ¨¡å‹
print(model)

# æ–¹æ³•2: ä½¿ç”¨torchinfo
from torchinfo import summary
summary(model, input_size=(1, 3, 224, 224))

# æ–¹æ³•3: å¯è§†åŒ–
from torchviz import make_dot
y = model(x)
make_dot(y, params=dict(model.named_parameters())).render("model", format="png")
```

### Q: æ¨¡å‹å¤ªå¤§ï¼Œå†…å­˜ä¸è¶³ï¼Ÿ

- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚MobileNetï¼‰
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- å‡å°batch size
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- ä½¿ç”¨æ¨¡å‹é‡åŒ–

### Q: å¦‚ä½•æ·»åŠ è‡ªå®šä¹‰å±‚ï¼Ÿ

```python
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        # ä½¿ç”¨ç°æœ‰æ¨¡å‹ä½œä¸ºbackbone
        self.backbone = get_model('torchvision', {
            'arch': 'resnet18',
            'pretrained': True
        })
        # ç§»é™¤åŸå§‹åˆ†ç±»å¤´
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])
        # æ·»åŠ è‡ªå®šä¹‰å±‚
        self.custom_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.backbone(x)
        x = self.custom_layers(x)
        return x
```

## å‚è€ƒèµ„æ–™

- [PyTorchæ¨¡å‹æ–‡æ¡£](https://pytorch.org/docs/stable/nn.html)
- [TorchVisionæ¨¡å‹](https://pytorch.org/vision/stable/models.html)
- [TIMMåº“æ–‡æ¡£](https://timm.fast.ai/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [ONNXæ–‡æ¡£](https://onnx.ai/)

---

æ›´å¤šç¤ºä¾‹è¯·æŸ¥çœ‹ `examples/models_basic_example.py` å’Œ `examples/` ç›®å½•ä¸­çš„Jupyter Notebooksã€‚

