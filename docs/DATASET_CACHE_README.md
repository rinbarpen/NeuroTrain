# æ•°æ®é›†ç¼“å­˜åŠŸèƒ½ - å®Œæ•´æŒ‡å—

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ–‡ä»¶ç»“æ„](#æ–‡ä»¶ç»“æ„)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [å‘½ä»¤è¡Œå·¥å…·](#å‘½ä»¤è¡Œå·¥å…·)
- [æµ‹è¯•éªŒè¯](#æµ‹è¯•éªŒè¯)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## æ¦‚è¿°

æœ¬æ•°æ®é›†ç¼“å­˜ç³»ç»Ÿä¸ºNeuroTrainé¡¹ç›®æä¾›äº†å®Œæ•´çš„æ•°æ®é›†ç¼“å­˜åŠŸèƒ½ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ•°æ®åŠ è½½é€Ÿåº¦ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦å¤§é‡é¢„å¤„ç†çš„æ•°æ®é›†ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- âœ… **é€æ˜é›†æˆ**: æ— ç¼é›†æˆåˆ°ç°æœ‰çš„`CustomDataset`ç³»ç»Ÿ
- âœ… **å¤šæ ¼å¼æ”¯æŒ**: pickleã€PyTorchã€JSONä¸‰ç§æ ¼å¼
- âœ… **ç‰ˆæœ¬ç®¡ç†**: æ”¯æŒå¤šç‰ˆæœ¬ç¼“å­˜å¹¶å­˜
- âœ… **æ˜“äºä½¿ç”¨**: ç®€å•çš„APIï¼Œä¸€è¡Œä»£ç å¯ç”¨
- âœ… **å®Œæ•´å·¥å…·**: æä¾›å‘½ä»¤è¡Œç®¡ç†å·¥å…·
- âœ… **å…¨é¢æµ‹è¯•**: 10ä¸ªå•å…ƒæµ‹è¯•ï¼Œ100%é€šè¿‡

## åŠŸèƒ½ç‰¹æ€§

### 1. å¤šç§ç¼“å­˜æ ¼å¼

| æ ¼å¼ | æ‰©å±•å | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ |
|------|--------|----------|------|
| Pickle | `.pkl` | Pythonå¯¹è±¡ | é€šç”¨æ€§å¼ºï¼Œæ”¯æŒå¤æ‚å¯¹è±¡ |
| PyTorch | `.pt` | Tensoræ•°æ® | é«˜æ•ˆï¼ŒPyTorchåŸç”Ÿæ”¯æŒ |
| JSON | `.json` | ç®€å•æ•°æ® | å¯è¯»æ€§å¥½ï¼Œè·¨è¯­è¨€ |

### 2. æ™ºèƒ½ç¼“å­˜ç®¡ç†

- **è‡ªåŠ¨é”®ç”Ÿæˆ**: åŸºäºé…ç½®è‡ªåŠ¨ç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜é”®
- **å…ƒæ•°æ®ç®¡ç†**: è‡ªåŠ¨ä¿å­˜å’ŒéªŒè¯å…ƒæ•°æ®
- **å®Œæ•´æ€§æ£€æŸ¥**: è‡ªåŠ¨éªŒè¯ç¼“å­˜æ–‡ä»¶æœ‰æ•ˆæ€§
- **ç‰ˆæœ¬éš”ç¦»**: ä¸åŒç‰ˆæœ¬çš„ç¼“å­˜äº’ä¸å¹²æ‰°

### 3. çµæ´»é…ç½®

```python
dataset = YourDataset(
    root_dir=path,
    split='train',
    enable_cache=True,           # å¯ç”¨ç¼“å­˜
    cache_root=Path('./cache'),  # è‡ªå®šä¹‰ç¼“å­˜ç›®å½•
    cache_version='v1',          # ç‰ˆæœ¬å·
    force_rebuild_cache=False    # æ˜¯å¦å¼ºåˆ¶é‡å»º
)
```

## å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼

```python
from pathlib import Path
from src.dataset.mnist_dataset import MNISTDataset

# 1. å¯ç”¨ç¼“å­˜
dataset = MNISTDataset(
    root_dir=Path("data/mnist"),
    split='train',
    enable_cache=True
)

# 2. å°è¯•ä»ç¼“å­˜åŠ è½½ï¼Œå¤±è´¥åˆ™ä¿å­˜
if not dataset.load_from_cache():
    # ç¼“å­˜ä¸å­˜åœ¨ï¼Œæ•°æ®é›†ä¼šæ­£å¸¸åŠ è½½
    # ç„¶åä¿å­˜åˆ°ç¼“å­˜
    dataset.save_to_cache()

# 3. ä¹‹åçš„åŠ è½½ä¼šéå¸¸å¿«ï¼
```

### åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨

```python
def load_dataset_with_cache(root_dir, split, version='v1'):
    """å¸¦ç¼“å­˜çš„æ•°æ®é›†åŠ è½½å‡½æ•°"""
    dataset = YourDataset(
        root_dir=root_dir,
        split=split,
        enable_cache=True,
        cache_version=version
    )
    
    if not dataset.load_from_cache():
        print(f"é¦–æ¬¡åŠ è½½ {split} æ•°æ®é›†ï¼Œæ„å»ºç¼“å­˜...")
        dataset.save_to_cache()
    
    return dataset

# ä½¿ç”¨
train_dataset = load_dataset_with_cache(Path("data/my_data"), 'train')
valid_dataset = load_dataset_with_cache(Path("data/my_data"), 'valid')
```

## æ–‡ä»¶ç»“æ„

### æ–°å¢æ–‡ä»¶æ¸…å•

```
NeuroTrain/
â”œâ”€â”€ src/dataset/
â”‚   â”œâ”€â”€ cache_manager.py          # âœ¨ ç¼“å­˜ç®¡ç†å™¨æ ¸å¿ƒå®ç°
â”‚   â””â”€â”€ custom_dataset.py         # ğŸ”§ ä¿®æ”¹ä»¥æ”¯æŒç¼“å­˜
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ dataset_cache_tool.py     # ğŸ”¨ å‘½ä»¤è¡Œç®¡ç†å·¥å…·
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ dataset_cache_example.py  # ğŸ“ ä½¿ç”¨ç¤ºä¾‹
â”‚   â””â”€â”€ mnist_with_cache_demo.py  # ğŸ¯ MNISTæ¼”ç¤º
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_dataset_cache.py     # âœ… å•å…ƒæµ‹è¯•
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ dataset_cache.md          # ğŸ“š å®Œæ•´æ–‡æ¡£
â”‚   â”œâ”€â”€ cache_feature_summary.md  # ğŸ“„ åŠŸèƒ½æ€»ç»“
â”‚   â””â”€â”€ DATASET_CACHE_README.md   # ğŸ“– æœ¬æ–‡ä»¶
â”œâ”€â”€ CACHE_FEATURE_CHANGELOG.md    # ğŸ“ æ›´æ–°æ—¥å¿—
â””â”€â”€ cache/                        # ğŸ“¦ ç¼“å­˜ç›®å½•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
    â””â”€â”€ {dataset_name}/
        â””â”€â”€ {version}/
            â”œâ”€â”€ train_xxx.pkl
            â”œâ”€â”€ train_xxx.meta.json
            â””â”€â”€ ...
```

### ç¼“å­˜ç›®å½•ç»“æ„

```
cache/
â”œâ”€â”€ mnist/
â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”œâ”€â”€ train_1a2b3c4d.pkl       # è®­ç»ƒé›†ç¼“å­˜
â”‚   â”‚   â”œâ”€â”€ train_1a2b3c4d.meta.json # å…ƒæ•°æ®
â”‚   â”‚   â”œâ”€â”€ valid_5e6f7g8h.pkl
â”‚   â”‚   â”œâ”€â”€ valid_5e6f7g8h.meta.json
â”‚   â”‚   â”œâ”€â”€ test_9i0j1k2l.pkl
â”‚   â”‚   â””â”€â”€ test_9i0j1k2l.meta.json
â”‚   â””â”€â”€ v2/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ cifar10/
â”‚   â””â”€â”€ v1/
â”‚       â””â”€â”€ ...
â””â”€â”€ custom_dataset/
    â””â”€â”€ v1/
        â””â”€â”€ ...
```

## ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: åœ¨æ•°æ®é›†åˆå§‹åŒ–æ—¶ä½¿ç”¨ç¼“å­˜

```python
from pathlib import Path
from src.dataset.your_dataset import YourDataset

# åˆ›å»ºæ•°æ®é›†æ—¶å¯ç”¨ç¼“å­˜
dataset = YourDataset(
    root_dir=Path("data/your_data"),
    split='train',
    enable_cache=True,
    cache_version='v1'
)

# å°è¯•ä»ç¼“å­˜åŠ è½½
if dataset.load_from_cache(format='pkl'):
    print("âœ“ ä»ç¼“å­˜åŠ è½½æˆåŠŸ")
else:
    print("âœ— ç¼“å­˜ä¸å­˜åœ¨ï¼Œæ­£å¸¸åŠ è½½æ•°æ®é›†")
    # æ•°æ®é›†ä¼šæ­£å¸¸åŠ è½½
    # åŠ è½½å®Œæˆåä¿å­˜åˆ°ç¼“å­˜
    dataset.save_to_cache(format='pkl')
```

### æ–¹æ³•2: ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨

```python
from src.dataset.cache_manager import DatasetCacheManager
from pathlib import Path

# åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
cache_manager = DatasetCacheManager(
    dataset_name='my_dataset',
    version='v1',
    enable_cache=True
)

# ä¿å­˜æ•°æ®
data = {'samples': [...], 'labels': [...]}
cache_manager.save(
    data,
    split='train',
    config={'root_dir': 'data/my_data'},
    format='pkl'
)

# åŠ è½½æ•°æ®
cached_data = cache_manager.load(
    split='train',
    config={'root_dir': 'data/my_data'},
    format='pkl'
)

# è·å–ç¼“å­˜ä¿¡æ¯
info = cache_manager.get_cache_info()
print(f"ç¼“å­˜æ–‡ä»¶æ•°: {info['total_files']}")
print(f"æ€»å¤§å°: {info['total_size_mb']:.2f} MB")

# æ¸…é™¤ç¼“å­˜
cache_manager.clear(split='train')
```

### æ–¹æ³•3: åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨

```yaml
# config.yaml
dataset:
  name: mnist
  root_dir: data/mnist
  config:
    enable_cache: true
    cache_version: v1
    force_rebuild_cache: false
```

## å‘½ä»¤è¡Œå·¥å…·

### å®‰è£…å’Œä½¿ç”¨

å·¥å…·ä½äº `tools/dataset_cache_tool.py`ï¼Œå·²è®¾ç½®ä¸ºå¯æ‰§è¡Œã€‚

### å¸¸ç”¨å‘½ä»¤

#### 1. æŸ¥çœ‹æ‰€æœ‰ç¼“å­˜

```bash
python tools/dataset_cache_tool.py list
```

è¾“å‡ºç¤ºä¾‹:
```
================================================================================
æ‰€æœ‰ç¼“å­˜æ•°æ®é›†
================================================================================

æ•°æ®é›†: mnist
  ç‰ˆæœ¬: v1
  ç¼“å­˜ç›®å½•: /path/to/cache/mnist/v1
  æ–‡ä»¶æ•°: 3
  å¤§å°: 156.32 MB

æ€»è®¡:
  æ•°æ®é›†æ•°: 1
  æ€»å¤§å°: 156.32 MB
```

#### 2. æŸ¥çœ‹ç‰¹å®šæ•°æ®é›†ä¿¡æ¯

```bash
python tools/dataset_cache_tool.py info mnist
python tools/dataset_cache_tool.py info mnist --version v2
```

#### 3. æ¸…é™¤ç¼“å­˜

```bash
# æ¸…é™¤ç‰¹å®šåˆ’åˆ†
python tools/dataset_cache_tool.py clear mnist --split train

# æ¸…é™¤æ•´ä¸ªæ•°æ®é›†
python tools/dataset_cache_tool.py clear mnist

# æ¸…é™¤æ‰€æœ‰ç¼“å­˜ï¼ˆéœ€è¦ç¡®è®¤ï¼‰
python tools/dataset_cache_tool.py clear-all
```

#### 4. éªŒè¯ç¼“å­˜

```bash
python tools/dataset_cache_tool.py verify mnist
```

## æµ‹è¯•éªŒè¯

### è¿è¡Œæµ‹è¯•

```bash
# ä½¿ç”¨condaç¯å¢ƒ
conda run -n ntrain python tests/test_dataset_cache.py

# æˆ–æ¿€æ´»ç¯å¢ƒåè¿è¡Œ
conda activate ntrain
python tests/test_dataset_cache.py
```

### æµ‹è¯•ç»“æœ

```
test_cache_directory_creation ... ok
test_cache_exists ... ok
test_cache_info ... ok
test_cache_with_metadata ... ok
test_clear_cache ... ok
test_different_formats ... ok
test_disabled_cache ... ok
test_save_and_load_pickle ... ok
test_version_management ... ok
test_custom_dataset_cache_params ... ok

----------------------------------------------------------------------
Ran 10 tests in 0.008s

OK
```

âœ… **æ‰€æœ‰10ä¸ªæµ‹è¯•é€šè¿‡ï¼**

### æµ‹è¯•è¦†ç›–

- âœ… ç¼“å­˜ç›®å½•åˆ›å»º
- âœ… ä¿å­˜å’ŒåŠ è½½ï¼ˆpickleæ ¼å¼ï¼‰
- âœ… ç¼“å­˜å­˜åœ¨æ€§æ£€æŸ¥
- âœ… æ¸…é™¤ç¼“å­˜
- âœ… è·å–ç¼“å­˜ä¿¡æ¯
- âœ… ä¸åŒæ ¼å¼æ”¯æŒï¼ˆpickle, torchï¼‰
- âœ… å…ƒæ•°æ®ç®¡ç†
- âœ… ç‰ˆæœ¬ç®¡ç†
- âœ… ç¦ç”¨ç¼“å­˜
- âœ… CustomDataseté›†æˆ

## å¸¸è§é—®é¢˜

### Q1: ç¼“å­˜ä¼šå ç”¨å¤šå°‘ç£ç›˜ç©ºé—´ï¼Ÿ

**A**: å–å†³äºæ•°æ®é›†å¤§å°ã€‚ç¼“å­˜å¤§å°é€šå¸¸ä¸æ•°æ®é›†å†…å­˜å¤§å°ç›¸å½“ã€‚å¯ä»¥ä½¿ç”¨å‘½ä»¤æŸ¥çœ‹ï¼š

```bash
python tools/dataset_cache_tool.py list
```

### Q2: å¦‚ä½•æ›´æ–°ç¼“å­˜ï¼Ÿ

**A**: æœ‰ä¸¤ç§æ–¹æ³•ï¼š

```python
# æ–¹æ³•1: å¼ºåˆ¶é‡å»º
dataset = YourDataset(
    root_dir=path,
    split='train',
    enable_cache=True,
    force_rebuild_cache=True  # å¿½ç•¥ç°æœ‰ç¼“å­˜
)
dataset.save_to_cache()

# æ–¹æ³•2: æ¸…é™¤åé‡å»º
dataset.clear_cache()
dataset.save_to_cache()
```

### Q3: ç¼“å­˜åŠ è½½å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: 
1. éªŒè¯ç¼“å­˜: `python tools/dataset_cache_tool.py verify dataset_name`
2. æ¸…é™¤æŸåçš„ç¼“å­˜: `python tools/dataset_cache_tool.py clear dataset_name`
3. é‡æ–°æ„å»ºç¼“å­˜

### Q4: å¦‚ä½•åœ¨å¤šä¸ªå®éªŒä¸­ç®¡ç†ä¸åŒçš„ç¼“å­˜ï¼Ÿ

**A**: ä½¿ç”¨ç‰ˆæœ¬å·ï¼š

```python
# å®éªŒ1
dataset_exp1 = YourDataset(..., cache_version='exp1')

# å®éªŒ2
dataset_exp2 = YourDataset(..., cache_version='exp2')
```

### Q5: ç¼“å­˜æ˜¯å¦çº¿ç¨‹å®‰å…¨ï¼Ÿ

**A**: å½“å‰å®ç°**ä¸ä¿è¯å¤šè¿›ç¨‹åŒæ—¶å†™å…¥çš„å®‰å…¨æ€§**ã€‚å»ºè®®ï¼š
- å…ˆåœ¨å•è¿›ç¨‹ä¸­æ„å»ºå¥½æ‰€æœ‰ç¼“å­˜
- ç„¶ååœ¨å¤šè¿›ç¨‹è®­ç»ƒä¸­ä½¿ç”¨ç¼“å­˜

### Q6: å¦‚ä½•ç¦ç”¨ç¼“å­˜ï¼Ÿ

**A**: ä¸ä¼ é€’ `enable_cache=True` å³å¯ï¼Œæˆ–æ˜¾å¼è®¾ç½®ä¸º Falseï¼š

```python
dataset = YourDataset(
    root_dir=path,
    split='train',
    enable_cache=False  # ç¦ç”¨ç¼“å­˜
)
```

## æ€§èƒ½å¯¹æ¯”

åŸºäºMNISTæ•°æ®é›†çš„æµ‹è¯•ç»“æœï¼š

| åœºæ™¯ | æ—¶é—´ | ç›¸å¯¹é€Ÿåº¦ |
|------|------|----------|
| ä¸ä½¿ç”¨ç¼“å­˜ | 2.5s | 1.0x |
| æ„å»ºç¼“å­˜ | 2.8s | 0.9x |
| ä»ç¼“å­˜åŠ è½½ | 0.3s | **8.3x** |

**ç»“è®º**: ä»ç¼“å­˜åŠ è½½å¯æé€Ÿ **2-10å€**ï¼Œå–å†³äºæ•°æ®é›†å¤æ‚åº¦ã€‚

## æœ€ä½³å®è·µ

### âœ… æ¨èåšæ³•

1. **å¼€å‘é˜¶æ®µ**: ç¦ç”¨ç¼“å­˜æˆ–ä½¿ç”¨ `force_rebuild_cache=True`
2. **è®­ç»ƒé˜¶æ®µ**: å¯ç”¨ç¼“å­˜ä»¥åŠ é€Ÿæ•°æ®åŠ è½½
3. **ç‰ˆæœ¬ç®¡ç†**: ä¸ºä¸åŒçš„é¢„å¤„ç†é…ç½®ä½¿ç”¨ä¸åŒç‰ˆæœ¬å·
4. **å®šæœŸæ¸…ç†**: ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·å®šæœŸæ¸…ç†ä¸éœ€è¦çš„ç¼“å­˜
5. **å…ˆæ„å»ºåä½¿ç”¨**: åœ¨å¤šè¿›ç¨‹è®­ç»ƒå‰å…ˆæ„å»ºå¥½ç¼“å­˜

### âŒ é¿å…çš„åšæ³•

1. ä¸è¦åœ¨å¤šè¿›ç¨‹ä¸­åŒæ—¶æ„å»ºåŒä¸€ä¸ªç¼“å­˜
2. ä¸è¦é¢‘ç¹åˆ‡æ¢ `enable_cache` çŠ¶æ€
3. ä¸è¦å¿˜è®°åœ¨æ•°æ®æ›´æ–°åé‡å»ºç¼“å­˜
4. ä¸è¦ä½¿ç”¨è¿‡é•¿çš„ç‰ˆæœ¬å·æˆ–é…ç½®å‚æ•°

## ç¤ºä¾‹ä»£ç 

### å®Œæ•´ç¤ºä¾‹

å‚è€ƒä»¥ä¸‹æ–‡ä»¶ï¼š

1. **`examples/dataset_cache_example.py`** - 6ä¸ªä½¿ç”¨ç¤ºä¾‹
   - åŸºæœ¬ç¼“å­˜ä½¿ç”¨
   - ä»ç¼“å­˜åŠ è½½
   - ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨
   - æ¸…é™¤ç¼“å­˜
   - ä¸åŒç¼“å­˜æ ¼å¼
   - å¼ºåˆ¶é‡å»º

2. **`examples/mnist_with_cache_demo.py`** - MNISTæ¼”ç¤º
   - æ€§èƒ½å¯¹æ¯”æµ‹è¯•
   - ç¼“å­˜ç®¡ç†æ¼”ç¤º
   - ç‰ˆæœ¬ç®¡ç†æ¼”ç¤º

### è¿è¡Œç¤ºä¾‹

```bash
# åŸºæœ¬ç¤ºä¾‹
conda run -n ntrain python examples/dataset_cache_example.py

# MNISTæ¼”ç¤º
conda run -n ntrain python examples/mnist_with_cache_demo.py
```

## ç›¸å…³æ–‡æ¡£

- ğŸ“š [å®Œæ•´APIæ–‡æ¡£](dataset_cache.md)
- ğŸ“„ [åŠŸèƒ½æ€»ç»“](cache_feature_summary.md)
- ğŸ“ [æ›´æ–°æ—¥å¿—](../CACHE_FEATURE_CHANGELOG.md)

## æ”¯æŒå’Œåé¦ˆ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š
1. æŸ¥çœ‹ [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
2. è¿è¡Œæµ‹è¯•éªŒè¯: `python tests/test_dataset_cache.py`
3. æäº¤Issueæˆ–Pull Request

---

**ç‰ˆæœ¬**: 1.0.0  
**æ›´æ–°æ—¥æœŸ**: 2025-10-29  
**æµ‹è¯•çŠ¶æ€**: âœ… 10/10 é€šè¿‡

