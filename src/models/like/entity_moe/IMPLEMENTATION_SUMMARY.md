# Vision Models with EntityMoE - å®ç°æ€»ç»“

## ğŸ“‹ å®ç°æ¦‚è¿°

å·²æˆåŠŸå®ç°åŸºäº **timm** å’Œ **transformers** åº“çš„å››ç§ä¸»æµè§†è§‰æ¨¡å‹ä¸ EntityMoE çš„é›†æˆï¼Œæ”¯æŒåŠ è½½é¢„è®­ç»ƒæƒé‡å¹¶çµæ´»æ³¨å…¥ EntityMoE å±‚ã€‚

## âœ… å·²å®Œæˆçš„åŠŸèƒ½

### 1. æ ¸å¿ƒæ¨¡å‹æ”¯æŒ

| æ¨¡å‹ | æ•°æ®æº | çŠ¶æ€ | é¢„å®šä¹‰å‡½æ•° |
|------|--------|------|-----------|
| **ViT** (Vision Transformer) | timm | âœ… | `vit_base_entity_moe`, `vit_large_entity_moe` |
| **Swin Transformer** | timm | âœ… | `swin_tiny_entity_moe`, `swin_base_entity_moe` |
| **ResNet** | timm | âœ… | `resnet50_entity_moe`, `resnet101_entity_moe` |
| **SAM** (Segment Anything) | transformers | âœ… | `sam_vit_base_entity_moe`, `sam_vit_large_entity_moe`, `sam_vit_huge_entity_moe` |

### 2. EntityMoE é›†æˆæ–¹å¼

é‡‡ç”¨ **åŒ…è£…å™¨æ¨¡å¼**ï¼ˆ`EntityMoEWrapper`ï¼‰å®ç°ï¼š

```python
åŸå§‹æ¨¡å‹å±‚ â†’ EntityMoE å¢å¼º â†’ è¾“å‡º
    â†“
å¯å­¦ä¹ èåˆæƒé‡ (alpha)
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ— éœ€ä¿®æ”¹åŸå§‹æ¨¡å‹ç»“æ„
- âœ… å¯åŠ è½½é¢„è®­ç»ƒæƒé‡
- âœ… æ”¯æŒçµæ´»çš„å±‚çº§æ³¨å…¥
- âœ… ä¿æŒä¸åŸæ¨¡å‹æ¥å£ä¸€è‡´

### 3. çµæ´»çš„æ³¨å…¥ç­–ç•¥

#### ViT
```python
inject_layers='all'          # æ‰€æœ‰ Transformer blocks
inject_layers='last'         # æœ€åä¸€å±‚
inject_layers=[9, 10, 11]    # æŒ‡å®šå±‚ç´¢å¼•
```

#### Swin Transformer
```python
inject_layers='all'              # æ‰€æœ‰ blocks
inject_layers='last_stage'       # æœ€åä¸€ä¸ª stage
inject_layers=[(3, 0), (3, 1)]   # (stage_idx, block_idx)
```

#### ResNet
```python
inject_layers='all'                      # æ‰€æœ‰ layers
inject_layers='layer4'                   # æŒ‡å®š layer
inject_layers=[('layer4', 0), ('layer4', 1)]  # (layer_name, block_idx)
```

#### SAM
```python
inject_layers='all'              # æ‰€æœ‰ encoder layers
inject_layers='last_half'        # ååŠéƒ¨åˆ†
inject_layers=[6, 7, 8, 9, 10, 11]  # æŒ‡å®šå±‚ç´¢å¼•
```

### 4. ä¸»è¦ API å‡½æ•°

#### é€šç”¨åˆ›å»ºå‡½æ•°
```python
create_vit_entity_moe(model_name, pretrained=True, ...)
create_swin_entity_moe(model_name, pretrained=True, ...)
create_resnet_entity_moe(model_name, pretrained=True, ...)
create_sam_entity_moe(model_name, pretrained=True, ...)
```

#### é¢„å®šä¹‰å¿«æ·å‡½æ•°
```python
# ViT
vit_base_entity_moe(pretrained=True, num_classes=1000)
vit_large_entity_moe(pretrained=True, num_classes=1000)

# Swin
swin_tiny_entity_moe(pretrained=True, num_classes=1000)
swin_base_entity_moe(pretrained=True, num_classes=1000)

# ResNet
resnet50_entity_moe(pretrained=True, num_classes=1000)
resnet101_entity_moe(pretrained=True, num_classes=1000)

# SAM
sam_vit_base_entity_moe(pretrained=True)
sam_vit_large_entity_moe(pretrained=True)
sam_vit_huge_entity_moe(pretrained=True)
```

#### å·¥å…·å‡½æ•°
```python
print_model_info(model)  # æ‰“å° EntityMoE é…ç½®ä¿¡æ¯
```

### 5. å¯é…ç½®å‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `pretrained` | æ˜¯å¦åŠ è½½é¢„è®­ç»ƒæƒé‡ | `True` |
| `num_classes` | åˆ†ç±»ç±»åˆ«æ•° | `1000` |
| `num_experts` | MoE ä¸“å®¶æ•°é‡ | `4` |
| `num_experts_shared` | å…±äº«ä¸“å®¶æ•°é‡ | `2` |
| `expert_k` | ç¨€ç–è·¯ç”±æ¿€æ´»ä¸“å®¶æ•° | `1` |
| `dropout` | Dropout æ¯”ä¾‹ | `0.1` |
| `mlp_ratio` | MLP éšè—å±‚å€æ•° | `4.0` |
| `inject_layers` | æ³¨å…¥ä½ç½®ç­–ç•¥ | æ¨¡å‹ç›¸å…³ |

## ğŸ“ æ–‡ä»¶ç»“æ„

```
src/models/like/entity_moe/
â”œâ”€â”€ EntityMoe.py              # EntityMoE æ ¸å¿ƒå®ç°
â”œâ”€â”€ vit_entity_moe.py         # ä¸»å®ç°æ–‡ä»¶ï¼ˆæ–°å¢ï¼‰
â”œâ”€â”€ README.md                 # è¯¦ç»†ä½¿ç”¨æ–‡æ¡£ï¼ˆæ–°å¢ï¼‰
â””â”€â”€ IMPLEMENTATION_SUMMARY.md # æœ¬æ–‡ä»¶ï¼ˆæ–°å¢ï¼‰

examples/
â””â”€â”€ entity_moe_example.py     # ä½¿ç”¨ç¤ºä¾‹ï¼ˆæ–°å¢ï¼‰
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### 1. å›¾åƒåˆ†ç±»
```python
from models.like.entity_moe.vit_entity_moe import vit_base_entity_moe

model = vit_base_entity_moe(pretrained=True, num_classes=1000)
output = model(images)  # (B, 1000)
```

### 2. ç‰¹å¾æå–
```python
model = vit_base_entity_moe(pretrained=True, num_classes=0)
features = model.forward_features(images)  # æå–ç‰¹å¾
```

### 3. è¿ç§»å­¦ä¹ 
```python
# å†»ç»“ä¸»å¹²ç½‘ç»œï¼Œåªè®­ç»ƒ EntityMoE å’Œåˆ†ç±»å¤´
for name, param in model.named_parameters():
    if 'entity_moe' not in name and 'head' not in name:
        param.requires_grad = False
```

### 4. å›¾åƒåˆ†å‰²ï¼ˆä½¿ç”¨ SAMï¼‰
```python
sam_model = sam_vit_base_entity_moe(pretrained=True)
output = sam_model(pixel_values=images)
vision_features = output.vision_outputs.last_hidden_state
```

## ğŸ”§ æŠ€æœ¯ç‰¹ç‚¹

### 1. EntityMoEWrapper è®¾è®¡

```python
class EntityMoEWrapper(nn.Module):
    def __init__(self, original_layer, dim, ...):
        self.original_layer = original_layer
        self.entity_moe = ObjectMoELayer(...)
        self.alpha = nn.Parameter(torch.ones(1) * 0.1)  # å¯å­¦ä¹ èåˆ
    
    def forward(self, x, *args, **kwargs):
        # 1. åŸå§‹å±‚å¤„ç†
        out = self.original_layer(x, *args, **kwargs)
        
        # 2. EntityMoE å¢å¼º
        moe_out = self.entity_moe(out)
        
        # 3. å¯å­¦ä¹ èåˆ
        enhanced_out = out + self.alpha * moe_out
        
        return enhanced_out
```

**å…³é”®è®¾è®¡ç‚¹**ï¼š
- âœ… ä¿æŒåŸå§‹å±‚çš„è¾“å…¥è¾“å‡ºæ¥å£
- âœ… è‡ªåŠ¨å¤„ç†ä¸åŒç»´åº¦çš„å¼ é‡ï¼ˆ3D/4Dï¼‰
- âœ… å¯å­¦ä¹ çš„èåˆæƒé‡ `alpha`
- âœ… æ”¯æŒè¿”å›å…ƒç»„çš„å±‚ï¼ˆå¦‚æ³¨æ„åŠ›å±‚ï¼‰

### 2. è‡ªé€‚åº”ç»´åº¦å¤„ç†

```python
if main_out.dim() == 3:  # (B, N, C) - Transformer
    moe_input = main_out.unsqueeze(1)  # (B, 1, N, C)
    
elif main_out.dim() == 4:  # (B, C, H, W) - CNN
    spatial_flat = main_out.view(B, C, H*W).transpose(1, 2)
    moe_input = spatial_flat.unsqueeze(1)
```

### 3. æ¨¡å‹ä¿¡æ¯è¿½è¸ª

æ¯ä¸ªæ¨¡å‹éƒ½ä¼šè®°å½• EntityMoE é…ç½®ï¼š

```python
model.entity_moe_config = {
    'model_type': 'vit',
    'base_model': 'vit_base_patch16_224',
    'inject_layers': [11],
    'num_experts': 4,
    'num_experts_shared': 2,
    'expert_k': 1,
}
```

## ğŸ“Š æ€§èƒ½è€ƒè™‘

### è®¡ç®—å¼€é”€åˆ†æ

| æ¨¡å‹ | åŸºç¡€å‚æ•° | EntityMoE å¢åŠ  | æ€»è®¡ |
|------|---------|---------------|------|
| ViT-Base (1å±‚) | ~86M | ~2-3M | ~88-89M |
| Swin-Tiny (æœ€åstage) | ~28M | ~1-2M | ~29-30M |
| ResNet-50 (layer4) | ~25M | ~3-4M | ~28-29M |
| SAM-ViT-Base (ååŠ) | ~90M | ~3-5M | ~93-95M |

### ä¼˜åŒ–å»ºè®®

1. **å‡å°‘æ³¨å…¥å±‚æ•°**ï¼š
   - ç®€å•ä»»åŠ¡ï¼šåªåœ¨æœ€åå‡ å±‚æ³¨å…¥
   - å¤æ‚ä»»åŠ¡ï¼šåœ¨å…³é”®å±‚æ³¨å…¥

2. **è°ƒæ•´ä¸“å®¶æ•°é‡**ï¼š
   - `num_experts=2-4`ï¼šé€‚åˆå¤§å¤šæ•°ä»»åŠ¡
   - `expert_k=1`ï¼šä¿æŒç¨€ç–æ€§

3. **ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ**ï¼š
   ```python
   from torch.cuda.amp import autocast
   with autocast():
       outputs = model(images)
   ```

## ğŸ” æµ‹è¯•éªŒè¯

### æ¨¡å—å¯¼å…¥æµ‹è¯•
```bash
python -c "
from models.like.entity_moe.vit_entity_moe import *
print('âœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ')
"
```

### å®Œæ•´ç¤ºä¾‹æµ‹è¯•
```bash
python examples/entity_moe_example.py
```

## ğŸ“š ä¾èµ–è¦æ±‚

```bash
# æ ¸å¿ƒä¾èµ–
pip install torch torchvision

# ViT, Swin, ResNet æ”¯æŒ
pip install timm

# SAM æ”¯æŒ
pip install transformers

# å¯é€‰ï¼šåŠ é€Ÿè®­ç»ƒ
pip install accelerate
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ€ç®€ç¤ºä¾‹

```python
# 1. å¯¼å…¥
from models.like.entity_moe.vit_entity_moe import vit_base_entity_moe
import torch

# 2. åˆ›å»ºæ¨¡å‹
model = vit_base_entity_moe(
    pretrained=True,      # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
    num_classes=1000,     # ImageNet åˆ†ç±»
    inject_layers='last'  # åªåœ¨æœ€åä¸€å±‚æ³¨å…¥
)

# 3. æ¨ç†
x = torch.randn(1, 3, 224, 224)
output = model(x)
print(output.shape)  # (1, 1000)
```

## ğŸ“ è¿›é˜¶ç”¨æ³•

### 1. å¤šé˜¶æ®µæ³¨å…¥

```python
# åœ¨å¤šä¸ªä½ç½®æ³¨å…¥ EntityMoE
model = create_vit_entity_moe(
    model_name='vit_base_patch16_224',
    inject_layers=[6, 9, 11],  # åœ¨æµ…ã€ä¸­ã€æ·±å±‚éƒ½æ³¨å…¥
)
```

### 2. å·®å¼‚åŒ–å­¦ä¹ ç‡

```python
optimizer = torch.optim.AdamW([
    {'params': backbone_params, 'lr': 1e-5},
    {'params': entitymoe_params, 'lr': 1e-4},
    {'params': head_params, 'lr': 1e-3},
])
```

### 3. åŠ¨æ€ä¸“å®¶æ•°é‡

```python
# æµ…å±‚ç”¨å°‘é‡ä¸“å®¶ï¼Œæ·±å±‚ç”¨æ›´å¤šä¸“å®¶
# éœ€è¦æ‰‹åŠ¨å®ç°ï¼Œä½†æ¡†æ¶æ”¯æŒ
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **å†…å­˜å ç”¨**ï¼šEntityMoE ä¼šå¢åŠ æ˜¾å­˜å ç”¨ï¼Œå»ºè®®ä»å°‘é‡å±‚å¼€å§‹
2. **è®­ç»ƒæ—¶é—´**ï¼šMoE ä¼šå¢åŠ è®­ç»ƒæ—¶é—´ï¼Œä½†å¯ä»¥æå‡æ¨¡å‹å®¹é‡
3. **é¢„è®­ç»ƒæƒé‡**ï¼šé¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨ä¸‹è½½ï¼Œéœ€è¦è‰¯å¥½çš„ç½‘ç»œè¿æ¥
4. **å…¼å®¹æ€§**ï¼šç¡®ä¿ timm å’Œ transformers ç‰ˆæœ¬è¾ƒæ–°

## ğŸ”— ç›¸å…³èµ„æº

- **EntityMoE è®ºæ–‡**ï¼š[å¾…è¡¥å……]
- **timm æ–‡æ¡£**ï¼šhttps://github.com/huggingface/pytorch-image-models
- **transformers æ–‡æ¡£**ï¼šhttps://huggingface.co/docs/transformers
- **SAM è®ºæ–‡**ï¼šhttps://arxiv.org/abs/2304.02643

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æå‡ºé—®é¢˜å’Œæ”¹è¿›å»ºè®®ï¼

## ğŸ“„ è®¸å¯

éµå¾ªé¡¹ç›®ä¸»è®¸å¯è¯ã€‚

---

**æœ€åæ›´æ–°**: 2025-10-28
**ç‰ˆæœ¬**: 1.0.0
**çŠ¶æ€**: âœ… ç¨³å®šå¯ç”¨
