{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5af83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple\n",
      "Collecting peft\n",
      "  Downloading https://mirrors.ustc.edu.cn/pypi/packages/68/85/8e6ea3d1089f2b6de3c1cd34bbbd7560912af9d34b057be3b8b8fefe1da3/peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "     ---------------------------------------- 0.0/411.1 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/411.1 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/411.1 kB ? eta -:--:--\n",
      "     -- ---------------------------------- 30.7/411.1 kB 186.2 kB/s eta 0:00:03\n",
      "     -- ---------------------------------- 30.7/411.1 kB 186.2 kB/s eta 0:00:03\n",
      "     -- ---------------------------------- 30.7/411.1 kB 186.2 kB/s eta 0:00:03\n",
      "     -- ---------------------------------- 30.7/411.1 kB 186.2 kB/s eta 0:00:03\n",
      "     -- ---------------------------------- 30.7/411.1 kB 186.2 kB/s eta 0:00:03\n",
      "     -- ---------------------------------- 30.7/411.1 kB 186.2 kB/s eta 0:00:03\n",
      "     -- ---------------------------------- 30.7/411.1 kB 186.2 kB/s eta 0:00:03\n",
      "     ------ ------------------------------ 71.7/411.1 kB 140.3 kB/s eta 0:00:03\n",
      "     ------ ------------------------------ 71.7/411.1 kB 140.3 kB/s eta 0:00:03\n",
      "     ------ ------------------------------ 71.7/411.1 kB 140.3 kB/s eta 0:00:03\n",
      "     --------- -------------------------- 112.6/411.1 kB 181.9 kB/s eta 0:00:02\n",
      "     --------- -------------------------- 112.6/411.1 kB 181.9 kB/s eta 0:00:02\n",
      "     --------- -------------------------- 112.6/411.1 kB 181.9 kB/s eta 0:00:02\n",
      "     --------- -------------------------- 112.6/411.1 kB 181.9 kB/s eta 0:00:02\n",
      "     ------------ ----------------------- 143.4/411.1 kB 177.5 kB/s eta 0:00:02\n",
      "     --------------- -------------------- 174.1/411.1 kB 205.6 kB/s eta 0:00:02\n",
      "     --------------- -------------------- 174.1/411.1 kB 205.6 kB/s eta 0:00:02\n",
      "     --------------- -------------------- 174.1/411.1 kB 205.6 kB/s eta 0:00:02\n",
      "     ------------------- ---------------- 225.3/411.1 kB 237.1 kB/s eta 0:00:01\n",
      "     ------------------- ---------------- 225.3/411.1 kB 237.1 kB/s eta 0:00:01\n",
      "     -------------------- --------------- 235.5/411.1 kB 218.3 kB/s eta 0:00:01\n",
      "     ---------------------- ------------- 256.0/411.1 kB 231.2 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 276.5/411.1 kB 247.0 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 307.2/411.1 kB 260.3 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 317.4/411.1 kB 255.4 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 368.6/411.1 kB 286.7 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 389.1/411.1 kB 295.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ 411.1/411.1 kB 305.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from peft) (2.1.2)\n",
      "Requirement already satisfied: transformers in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from peft) (4.45.2)\n",
      "Requirement already satisfied: tqdm in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from peft) (4.66.5)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading https://mirrors.ustc.edu.cn/pypi/packages/91/d9/e044c9d42d8ad9afa96533b46ecc9b7aea893d362b3c52bd78fb9fe4d7b3/accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "     ---------------------------------------- 0.0/365.3 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 30.7/365.3 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 30.7/365.3 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 30.7/365.3 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 30.7/365.3 kB ? eta -:--:--\n",
      "     ------------------- ---------------- 194.6/365.3 kB 908.0 kB/s eta 0:00:01\n",
      "     ------------------- ---------------- 194.6/365.3 kB 908.0 kB/s eta 0:00:01\n",
      "     ------------------- ---------------- 194.6/365.3 kB 908.0 kB/s eta 0:00:01\n",
      "     ---------------------- ------------- 225.3/365.3 kB 625.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ 365.3/365.3 kB 910.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: safetensors in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from peft) (0.4.4)\n",
      "Collecting huggingface_hub>=0.25.0 (from peft)\n",
      "  Downloading https://mirrors.ustc.edu.cn/pypi/packages/d0/fb/5307bd3612eb0f0e62c3a916ae531d3a31e58fb5c82b58e3ebf7fd6f47a1/huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
      "     ---------------------------------------- 0.0/515.4 kB ? eta -:--:--\n",
      "     -- ------------------------------------ 30.7/515.4 kB 1.4 MB/s eta 0:00:01\n",
      "     -- ---------------------------------- 41.0/515.4 kB 653.6 kB/s eta 0:00:01\n",
      "     -- ---------------------------------- 41.0/515.4 kB 653.6 kB/s eta 0:00:01\n",
      "     -- ---------------------------------- 41.0/515.4 kB 653.6 kB/s eta 0:00:01\n",
      "     ----- ------------------------------- 71.7/515.4 kB 281.8 kB/s eta 0:00:02\n",
      "     -------- --------------------------- 122.9/515.4 kB 481.4 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 143.4/515.4 kB 448.2 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 153.6/515.4 kB 459.5 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 153.6/515.4 kB 459.5 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 153.6/515.4 kB 459.5 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 153.6/515.4 kB 459.5 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 153.6/515.4 kB 459.5 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 153.6/515.4 kB 459.5 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 153.6/515.4 kB 459.5 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 399.4/515.4 kB 579.0 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 399.4/515.4 kB 579.0 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 399.4/515.4 kB 579.0 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 399.4/515.4 kB 579.0 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 399.4/515.4 kB 579.0 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 399.4/515.4 kB 579.0 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 399.4/515.4 kB 579.0 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 399.4/515.4 kB 579.0 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 399.4/515.4 kB 579.0 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 399.4/515.4 kB 579.0 kB/s eta 0:00:01\n",
      "     ------------------------------------ 515.4/515.4 kB 455.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2024.3.1)\n",
      "Requirement already satisfied: requests in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.11.0)\n",
      "Requirement already satisfied: sympy in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: colorama in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from transformers->peft) (0.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.12.14)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\program\\anaconda\\envs\\py310\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Installing collected packages: huggingface_hub, accelerate, peft\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.24.5\n",
      "    Uninstalling huggingface-hub-0.24.5:\n",
      "      Successfully uninstalled huggingface-hub-0.24.5\n",
      "Successfully installed accelerate-1.8.1 huggingface_hub-0.24.6 peft-0.15.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae3ea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing huggingface_hub._snapshot_download: cannot import name 'GatedRepoError' from 'huggingface_hub.errors' (d:\\Program\\Anaconda\\envs\\py310\\lib\\site-packages\\huggingface_hub\\errors.py)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GatedRepoError' from 'huggingface_hub.errors' (d:\\Program\\Anaconda\\envs\\py310\\lib\\site-packages\\huggingface_hub\\errors.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m      4\u001b[0m     AutoTokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset  \u001b[38;5;66;03m# Dataset用于从列表创建数据集\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# --- 0. 配置参数 ---\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 替换为你要使用的基础模型路径或Hugging Face Hub ID\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 常用模型示例: \"THUDM/chatglm2-6b\", \"baichuan-inc/Baichuan2-7B-Base\", \"mistralai/Mistral-7B-v0.1\", \"gpt2\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\py310\\lib\\site-packages\\peft\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.15.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[0;32m     19\u001b[0m     AutoPeftModel,\n\u001b[0;32m     20\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[0;32m     21\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[0;32m     22\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[0;32m     23\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[0;32m     24\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[0;32m     25\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig, PromptLearningConfig\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001b[0;32m     30\u001b[0m     PEFT_TYPE_TO_MIXED_MODEL_MAPPING,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     inject_adapter_in_model,\n\u001b[0;32m     34\u001b[0m )\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\py310\\lib\\site-packages\\peft\\auto.py:31\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     AutoModel,\n\u001b[0;32m     23\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     AutoTokenizer,\n\u001b[0;32m     29\u001b[0m )\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     PeftModel,\n\u001b[0;32m     34\u001b[0m     PeftModelForCausalLM,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     PeftModelForTokenClassification,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\py310\\lib\\site-packages\\peft\\config.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CONFIG_NAME, PeftType, TaskType\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# we expect at least these keys to be present in a PEFT adapter_config.json\u001b[39;00m\n\u001b[0;32m     28\u001b[0m MIN_EXPECTED_CONFIG_KEYS \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_type\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\py310\\lib\\site-packages\\peft\\utils\\__init__.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_cache_to_layer_device_map\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloftq_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m replace_lora_weights_loftq\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mother\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     CONFIG_NAME,\n\u001b[0;32m     19\u001b[0m     INCLUDE_LINEAR_LAYERS_SHORTHAND,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     transpose,\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftType, TaskType, register_peft_method\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\py310\\lib\\site-packages\\peft\\utils\\loftq_utils.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clear_device_cache\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m snapshot_download\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HFValidationError, LocalEntryNotFoundError\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msafetensors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SafetensorError, safe_open\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\py310\\lib\\site-packages\\huggingface_hub\\__init__.py:520\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     51\u001b[0m # Alphabetical order of definitions is ensured in tests\n\u001b[0;32m     52\u001b[0m # WARNING: any comment added in this dictionary definition will be lost when\n\u001b[0;32m     53\u001b[0m # re-generating the file !\n\u001b[0;32m     54\u001b[0m _SUBMOD_ATTRS = {\n\u001b[0;32m     55\u001b[0m     \"_commit_scheduler\": [\n\u001b[0;32m     56\u001b[0m         \"CommitScheduler\",\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m     ],\n\u001b[0;32m    517\u001b[0m }\n\u001b[0;32m    519\u001b[0m # WARNING: __all__ is generated automatically, Any manual edit will be lost when re-generating this file !\n\u001b[1;32m--> 520\u001b[0m #\n\u001b[0;32m    521\u001b[0m # To update the static imports, please run the following command and commit the changes.\n\u001b[0;32m    522\u001b[0m # ```\n\u001b[0;32m    523\u001b[0m # # Use script\n\u001b[0;32m    524\u001b[0m # python utils/check_all_variable.py --update\n\u001b[0;32m    525\u001b[0m #\n\u001b[0;32m    526\u001b[0m # # Or run style on codebase\n\u001b[0;32m    527\u001b[0m # make style\n\u001b[0;32m    528\u001b[0m # ```\n\u001b[0;32m    530\u001b[0m __all__ = [\n\u001b[0;32m    531\u001b[0m     \"Agent\",\n\u001b[0;32m    532\u001b[0m     \"AsyncInferenceClient\",\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m     \"whoami\",\n\u001b[0;32m    935\u001b[0m ]\n\u001b[0;32m    938\u001b[0m def _attach(package_name, submodules=None, submod_attrs=None):\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\py310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program\\Anaconda\\envs\\py310\\lib\\site-packages\\huggingface_hub\\_snapshot_download.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     GatedRepoError,\n\u001b[0;32m     12\u001b[0m     HfHubHTTPError,\n\u001b[0;32m     13\u001b[0m     LocalEntryNotFoundError,\n\u001b[0;32m     14\u001b[0m     RepositoryNotFoundError,\n\u001b[0;32m     15\u001b[0m     RevisionNotFoundError,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_download\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m REGEX_COMMIT_HASH, hf_hub_download, repo_folder_name\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo, HfApi, ModelInfo, RepoFile, SpaceInfo\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'GatedRepoError' from 'huggingface_hub.errors' (d:\\Program\\Anaconda\\envs\\py310\\lib\\site-packages\\huggingface_hub\\errors.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset, Dataset  # Dataset用于从列表创建数据集\n",
    "\n",
    "# --- 0. 配置参数 ---\n",
    "# 替换为你要使用的基础模型路径或Hugging Face Hub ID\n",
    "# 常用模型示例: \"THUDM/chatglm2-6b\", \"baichuan-inc/Baichuan2-7B-Base\", \"mistralai/Mistral-7B-v0.1\", \"gpt2\"\n",
    "model_name_or_path = \"gpt2\"  # 示例使用GPT-2，你可以替换为更大的模型\n",
    "\n",
    "output_dir = \"./lora_finetuned_model_gpt2\"  # 模型保存路径\n",
    "\n",
    "# LoRA配置\n",
    "lora_r = 8  # LoRA的秩，影响适配器参数量和表达能力，越大参数越多\n",
    "lora_alpha = 16  # LoRA的缩放因子，通常设为 r 的2倍\n",
    "lora_dropout = 0.05  # LoRA层的Dropout率\n",
    "# target_modules: 关键！根据你使用的模型类型调整\n",
    "# 对于 Llama/Mistral/Baichuan 等: [\"q_proj\", \"k_proj\", \"v_proj\"] 是常见选择\n",
    "# 对于 ChatGLM2/3: [\"query_key_value\"]\n",
    "# 对于 GPT-2: [\"c_attn\"] (或者更全面地，可以包含 c_proj, mlp.c_fc, mlp.c_proj等)\n",
    "# 推荐使用 peft.tuners.lora.model.find_all_linear_names 来自动查找线性层\n",
    "# 或者查看模型结构 model.print_trainable_parameters() (运行到 get_peft_model 之后)\n",
    "lora_target_modules = [\"c_attn\"]  # GPT-2 的LoRA目标模块\n",
    "\n",
    "# 训练参数\n",
    "training_epochs = 3\n",
    "learning_rate = 2e-4\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 1  # 增加这个可以模拟更大的batch size，节省显存\n",
    "# QLoRA (4比特量化) 配置\n",
    "use_4bit_quantization = False  # 如果你的GPU显存较大，可以设置为False\n",
    "# 如果显存不够，强烈建议设置为 True，需要安装 bitsandbytes\n",
    "# 如果设置为True，优化器通常选 paged_adamw_8bit\n",
    "\n",
    "# --- 1. 数据准备 ---\n",
    "# 示例：创建一个简单的指令微调数据集\n",
    "# 实际应用中，你会从文件加载，例如 load_dataset('json', data_files={'train': 'your_data.jsonl'})\n",
    "raw_data = [\n",
    "    {\"instruction\": \"请问中国的首都是哪里？\", \"output\": \"中国的首都是北京。\"},\n",
    "    {\"instruction\": \"列举三个水果。\", \"output\": \"苹果、香蕉、橙子。\"},\n",
    "    {\n",
    "        \"instruction\": \"介绍一下LoRA微调技术。\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"LoRA（Low-Rank Adaptation）是一种参数高效的微调技术，它通过在预训练模型的每一层注入一小部分可训练的低秩矩阵来更新模型，而无需修改原始模型的权重。\",\n",
    "    },\n",
    "    {\"instruction\": \"计算1加1等于多少？\", \"input\": \"\", \"output\": \"1加1等于2。\"},\n",
    "]\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "\n",
    "# 定义数据格式化函数（指令微调通常需要特定的prompt格式）\n",
    "def format_instruction_dataset(example):\n",
    "    # Alpaca风格指令微调格式\n",
    "    if \"input\" in example and example[\"input\"]:\n",
    "        prompt = f\"### Instruction:\\n{example['instruction']}\\n### Input:\\n{example['input']}\\n### Output:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{example['instruction']}\\n### Output:\\n\"\n",
    "\n",
    "    return {\"text\": prompt + example[\"output\"]}\n",
    "\n",
    "\n",
    "# 应用格式化函数到数据集\n",
    "dataset = dataset.map(format_instruction_dataset)\n",
    "\n",
    "# --- 2. 加载基础模型和分词器 ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "# 检查并设置pad_token，某些模型可能没有\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\n",
    "        f\"Warning: tokenizer.pad_token was None, set to tokenizer.eos_token: {tokenizer.pad_token}\"\n",
    "    )\n",
    "\n",
    "# 根据是否使用4比特量化加载模型\n",
    "if use_4bit_quantization:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        load_in_4bit=True,  # 启用4比特量化\n",
    "        torch_dtype=torch.bfloat16,  # 计算数据类型，bfloat16在Ampere及更新架构上性能好\n",
    "        device_map=\"auto\",  # 自动分配到GPU\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    # 针对量化模型，PEFT 推荐使用 prepare_model_for_kbit_training\n",
    "    # 它会做一些处理，如设置梯度检查点、调整LayerNorm的dtype等\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.float16,  # 或 torch.bfloat16，根据GPU能力选择\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "model.config.use_cache = False  # 训练时禁用cache，可以节省显存\n",
    "\n",
    "# --- 3. 配置 LoRA (LoraConfig) ---\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # 任务类型：因果语言模型 (文本生成)\n",
    "    inference_mode=False,  # 训练模式\n",
    "    r=lora_r,  # LoRA秩\n",
    "    lora_alpha=lora_alpha,  # LoRA缩放因子\n",
    "    lora_dropout=lora_dropout,  # LoRA dropout\n",
    "    target_modules=lora_target_modules,  # 目标模块\n",
    "    bias=\"none\",  # 偏置项通常不参与微调\n",
    ")\n",
    "\n",
    "# --- 4. 准备 PEFT 模型 ---\n",
    "# 将基础模型和LoRA配置结合，生成PEFT模型\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 打印模型可训练参数量，你会发现它非常小！\n",
    "model.print_trainable_parameters()\n",
    "# 示例输出: trainable params: 393216 || all params: 124430400 || trainable%: 0.31601768832626075\n",
    "\n",
    "\n",
    "# --- 5. 数据 Tokenization ---\n",
    "def tokenize_function(examples):\n",
    "    # 对于Causal LM，我们通常拼接所有文本并设置 max_length\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,  # 根据模型和数据调整最大长度\n",
    "        padding=\"max_length\",  # 对齐到最大长度\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "        \"text\"\n",
    "    ],  # 移除原始文本列，只保留 tokenized 后的 input_ids, attention_mask\n",
    ")\n",
    "\n",
    "# --- 6. 设置训练器 (Trainer) ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=training_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=100,  # 每100步保存一次模型\n",
    "    save_total_limit=3,  # 最多保存3个检查点\n",
    "    fp16=True if torch.cuda.is_available() else False,  # 启用混合精度训练\n",
    "    bf16=(\n",
    "        True if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else False\n",
    "    ),\n",
    "    # 优化器选择：对于QLoRA，推荐 paged_adamw_8bit\n",
    "    optim=\"paged_adamw_8bit\" if use_4bit_quantization else \"adamw_torch\",\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",  # 可以将训练过程可视化到TensorBoard\n",
    ")\n",
    "\n",
    "# Data Collator 负责将 tokenize 后的数据批处理，并进行 padding\n",
    "# mlm=False 表示非掩码语言模型，适用于因果语言模型\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# --- 7. 开始训练 ---\n",
    "print(\"开始LoRA微调...\")\n",
    "trainer.train()\n",
    "print(\"LoRA微调完成！\")\n",
    "\n",
    "# --- 8. 保存 LoRA 适配器 ---\n",
    "# PEFT 库只会保存 LoRA 适配器（很小），而不是整个模型\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)  # 分词器也要保存\n",
    "print(f\"LoRA适配器已保存到: {output_dir}\")\n",
    "\n",
    "\n",
    "# --- 9. 加载 LoRA 适配器进行推理或合并 ---\n",
    "\n",
    "print(\"\\n--- 加载LoRA适配器进行推理 (推荐方式) ---\")\n",
    "from peft import PeftModel\n",
    "\n",
    "# 重新加载基础模型 (推理时不需要进行kbit训练，所以可以不加 prepare_model_for_kbit_training)\n",
    "# 注意：这里的dtype应与基础模型一致，或者使用fp16/bf16\n",
    "base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.float16,  # 推理时也建议使用fp16以节省显存\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 通过 from_pretrained 方法加载 LoRA 适配器到基础模型\n",
    "peft_model_for_inference = PeftModel.from_pretrained(\n",
    "    base_model_for_inference, output_dir\n",
    ")\n",
    "peft_model_for_inference.eval()  # 设置为评估模式\n",
    "\n",
    "# 进行推理\n",
    "prompt_text = \"### Instruction:\\n请问LoRA微调的优势是什么？\\n### Output:\\n\"\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")  # 确保输入在GPU上\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = peft_model_for_inference.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,  # 采样生成\n",
    "        top_p=0.9,  # Top-p 采样\n",
    "        temperature=0.7,  # 温度\n",
    "        repetition_penalty=1.1,  # 惩罚重复\n",
    "    )\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"原始prompt:\\n{prompt_text}\")\n",
    "print(f\"生成结果:\\n{generated_text}\")\n",
    "\n",
    "\n",
    "# --- 10. (可选) 将 LoRA 适配器合并回基础模型 ---\n",
    "# 合并后的模型不再是PEFT模型，而是标准的transformers模型，可以像普通模型一样部署。\n",
    "# 注意：如果基础模型是4bit量化的，merge_and_unload()会将其卸载为全精度(如fp16)，这将增加模型大小。\n",
    "print(\"\\n--- 合并LoRA适配器回基础模型 (生成完整模型文件) ---\")\n",
    "\n",
    "# 使用之前加载的peft_model_for_inference进行合并\n",
    "merged_model = peft_model_for_inference.merge_and_unload()\n",
    "\n",
    "# 保存合并后的完整模型 (这将保存所有参数，文件会很大，与原始基础模型大小相近)\n",
    "merged_output_dir = \"./merged_finetuned_model_gpt2\"\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "print(f\"合并后的完整模型已保存到: {merged_output_dir}\")\n",
    "\n",
    "# 现在可以像加载普通Transformers模型一样加载 merged_model\n",
    "# loaded_merged_model = AutoModelForCausalLM.from_pretrained(merged_output_dir, device_map=\"auto\")\n",
    "# loaded_tokenizer = AutoTokenizer.from_pretrained(merged_output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
