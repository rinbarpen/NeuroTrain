{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d691a499",
      "metadata": {},
      "source": [
        "# Medical Large Language Models Support\n",
        "\n",
        "This section adds support for medical LLMs:\n",
        "- Echelon-AI/Med-Qwen2-7B\n",
        "- LLavaMed-7B\n",
        "\n",
        "Testing with ISIC dataset for medical image analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e56b30",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yons/miniconda3/envs/ntrain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
        "# os.environ[\"HTTP_PROXY\"] = \"http://127.0.0.1:18080\"\n",
        "# os.environ[\"HTTPS_PROXY\"] = \"http://127.0.0.1:18080\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e3083714",
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'PRETRAINED_MODEL_DIR' from 'src.config' (/home/yons/workspace/sxy/lab/NeuroTrain/src/config.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m model_id \u001b[38;5;241m=\u001b[39m LLAVA_MODEL_ID_DEFAULT\n\u001b[1;32m      4\u001b[0m model, tokenizer, processor \u001b[38;5;241m=\u001b[39m build_transformers(model_id)\n",
            "File \u001b[0;32m~/workspace/sxy/lab/NeuroTrain/src/models/llm/transformers.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m image_to_PIL\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_history\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatHistory\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PRETRAINED_MODEL_DIR\n\u001b[1;32m     13\u001b[0m LLAVA_MODEL_ID_DEFAULT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllava-hf/llava-1.5-7b-hf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m CLIP_MODEL_ID_DEFAULT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'PRETRAINED_MODEL_DIR' from 'src.config' (/home/yons/workspace/sxy/lab/NeuroTrain/src/config.py)"
          ]
        }
      ],
      "source": [
        "from src.models.llm.transformers import *\n",
        "\n",
        "model_id = LLAVA_MODEL_ID_DEFAULT\n",
        "model, tokenizer, processor = build_transformers(model_id)\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "text = \"Hello, I am a student.\"\n",
        "input_ids = tokenizer(\n",
        "    text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128\n",
        ")[\"input_ids\"]\n",
        "input_ids = input_ids.to(\"cuda\")\n",
        "outputs = model(**input_ids)\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88497902",
      "metadata": {},
      "source": [
        "# Medical Large Language Models Support\n",
        "\n",
        "This section adds support for medical LLMs:\n",
        "- Echelon-AI/Med-Qwen2-7B\n",
        "- LLavaMed-7B\n",
        "\n",
        "Testing with ISIC dataset for medical image analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b8437e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Echelon-AI/Med-Qwen2-7B...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Med-Qwen2-7B loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Load Med-Qwen2-7B model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Med-Qwen2-7B is a text-only model\n",
        "MED_QWEN2_MODEL_ID = \"Echelon-AI/Med-Qwen2-7B\"\n",
        "CACHE_DIR = \"cache/models/pretrained\"\n",
        "\n",
        "\n",
        "def load_med_qwen2(model_id=MED_QWEN2_MODEL_ID):\n",
        "    \"\"\"Load Med-Qwen2-7B model and tokenizer\"\"\"\n",
        "    print(f\"Loading {model_id}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_id, \n",
        "        cache_dir=CACHE_DIR, \n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        cache_dir=CACHE_DIR,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "# Load the model\n",
        "med_qwen2_model, med_qwen2_tokenizer = load_med_qwen2()\n",
        "print(\"Med-Qwen2-7B loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7195e5f6",
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "The checkpoint you are trying to load has model type `llava_mistral` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1360\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1360\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1048\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m-> 1048\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m   1049\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
            "\u001b[0;31mKeyError\u001b[0m: 'llava_mistral'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/llava-med-v1.5-mistral-7b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache/models/pretrained/llava-med-v1.5-mistral-7b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLavaMed-7B loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:549\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    547\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 549\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1362\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m-> 1362\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1363\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1364\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1365\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not work, and the checkpoint is very new, then there may not be a release version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers from source with the command \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1370\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1371\u001b[0m         )\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `llava_mistral` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
          ]
        }
      ],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/llava-med-v1.5-mistral-7b\", dtype=\"auto\", cache_dir=\"cache/models/pretrained\")\n",
        "print(\"LLavaMed-7B loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70276748",
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "unmatched ')' (diffusion_dataset.py, line 484)",
          "output_type": "error",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
            "\u001b[0m  Cell \u001b[1;32mIn[4], line 3\u001b[0m\n    from src.dataset.medical.isic2018_dataset import ISIC2018Dataset\u001b[0m\n",
            "\u001b[0m  File \u001b[1;32m~/workspace/sxy/lab/NeuroTrain/src/dataset/__init__.py:1\u001b[0m\n    from .dataset import get_dataset, get_train_dataset, get_test_dataset, get_valid_dataset, get_train_valid_test_dataloader, get_all_dataloader\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m~/workspace/sxy/lab/NeuroTrain/src/dataset/dataset.py:11\u001b[0;36m\n\u001b[0;31m    from .diffusion_dataset import (\u001b[0;36m\n",
            "\u001b[0;36m  File \u001b[0;32m~/workspace/sxy/lab/NeuroTrain/src/dataset/diffusion_dataset.py:484\u001b[0;36m\u001b[0m\n\u001b[0;31m    ondition_type='label')\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
          ]
        }
      ],
      "source": [
        "# Load ISIC dataset for testing\n",
        "from pathlib import Path\n",
        "from src.dataset.medical.isic2018_dataset import ISIC2018Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set ISIC dataset path (adjust as needed)\n",
        "ISIC_DATA_PATH = Path(\"data/ISIC2018\")  # Adjust to your data path\n",
        "\n",
        "\n",
        "def tensor_to_pil(tensor):\n",
        "    \"\"\"Convert tensor to PIL Image\"\"\"\n",
        "    if isinstance(tensor, Image.Image):\n",
        "        return tensor\n",
        "\n",
        "    # Handle tensor format (C, H, W) or (H, W, C)\n",
        "    if tensor.dim() == 3:\n",
        "        if tensor.shape[0] == 3 or tensor.shape[0] == 1:  # (C, H, W)\n",
        "            tensor = tensor.permute(1, 2, 0)\n",
        "        # Now tensor is (H, W, C)\n",
        "        if tensor.shape[2] == 1:  # Grayscale\n",
        "            tensor = tensor.squeeze(2)\n",
        "\n",
        "    # Convert to numpy and denormalize if needed\n",
        "    img_array = tensor.cpu().numpy()\n",
        "    if img_array.max() <= 1.0:  # Normalized to [0, 1]\n",
        "        img_array = (img_array * 255).astype(\"uint8\")\n",
        "    else:\n",
        "        img_array = img_array.astype(\"uint8\")\n",
        "\n",
        "    # Convert to PIL Image\n",
        "    if len(img_array.shape) == 2:  # Grayscale\n",
        "        return Image.fromarray(img_array, mode=\"L\")\n",
        "    else:  # RGB\n",
        "        return Image.fromarray(img_array, mode=\"RGB\")\n",
        "\n",
        "\n",
        "def load_isic_dataset(split=\"test\", num_samples=5):\n",
        "    \"\"\"Load ISIC dataset samples for testing\"\"\"\n",
        "    try:\n",
        "        dataset = ISIC2018Dataset(ISIC_DATA_PATH, split=split, is_rgb=True)\n",
        "        print(f\"ISIC {split} dataset loaded: {len(dataset)} samples\")\n",
        "\n",
        "        # Get a few samples\n",
        "        samples = []\n",
        "        for i in range(min(num_samples, len(dataset))):\n",
        "            sample = dataset[i].copy()\n",
        "            # Convert tensor to PIL Image if needed\n",
        "            sample[\"image\"] = tensor_to_pil(sample[\"image\"])\n",
        "            if \"mask\" in sample and isinstance(sample[\"mask\"], torch.Tensor):\n",
        "                sample[\"mask\"] = tensor_to_pil(sample[\"mask\"])\n",
        "            samples.append(sample)\n",
        "\n",
        "        return samples\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ISIC dataset: {e}\")\n",
        "        print(f\"Please check if data exists at: {ISIC_DATA_PATH}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# Load test samples\n",
        "isic_samples = load_isic_dataset(split=\"test\", num_samples=5)\n",
        "print(f\"Loaded {len(isic_samples)} ISIC samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb4c491",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Med-Qwen2-7B with medical text questions\n",
        "def test_med_qwen2(model, tokenizer, questions):\n",
        "    \"\"\"Test Med-Qwen2-7B with medical questions\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for question in questions:\n",
        "        # Format prompt\n",
        "        prompt = f\"Question: {question}\\nAnswer:\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(\n",
        "            prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs, max_new_tokens=256, temperature=0.7, do_sample=True, top_p=0.9\n",
        "            )\n",
        "\n",
        "        # Decode\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        results.append({\"question\": question, \"answer\": response})\n",
        "\n",
        "        print(f\"\\nQuestion: {question}\")\n",
        "        print(f\"Answer: {response}\\n\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Test with medical questions related to skin lesions\n",
        "medical_questions = [\n",
        "    \"What are the common characteristics of malignant skin lesions?\",\n",
        "    \"How do you differentiate between benign and malignant skin lesions?\",\n",
        "    \"What is the ABCDE rule for melanoma detection?\",\n",
        "    \"Describe the typical features of a dysplastic nevus.\",\n",
        "]\n",
        "\n",
        "print(\"Testing Med-Qwen2-7B with medical questions...\")\n",
        "med_qwen2_results = test_med_qwen2(\n",
        "    med_qwen2_model, med_qwen2_tokenizer, medical_questions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63db805f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test LLavaMed-7B with ISIC images\n",
        "def test_llavamed(model, processor, images, questions):\n",
        "    \"\"\"Test LLavaMed-7B with medical images and questions\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for i, (image, question) in enumerate(zip(images, questions)):\n",
        "        try:\n",
        "            # Prepare inputs - LLaVA-Med uses specific prompt format\n",
        "            prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
        "\n",
        "            inputs = processor(prompt, image, return_tensors=\"pt\")\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = {\n",
        "                    k: v.to(model.device) if isinstance(v, torch.Tensor) else v\n",
        "                    for k, v in inputs.items()\n",
        "                }\n",
        "\n",
        "            # Generate\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs, max_new_tokens=256, temperature=0.7, do_sample=True\n",
        "                )\n",
        "\n",
        "            # Decode\n",
        "            response = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "            results.append({\"image_idx\": i, \"question\": question, \"answer\": response})\n",
        "\n",
        "            print(f\"\\nImage {i+1} - Question: {question}\")\n",
        "            print(f\"Answer: {response}\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {i}: {e}\")\n",
        "            results.append(\n",
        "                {\"image_idx\": i, \"question\": question, \"answer\": f\"Error: {e}\"}\n",
        "            )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Prepare questions for ISIC images\n",
        "isic_questions = [\n",
        "    \"What do you observe in this skin lesion image?\",\n",
        "    \"Describe the characteristics of this skin lesion.\",\n",
        "    \"What are the potential diagnostic features visible in this image?\",\n",
        "    \"Analyze this dermatoscopic image and provide clinical observations.\",\n",
        "    \"What should be considered when evaluating this skin lesion?\",\n",
        "]\n",
        "\n",
        "if len(isic_samples) > 0:\n",
        "    print(\"Testing LLavaMed-7B with ISIC images...\")\n",
        "    isic_images = [sample[\"image\"] for sample in isic_samples[: len(isic_questions)]]\n",
        "    llavamed_results = test_llavamed(\n",
        "        llavamed_model,\n",
        "        llavamed_processor,\n",
        "        isic_images,\n",
        "        isic_questions[: len(isic_images)],\n",
        "    )\n",
        "else:\n",
        "    print(\"No ISIC samples available for testing. Please check the data path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13f1d28a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize ISIC samples with model predictions\n",
        "def visualize_isic_results(samples, results, num_samples=3):\n",
        "    \"\"\"Visualize ISIC images with model predictions\"\"\"\n",
        "    if num_samples == 0:\n",
        "        print(\"No samples to visualize\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 4 * num_samples))\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i in range(min(num_samples, len(samples), len(results))):\n",
        "        # Original image\n",
        "        img = samples[i][\"image\"]\n",
        "        if isinstance(img, Image.Image):\n",
        "            axes[i, 0].imshow(img)\n",
        "        else:\n",
        "            axes[i, 0].imshow(np.array(img))\n",
        "        axes[i, 0].set_title(f\"ISIC Sample {i+1} - Original Image\", fontsize=10)\n",
        "        axes[i, 0].axis(\"off\")\n",
        "\n",
        "        # Show mask or model response\n",
        "        if \"mask\" in samples[i] and samples[i][\"mask\"] is not None:\n",
        "            mask = samples[i][\"mask\"]\n",
        "            if isinstance(mask, Image.Image):\n",
        "                axes[i, 1].imshow(np.array(mask), cmap=\"gray\")\n",
        "            elif isinstance(mask, torch.Tensor):\n",
        "                mask_np = mask.cpu().numpy()\n",
        "                if mask_np.ndim == 3 and mask_np.shape[0] == 1:\n",
        "                    mask_np = mask_np[0]\n",
        "                axes[i, 1].imshow(mask_np, cmap=\"gray\")\n",
        "            else:\n",
        "                axes[i, 1].imshow(mask, cmap=\"gray\")\n",
        "            axes[i, 1].set_title(f\"Ground Truth Mask\", fontsize=10)\n",
        "            axes[i, 1].axis(\"off\")\n",
        "        else:\n",
        "            # Show model response as text\n",
        "            question = results[i].get(\"question\", \"N/A\")\n",
        "            answer = results[i].get(\"answer\", \"N/A\")\n",
        "            # Truncate answer for display\n",
        "            answer_short = answer[:300] + \"...\" if len(answer) > 300 else answer\n",
        "            text_content = f\"Question: {question}\\n\\nAnswer: {answer_short}\"\n",
        "            axes[i, 1].text(\n",
        "                0.5,\n",
        "                0.5,\n",
        "                text_content,\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                wrap=True,\n",
        "                fontsize=9,\n",
        "                bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5),\n",
        "            )\n",
        "            axes[i, 1].set_title(f\"LLavaMed-7B Response\", fontsize=10)\n",
        "            axes[i, 1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Visualize results if available\n",
        "if (\n",
        "    len(isic_samples) > 0\n",
        "    and \"llavamed_results\" in locals()\n",
        "    and len(llavamed_results) > 0\n",
        "):\n",
        "    visualize_isic_results(\n",
        "        isic_samples, llavamed_results, num_samples=min(3, len(isic_samples))\n",
        "    )\n",
        "else:\n",
        "    print(\"No results available for visualization.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ntrain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
