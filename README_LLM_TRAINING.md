# NeuroTrain LLM/VLM è®­ç»ƒæ¨¡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. å®‰è£…ä¾èµ–
proxy_on
uv pip install transformers datasets trl accelerate peft bitsandbytes deepspeed

# 2. å•å¡è®­ç»ƒ
python scripts/train_llm.py --config configs/llm_training_example.toml

# 3. å¤šå¡è®­ç»ƒ + DeepSpeed
torchrun --nproc_per_node=4 scripts/train_llm.py \
    --config configs/llm_training_example.toml \
    --deepspeed configs/deepspeed/ds_config_zero2.json
```

## âœ¨ ä¸»è¦ç‰¹æ€§

- âœ… **å¤šé˜¶æ®µè®­ç»ƒ**: é¢„è®­ç»ƒ â†’ SFT â†’ DPO â†’ PPO â†’ GRPO
- âœ… **LLM & VLM**: åŒæ—¶æ”¯æŒçº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€æ¨¡å‹
- âœ… **é«˜æ•ˆè®­ç»ƒ**: LoRA/QLoRAã€Gradient Checkpointingã€Flash Attention
- âœ… **åˆ†å¸ƒå¼**: DeepSpeed ZeROã€torchrun DDP
- âœ… **çµæ´»é…ç½®**: TOML/YAML/JSON é…ç½®æ–‡ä»¶

## ğŸ“ é¡¹ç›®ç»“æ„

```
src/training/llm/
â”œâ”€â”€ __init__.py           # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ config.py             # é…ç½®æ•°æ®ç±» (ModelConfig, StageConfig, TrainingPlan)
â”œâ”€â”€ utils.py              # å·¥å…·å‡½æ•° (æ¨¡å‹åŠ è½½ã€LoRAã€é‡åŒ–ã€reward)
â””â”€â”€ pipeline.py           # è®­ç»ƒç®¡çº¿ (LLMVLMTrainingPipeline)

configs/
â”œâ”€â”€ llm_training_example.toml    # LLM è®­ç»ƒç¤ºä¾‹ (SFT+DPO)
â”œâ”€â”€ vlm_training_example.toml    # VLM è®­ç»ƒç¤ºä¾‹
â””â”€â”€ deepspeed/
    â”œâ”€â”€ ds_config_zero2.json     # DeepSpeed ZeRO Stage 2
    â””â”€â”€ ds_config_zero3.json     # DeepSpeed ZeRO Stage 3

scripts/
â””â”€â”€ train_llm.py          # è®­ç»ƒå…¥å£è„šæœ¬

docs/
â””â”€â”€ LLM_VLM_TRAINING_GUIDE.md    # è¯¦ç»†æ–‡æ¡£
```

## ğŸ¯ æ”¯æŒçš„è®­ç»ƒé˜¶æ®µ

| é˜¶æ®µ | è¯´æ˜ | æ•°æ®æ ¼å¼ |
|------|------|---------|
| **Pretrain** | é¢„è®­ç»ƒ/ç»§ç»­é¢„è®­ç»ƒ | `{"text": "..."}` |
| **SFT** | ç›‘ç£å¾®è°ƒ/æŒ‡ä»¤å¾®è°ƒ | `{"instruction": "...", "output": "..."}` |
| **DPO** | ç›´æ¥åå¥½ä¼˜åŒ– | `{"prompt": "...", "chosen": "...", "rejected": "..."}` |
| **PPO** | è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– | `{"prompt": "..."}` + reward_fn |
| **GRPO** | ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– | `{"prompt": "..."}` + reward_fn |

## ğŸ“ é…ç½®ç¤ºä¾‹

```toml
task_name = "llama2_sft_dpo"
seed = 42

[model]
model_name_or_path = "meta-llama/Llama-2-7b-hf"
model_type = "llm"
dtype = "bfloat16"
use_lora = true
load_in_4bit = true

[model.lora_config]
r = 16
lora_alpha = 32

[datasets.sft]
dataset_path = "data/sft_instructions.jsonl"
max_length = 2048

[[stages]]
stage_type = "sft"
stage_name = "sft"
num_train_epochs = 3
per_device_train_batch_size = 4
gradient_accumulation_steps = 4
learning_rate = 2e-5
bf16 = true

[[stages]]
stage_type = "dpo"
stage_name = "dpo"
num_train_epochs = 2
dpo_beta = 0.1
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### LoRA/QLoRA å¾®è°ƒ

```toml
[model]
use_lora = true
load_in_4bit = true
[model.lora_config]
r = 16
lora_alpha = 32
target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
```

### DeepSpeed ZeRO

```bash
torchrun --nproc_per_node=4 scripts/train_llm.py \
    --config configs/llm_training_example.toml \
    --deepspeed configs/deepspeed/ds_config_zero2.json
```

### è‡ªå®šä¹‰ Reward å‡½æ•° (PPO/GRPO)

```python
# src/utils/rewards.py
def my_reward_function(prompts: list[str], responses: list[str]) -> list[float]:
    scores = []
    for prompt, response in zip(prompts, responses):
        # è‡ªå®šä¹‰è¯„åˆ†é€»è¾‘
        score = compute_score(prompt, response)
        scores.append(score)
    return scores
```

```toml
[[stages]]
stage_type = "ppo"
reward_function = "src.utils.rewards:my_reward_function"
```

### VLM è®­ç»ƒ

```toml
[model]
model_name_or_path = "llava-hf/llava-1.5-7b-hf"
model_type = "vlm"

[datasets.vlm_sft]
dataset_path = "data/vlm_instructions.jsonl"
image_field = "image"
```

## ğŸ“Š è®­ç»ƒè¾“å‡º

```
runs/llm/{task_name}/{timestamp}/
â”œâ”€â”€ sft_stage/
â”‚   â”œâ”€â”€ checkpoint-500/
â”‚   â”œâ”€â”€ checkpoint-1000/
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ tokenizer*
â”œâ”€â”€ dpo_stage/
â”‚   â””â”€â”€ ...
â””â”€â”€ ppo_stage/
    â””â”€â”€ ...
```

## ğŸ› å¸¸è§é—®é¢˜

### CUDA OOM
- å‡å° `per_device_train_batch_size`
- å¢å¤§ `gradient_accumulation_steps`
- å¯ç”¨ `gradient_checkpointing = true`
- ä½¿ç”¨ `load_in_4bit = true`

### è®­ç»ƒé€Ÿåº¦æ…¢
- ä½¿ç”¨ `bf16 = true`
- å¯ç”¨ DeepSpeed
- æ£€æŸ¥æ•°æ®åŠ è½½æ˜¯å¦ä¸ºç“¶é¢ˆ

### æ¨¡å‹ä¸‹è½½å¤±è´¥
```bash
proxy_on
python scripts/train_llm.py --config ...
```

## ğŸ“– è¯¦ç»†æ–‡æ¡£

æŸ¥çœ‹ [LLM_VLM_TRAINING_GUIDE.md](docs/LLM_VLM_TRAINING_GUIDE.md) è·å–å®Œæ•´æ–‡æ¡£ã€‚

## ğŸ“ ç¤ºä¾‹

### 1. LLaMA-2 SFT è®­ç»ƒ

```bash
python scripts/train_llm.py --config configs/llm_training_example.toml
```

### 2. LLaVA VLM è®­ç»ƒ

```bash
python scripts/train_llm.py --config configs/vlm_training_example.toml
```

### 3. å®Œæ•´ RLHF æµç¨‹ (SFTâ†’DPOâ†’PPO)

ç¼–è¾‘é…ç½®æ–‡ä»¶æ·»åŠ æ‰€æœ‰é˜¶æ®µï¼Œç„¶åï¼š

```bash
torchrun --nproc_per_node=4 scripts/train_llm.py \
    --config configs/full_rlhf_pipeline.toml \
    --deepspeed configs/deepspeed/ds_config_zero2.json
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯

æœ¬é¡¹ç›®éµå¾ªä¸ NeuroTrain ä¸»é¡¹ç›®ç›¸åŒçš„è®¸å¯åè®®ã€‚

