{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a30e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import os.path as osp\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa4e0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"cache/models/pretrained\"\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://192.168.196.249:18080\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://192.168.196.249:18080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d3b770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yons/miniconda3/envs/ntrain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:src.dataset.cache_manager:缓存目录: /home/yons/workspace/sxy/lab/NeuroTrain/cache/datasets/OIA-DDR/v1\n",
      "INFO:src.dataset.cache_manager:缓存目录: /home/yons/workspace/sxy/lab/NeuroTrain/cache/datasets/PMC-OA/v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[[255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [254, 254, 254,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [  0,   0,   0,  ..., 255, 255, 255],\n",
      "          [  0,   0,   0,  ..., 255, 255, 255],\n",
      "          [  0,   0,   0,  ..., 255, 255, 255]],\n",
      "\n",
      "         [[255, 255, 255,  ..., 254, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 254, 254,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [  0,   0,   0,  ..., 255, 255, 255],\n",
      "          [  0,   0,   0,  ..., 255, 255, 255],\n",
      "          [  0,   0,   0,  ..., 255, 255, 255]],\n",
      "\n",
      "         [[254, 254, 254,  ..., 255, 255, 255],\n",
      "          [250, 251, 251,  ..., 254, 255, 255],\n",
      "          [250, 251, 252,  ..., 251, 253, 255],\n",
      "          ...,\n",
      "          [  0,   0,   0,  ..., 255, 255, 255],\n",
      "          [  0,   0,   0,  ..., 255, 255, 255],\n",
      "          [  0,   0,   0,  ..., 255, 255, 255]]],\n",
      "\n",
      "\n",
      "        [[[255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [  3,   3,   3,  ...,   3,   3,   3],\n",
      "          [102, 102, 102,  ..., 102, 102, 102],\n",
      "          [184, 184, 184,  ..., 184, 184, 184]],\n",
      "\n",
      "         [[255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [  3,   3,   3,  ...,   3,   3,   3],\n",
      "          [102, 102, 102,  ..., 102, 102, 102],\n",
      "          [184, 184, 184,  ..., 184, 184, 184]],\n",
      "\n",
      "         [[255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          ...,\n",
      "          [  3,   3,   3,  ...,   3,   3,   3],\n",
      "          [102, 102, 102,  ..., 102, 102, 102],\n",
      "          [184, 184, 184,  ..., 184, 184, 184]]],\n",
      "\n",
      "\n",
      "        [[[118, 102,  68,  ...,  55,  65,  80],\n",
      "          [147, 123,  81,  ...,  59,  65,  74],\n",
      "          [100,  84,  54,  ...,  58,  58,  59],\n",
      "          ...,\n",
      "          [ 83,  80,  86,  ...,   5,   5,   5],\n",
      "          [ 59,  60,  67,  ...,   5,   5,   5],\n",
      "          [ 25,  31,  37,  ...,   5,   5,   5]],\n",
      "\n",
      "         [[114,  98,  64,  ...,  55,  65,  80],\n",
      "          [143, 119,  77,  ...,  59,  65,  74],\n",
      "          [ 98,  82,  52,  ...,  58,  58,  59],\n",
      "          ...,\n",
      "          [ 83,  80,  86,  ...,   6,   6,   6],\n",
      "          [ 59,  60,  67,  ...,   6,   6,   6],\n",
      "          [ 25,  31,  37,  ...,   6,   6,   6]],\n",
      "\n",
      "         [[115,  99,  65,  ...,  55,  65,  80],\n",
      "          [144, 120,  78,  ...,  59,  65,  74],\n",
      "          [ 99,  83,  53,  ...,  58,  58,  59],\n",
      "          ...,\n",
      "          [ 85,  82,  88,  ...,   8,   8,   8],\n",
      "          [ 61,  62,  69,  ...,   8,   8,   8],\n",
      "          [ 27,  33,  39,  ...,   8,   8,   8]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[249, 252, 254,  ..., 251, 253, 254],\n",
      "          [244, 246, 247,  ..., 247, 252, 249],\n",
      "          [130, 131, 131,  ..., 143, 223, 249],\n",
      "          ...,\n",
      "          [ 20,  19,  15,  ...,  27, 195, 255],\n",
      "          [ 14,  20,  18,  ...,  26, 197, 253],\n",
      "          [  6,  14,  12,  ...,  26, 197, 252]],\n",
      "\n",
      "         [[253, 254, 255,  ..., 251, 252, 252],\n",
      "          [251, 251, 251,  ..., 250, 253, 248],\n",
      "          [144, 143, 142,  ..., 149, 226, 251],\n",
      "          ...,\n",
      "          [ 57,  56,  50,  ...,  27, 193, 251],\n",
      "          [ 49,  56,  56,  ...,  26, 196, 251],\n",
      "          [ 41,  50,  51,  ...,  26, 197, 252]],\n",
      "\n",
      "         [[255, 255, 253,  ..., 252, 254, 254],\n",
      "          [247, 247, 246,  ..., 249, 252, 249],\n",
      "          [129, 128, 129,  ..., 145, 224, 248],\n",
      "          ...,\n",
      "          [ 13,  12,   9,  ...,  25, 190, 249],\n",
      "          [  4,  11,  11,  ...,  26, 196, 252],\n",
      "          [  0,   4,   4,  ...,  26, 197, 252]]],\n",
      "\n",
      "\n",
      "        [[[  0,   0,   0,  ...,  90,  90,  91],\n",
      "          [  0,   0,   0,  ...,  92,  89,  87],\n",
      "          [  1,   1,   0,  ...,  96,  89,  83],\n",
      "          ...,\n",
      "          [  2,   1,   0,  ...,   1,   0,   0],\n",
      "          [  0,   0,   0,  ...,   2,   1,   0],\n",
      "          [  0,   0,   0,  ...,   1,   1,   1]],\n",
      "\n",
      "         [[  0,   0,   0,  ..., 115, 113, 112],\n",
      "          [  0,   0,   0,  ..., 118, 114, 112],\n",
      "          [  1,   1,   0,  ..., 124, 118, 112],\n",
      "          ...,\n",
      "          [  4,   2,   1,  ...,   2,   2,   2],\n",
      "          [  0,   0,   0,  ...,   2,   1,   0],\n",
      "          [  0,   0,   0,  ...,   1,   1,   1]],\n",
      "\n",
      "         [[  0,   0,   0,  ...,  83,  84,  85],\n",
      "          [  0,   0,   0,  ...,  86,  84,  83],\n",
      "          [  1,   1,   0,  ...,  91,  86,  81],\n",
      "          ...,\n",
      "          [  2,   2,   2,  ...,   3,   4,   4],\n",
      "          [  7,   7,   7,  ...,   7,   8,   8],\n",
      "          [  5,   5,   5,  ...,   2,   3,   3]]],\n",
      "\n",
      "\n",
      "        [[[251, 247, 252,  ..., 255, 255, 255],\n",
      "          [249, 245, 249,  ..., 251, 252, 254],\n",
      "          [249, 244, 243,  ..., 240, 245, 246],\n",
      "          ...,\n",
      "          [255, 255, 255,  ..., 254, 254, 254],\n",
      "          [255, 255, 255,  ..., 254, 254, 254],\n",
      "          [255, 254, 254,  ..., 254, 254, 254]],\n",
      "\n",
      "         [[253, 250, 255,  ..., 253, 251, 252],\n",
      "          [251, 248, 253,  ..., 244, 245, 247],\n",
      "          [251, 247, 247,  ..., 233, 237, 238],\n",
      "          ...,\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255],\n",
      "          [255, 255, 255,  ..., 255, 255, 255]],\n",
      "\n",
      "         [[255, 255, 255,  ..., 251, 249, 250],\n",
      "          [255, 254, 254,  ..., 244, 245, 247],\n",
      "          [255, 254, 248,  ..., 235, 240, 240],\n",
      "          ...,\n",
      "          [249, 249, 250,  ..., 254, 254, 254],\n",
      "          [247, 248, 250,  ..., 253, 253, 253],\n",
      "          [246, 246, 249,  ..., 253, 253, 253]]]], dtype=torch.uint8), 'caption': ['(a) Axial-view MRI of prostate. (b) MRI overlay on axial TRUS. (c) Axial-view TRUS of prostate.', 'Experimental and computational analyses of xanthophylls in model\\nmembranes. Images of the equatorial cross section of the lipid vesicle\\ncontaining m-Zea (a) or Zea (b) recorded with increasing\\nlaser power. Imaging is based on fluorescence intensity and anisotropy\\n(a) or Raman scattering (b). On the right-hand side of (b), results\\nof the component analysis are shown. Molecular configurations all-trans, 9-cis, and 13-cis were identified (the component spectra are shown in Figure S5). (c) Free energy profiles (top) and\\nthe corresponding probability densities (bottom) for the tilt angle\\nbetween the polyene chain and the membrane normal (see Figure S7a) for Zea all-trans and its 9-cis and 13-cis isomers\\nin the DPPC bilayer. (d) Representative structures for the perpendicular\\norientation of all-trans Zea (left) and the horizontal\\norientations of its two cis isomers (right) in the\\nDPPC membrane.', 'A. Positron emission tomography-computed tomography of head. Red arrow on left indicates a high-density shadow at basal ganglia; B. Green arrow in frontal lobe indicates a high-density shadow, considered calcification from cysticercosis. C. Magnetic resonance imaging (MRI) of head. Anterior longitudinal fissure, suprasellar cisterna, anterior pontine cisterna, multiple abnormal signals in left hippocampus, imaging features of cysticercosis.', 'Antero-posterior radiographs of the lower extremities in standing postures.Natural standing posture represents excessive knee flexion of the longer (left) leg to adjust for the LLD (a). On the complete extension of the left knee, she was forced to stand on the toes of her affected right foot, causing plantar flexion of the foot of the shorter limb (b).', 'Temporal monitoring. Imaging allows for repeat measurements to quantify temporal changes in the same subject. (A) Serial monitoring of individual TB pulmonary lesions in the same mouse demonstrates dynamic and independent evolution. (B) Serial 11C-rifampin PET in a rabbit model of TB meningitis demonstrates spatially heterogeneous brain penetration that rapidly decreased as early as 2 weeks into treatment (adapted from Tucker et al. [29]). (C) 18F-FDS PET performed before and after initiation of antimicrobial treatment in a murine model of E. coli myositis can rapidly monitor treatment efficacy, demonstrating a PET signal proportionate to the bacterial burden. This method can also be used to detect therapeutic failures due to infections with multidrug-resistant, extended-spectrum β-lactamase (ESBL)-producing E. coli (adapted from Weinstein et al. [36]).', 'Histopathological examination of treatment efficacy in traumatic ONFH by H&E staining, and immunohistochemical staining and semi-quantitative analysis of CD105.(A) Representative image of a section of femoral head stained with H&E. Scale bar\\u200a=\\u200a200 µm. In the treated femoral head, the number of empty lacunae decreased and hematopoietic tissue partially recovered, accompanied by an increase in the number of OBs, which was most significant in animals that received HGF-transgenic MSCs. a. empty lacuna; b. OBs. (B) Bar graph (left panel) represents the ratio of bone marrow cells to the area of bone marrow. A second bar graph (right panel) represents the ratio of empty lacunae to the area of trabeculae. (C) Immunohistochemical assays of CD105 expression. Scale bar\\u200a=\\u200a20 µm. (D) Bar graphs represent the expression density of CD105 as unit area of bone marrow or unit area of bone trabeculae. After MSC transplantation, the expression of CD105 increased compared with that of untreated ONFH group, indicating the occurrence of revascularization. Quantification was based on at least 10 fields per section. *P<0.05.', 'CK1α is required for polytene pairing in Drosophila salivary glands.(A-B) Salivary gland nuclei from control larvae (43B>Gal4; Gal4 under a salivary gland specific driver) (A) and larvae expressing hairpin RNAi to CK1α (CK1α\\nRNAi) driven by 43B>Gal4 (B) were hybridized with FISH probes specific to a region of Chromosome 2L (green) and Chromosome X (red) and counterstained with DAPI (blue). Chromosomes are highly paired in control (fewer FISH foci) (A) nuclei whereas expression of a TRiP hairpin RNAi targeting CK1α (B) induces unpairing of the chromosomes (multiple FISH foci). (C) Histogram showing average number of FISH spots per nucleus after CK1α RNAi is expressed in the salivary glands. CK1α depletion results in an increase in number of FISH spots for both 2L and X probes; (n = 24–38 nuclei per genotype). *p-value < 7.2x10−7 (calculated by using students’ t-test in excel). Error bars indicate SEM. (A-B) DAPI channel image is a single z-slice from the nucleus and FISH channel images are from maximum projection image of multiple z-slices. Scale, 10μm.', 'Volume rendering results of cerebral vessels. From left to right: The contour enhancement based on gradient; The contour enhancement based on curvature; The boundary enhancement based on depth.', 'Electrograms showing the sudden change in CS activation from distal-to-proximal to proximal-to-distal (indicated by red arrows) when pacing from the multipolar catheter (PV) in the left atrial appendage, indicating mitral isthmus block. Abl indicates ablation catheter; CSd , coronary sinus distal bipole and CSp, coronary sinus proximal bipole.', 'Microscopic findings of the tumor. a A cross-section of the tumor. Upper-case alphabets with windows correspond to microscopic images labeled with lower-case alphabets. b Image around the capsule showing hemorrhagic area along with rim calcification inside the fibrous capsule (×\\u200910 hematoxylin and eosin stain; white arrows indicate calcifications). c Image around another area of the capsule showing viable cancer cells beneath the capsule (×\\u200910 hematoxylin and eosin stain). d Image around the tumor septum showing calcification and viable cancer cells (×\\u200910 hematoxylin and eosin stain; white arrows indicate calcifications)', 'Transmission electron micrographs of field samples of Chamaesiphon spp. a–c\\nC. starmachii from Nederbach: a cross section of multilayered cell aggregation; asterisks indicate three successive stages of unequal cell budding, b\\nC. starmachii, mature pseudosporangium with upward broken pseudovagina, c detail of b showing several peripheral bundles of thylakoid membranes (arrows), d–f\\nC. polonicus from River Isar: d encysted pseudosporangium with electron dense sheath layer partly frayed in the upward part where exospores would be released, e viable cell with thylakoid double membranes distributed over the whole cell cross section (arrows); white arrowheads indicate mucus secreting pores, f cross section of a drought stressed exosporangium with parts of the cell wall exhibiting the many basal mucus pores (white arrow), g–i\\nC. geitleri from River Isar, notice the wall parallel thylakoids (arrows), g presumably extending young exospore, h–i details of mature exosporangia, i wall parallel thylakoids (arrow). cb carboxisomes, L lipid body, PV pseudovagina. Scale bars: a 10 μm, b, g, d 1 μm, c, e–f, h–i 500 nm', 'a Preoperative chest X-ray. b Postoperative chest X-ray immediately after myocardial revascularization and mitral valve replacement. c Postoperative chest X-ray immediately after the chest wall reconstruction with plates and screws. d Postoperative chest X-ray immediately after diaphragm plication', '3D Analysis of GMP-PNP-bound DRP1 helices.(A−B) TEM images of DRP1-GMP-PNP helices after negative staining with NanoVan. The mean pitch of the helices is 13.7 nm (yellow markers; Scale bar = 30 nm). (C) Image represents the average of 20 slices from the tomogram of DRP1-GMP-PNP helix in cryo-ET. The contrast has been inverted for viewing purposes. Protein density is shown in white. Black arrows show the helical turns (Scale bar = 15 nm). (D) A filtered-traced helix traced from the cryo-tomogram of a DRP1-GMP-PNP helix (grey chicken wire). The position of the monomers with an ideal helix of 30 nm in diameter and a pitch of 13.7 nm (Red dots; Scale bar = 15 nm). (E), (F), (G) Isosurface representation of Nanogold-bound DRP1-GMP-PNP helices reconstructed from the tomogram with (E) rotated 90° about the y-axis; (F) rotated 90° about the x- axis, and (G) cross-sectional view showing the gold labelled inside the helix. The map displayed by chimera has been low pass filtered to ~30Å (Scale bar = 30 nm).', 'Immunofluorescence staining of human fetal (a) and postnatal (b) kidneys with the UBASH3A (green) marker and the isolated signal with DAPI nuclear staining. Expression of UBASH3A (arrows), glomeruli (G), and immature glomeruli (IG). Significant differences in immunoreactivity were found between fully differentiated (b) and immature (a) glomeruli. Images (a,b) were taken on magnification 40×. Scale bar is 40 μm.', 'Representative case of a glaucomatous eye with central visual field loss. (a–c) cpRNFL thinning did not occur, but superior mGCL thinning was presented, indicating VF loss. (d,f) A fundus photograph superimposed on an OCTA en face image shows glaucomatous optic neuropathy and an RNFL defect. (d,e) The colour fundus photograph and the corresponding SRL OCTA projection do not indicate microvascular changes. (f,g) The DRL OCTA image shows ADRVD (yellow arrowheads) and locations of the corresponding ADRVD, mGCL thinning, and central VF loss. (h,i) An OCTA image shows the location, in which a vertical line scan (h, green arrow) of the microvasculature was obtained. (j,k) Magnified views are also shown of the white squares. The red shading indicates the blood flow. The SRL microvascular signal is intact, but the DRL microvascular signal is reduced (lll arrow heads). cpRNFL, circumpapillary retinal nerve fiver layer; mGCL, macular ganglion cell layer; VF, visual field; OCTA, optical coherence tomography angiography; ADRVD, vertical asymmetrical deep retinal vessel density reduction; DRL, deep retinal layer; SRL, superficial retinal layer.', 'PAJD-1 reduced P. aeruginosa-induced mammary gland lesions of mice. Mice were infected with P. aeruginosa PAmas5 strains and treated with (C) PAJD-1 and (D) ceftiofur sodium. (A) Mice were treated with PBS after infection as a medium-treated group. (B) An uninfected mouse served as a positive control. (E) The tissue alteration score was measured in tissue sections above. The scoring criteria were present in the materials and methods section. Mammary glands were collected 24\\xa0h after treatment and processed for H&E staining and microscopic examination. Results are shown as means ± SEM. Asterisks indicate when the tissue alteration score of uninfected mice and mice treated with PAJD-1 or ceftiofur sodium after infection were significantly lower (***p < 0.001 and *p < 0.05) than untreated mice after infection. The red arrows indicate neutrophilic infiltration. Magnification × 100, scale bars represent 200 nm.']}\n"
     ]
    }
   ],
   "source": [
    "from src.dataset.medical.ddr_dataset import DDRDataset\n",
    "from src.dataset.medical.pmc_oa_dataset import PMCOADataset\n",
    "\n",
    "ds = DDRDataset(root_dir=\"/media/yons/Datasets/OIA-DDR/DDR-dataset/\", split=\"train\")\n",
    "dl = ds.dataloader(batch_size=16, num_workers=16, shuffle=True)\n",
    "\n",
    "ds = PMCOADataset(root_dir=\"/media/yons/Datasets/PMC-OA/\", split=\"train\")\n",
    "dl = ds.dataloader(batch_size=16, num_workers=16, shuffle=True)\n",
    "\n",
    "for batch in dl:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入LLaVA推理模块\n",
    "from llava import load_llava_med, llava_med_chat\n",
    "\n",
    "# 导入Qwen推理模块\n",
    "from qwen import (\n",
    "    load_qwen_med,\n",
    "    load_qwen_vl_med,\n",
    "    qwen_med_chat,\n",
    "    qwen_vl_med_chat,\n",
    ")\n",
    "\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "cache_dir = \"cache/models/pretrained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d72699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型诊断输出： ['The image shows a detailed view of the retina, with a clear central macula and a well-defined optic disc. The retinal vessels are visible, branching out from the optic disc. There are no obvious signs of hemorrhage, exudates, or other abnormalities in the retina. The overall appearance suggests a healthy retina with no significant pathological findings. However, without clinical context or additional imaging, it is not possible to rule out all possible conditions. A thorough ophthalmic examination by a qualified medical professional is necessary to provide a comprehensive assessment.']\n",
      "模型诊断输出： ['The image shows a detailed view of the retina, with a clear central macular area and a well-defined optic disc. The retinal vessels are visible, branching out from the optic disc and spreading across the retina. The macular area appears to have a normal appearance, with no obvious signs of pathology such as hemorrhages, exudates, or lesions. The overall retinal architecture appears intact, with no significant abnormalities noted. This image suggests a healthy retinal condition, with no immediate signs of disease or injury.']\n",
      "模型诊断输出： ['这张视网膜图片显示了典型的糖尿病视网膜病变特征。糖尿病视网膜病变是一种由糖尿病引起的视网膜疾病，其主要表现为视网膜血管病变和视网膜色素上皮病变。图片中可以看到多个小的、不规则的黄色斑块，这些是糖尿病视网膜病变中常见的特征之一。此外，图片中还可见到一些小的、不规则的黄色斑块，这些是糖尿病视网膜病变中常见的特征之一。']\n",
      "模型诊断输出： ['这张视网膜图片显示了正常的人工晶状体植入后视网膜的影像。人工晶状体的植入使得视网膜的结构清晰可见。在中央,我们可以看到一个明亮的光点,这是人工晶状体的光点。周围有几条红色的血管,这是来自视网膜的血流。中央的光点周围有几条细小的血管,这是人工晶状体的血管。整个视网膜的表面光滑,没有明显的损伤或病变。']\n",
      "模型诊断输出： ['The image shows a detailed view of the retina, with a clear central macular hole. The macular hole appears as a dark, circular area in the center of the retina, surrounded by a bright, reddish-orange rim. The retinal vessels are visible, branching out from the optic disc at the center of the image. The overall appearance is consistent with a macular hole, a common condition affecting the central part of the retina. The image provides a clear and detailed view of the retinal anatomy, allowing for a thorough medical assessment and diagnosis.']\n",
      "模型诊断输出： ['这张视网膜图片显示了典型的糖尿病视网膜病变特征。糖尿病视网膜病变是一种由糖尿病引起的视网膜疾病，其主要表现为视网膜血管病变和视网膜色素上皮病变。图片中可以看到明显的视网膜血管病变，包括视网膜血管的增生和硬化，以及视网膜色素上皮的病变。这些病变可以导致视力下降和视网膜脱落等严重后果。因此，这张图片提示患者可能患有糖尿病视网膜病变，需要进行进一步的检查和治疗。']\n",
      "模型诊断输出： ['The image shows a detailed view of the retina, with a clear view of the optic disc, blood vessels, and the surrounding retinal tissue. The optic disc appears to be normal in size and shape, with a distinct border and a healthy pink color. The blood vessels emanating from the optic disc are well-defined, with no signs of hemorrhage, exudates, or other abnormalities. The retinal tissue appears to be healthy, with no visible lesions or abnormalities. Overall, the image suggests a normal, healthy retinal condition.']\n",
      "模型诊断输出： ['这张视网膜图片显示了典型的糖尿病视网膜病变特征。糖尿病视网膜病变是一种由糖尿病引起的视网膜疾病，其主要表现为视网膜血管病变和视网膜色素上皮病变。图片中可以看到明显的视网膜血管病变，血管变细、变脆，甚至可能有出血和渗出。此外，还可见到视网膜色素上皮病变，表现为视网膜色素上皮细胞的退化和色素沉着。这些病变通常会导致视力下降和视野缩小。糖尿病视网膜病变的诊断需要结合临床症状和体征，以及相关的实验室检查结果。']\n",
      "模型诊断输出： ['这张视网膜图片显示了多个明显的病变。最显著的是中央的黄色斑块，这可能是视网膜色素变性或视网膜色素沉着症。周围有多个小的黄色斑块，可能是视网膜血管病变或视网膜色素变性。此外，还有一些散落在视网膜上的小点，可能是视网膜血管病变或视网膜色素变性。这些病变可能与糖尿病视网膜病变有关。然而，需要更多的临床信息和检查才能确定具体的诊断。建议患者尽快就医，进行详细的眼部检查和相关检查，以确定具体的疾病和治疗方案。']\n",
      "模型诊断输出： ['The image shows a detailed view of the retina, with a clear view of the optic disc and the surrounding retinal tissue. The optic disc appears to be normal in size and shape, with a distinct border and a bright, reflective center. The retinal vessels are visible, branching out from the optic disc and spreading across the retina. The overall appearance of the retina suggests a healthy, well-vascularized tissue. There are no obvious signs of pathology, such as hemorrhages, exudates, or lesions, in this image. The retinal pigment epithelium appears to be intact, and the overall coloration of the retina is consistent with normal retinal tissue.']\n",
      "模型诊断输出： ['The image shows a detailed view of the retina, with a central bright spot that appears to be the optic disc. The surrounding retinal tissue has a reddish-orange hue, indicating the presence of blood vessels and the presence of blood. The image suggests the presence of a retinal condition, such as a retinal hemorrhage or a retinal detachment, which can be caused by various underlying medical conditions. The image provides a clear and detailed view of the retinal structure, allowing for a comprehensive medical assessment and diagnosis.']\n",
      "模型诊断输出： ['这张视网膜图片显示了典型的糖尿病视网膜病变特征。糖尿病视网膜病变是一种由糖尿病引起的视网膜疾病，其主要表现为视网膜血管病变和视网膜色素上皮病变。图片中可以看到明显的视网膜血管病变，包括视网膜血管的增生和硬化，以及视网膜血管的阻塞和萎缩。此外，还可见到视网膜色素上皮病变，表现为视网膜色素上皮的增生和色素沉着。这些病变的出现是由于糖尿病患者体内血糖水平过高，导致视网膜血管和色素上皮受损。这种病变如果不及时治疗，可能会导致严重的眼部并发症，如视网膜脱落、视力丧失等。因此，对于糖尿病患者来说，定期进行眼部检查和治疗是非常重要的。']\n",
      "模型诊断输出： ['The image shows a detailed view of the retina, with a clear central area that appears to be the optic disc. The retinal vessels are visible, branching out from the optic disc. The image is taken in a dark field, which is typical for fundus photography. There are no obvious signs of hemorrhage, exudates, or other abnormalities in the visible portion of the retina. The macula, which is the central part of the retina responsible for sharp central vision, appears to be intact. The overall appearance of the retina suggests a normal, healthy state without any significant pathological findings.']\n",
      "模型诊断输出： ['这张视网膜图片显示了典型的糖尿病视网膜病变特征。糖尿病视网膜病变是一种由糖尿病引起的视网膜疾病，其主要表现为视网膜血管病变和视网膜色素上皮病变。图片中可以看到明显的视网膜血管病变，包括视网膜血管的增生和硬化，以及视网膜血管的阻塞和萎缩。此外，还可见到视网膜色素上皮病变，表现为视网膜色素上皮的增生和色素沉着。这些病变的出现是由于糖尿病患者体内血糖水平过高，导致视网膜血管和色素上皮受损。这种病变如果不及时治疗，可能会导致严重的眼部并发症，如视网膜脱落、视力丧失等。因此，对于糖尿病患者来说，定期进行眼部检查和治疗是非常重要的。']\n",
      "模型诊断输出： ['这张视网膜图片显示了典型的糖尿病视网膜病变特征。糖尿病视网膜病变是一种由糖尿病引起的视网膜疾病，其主要表现为视网膜血管病变和视网膜色素上皮病变。图片中可以看到明显的视网膜血管病变，包括视网膜血管的增生和硬化，以及视网膜血管的阻塞和萎缩。此外，还可见到视网膜色素上皮病变，表现为视网膜色素上皮的增生和色素沉着。这些病变的出现是由于糖尿病患者体内血糖水平过高，导致视网膜血管和色素上皮受损。这种病变如果不及时治疗，可能会导致严重的眼部并发症，如视网膜脱落、视力丧失等。因此，对于糖尿病患者来说，定期进行眼部检查和治疗是非常重要的。']\n",
      "模型诊断输出： ['这张视网膜图片显示了典型的糖尿病视网膜病变特征。糖尿病视网膜病变是一种由糖尿病引起的视网膜疾病，其主要表现为视网膜血管病变和视网膜色素上皮病变。图片中可以看到明显的视网膜血管病变，包括视网膜血管的增生和硬化，以及视网膜血管的阻塞和萎缩。此外，还可见到视网膜色素上皮病变，表现为视网膜色素上皮的增生和色素沉着。这些病变的出现是由于糖尿病患者体内血糖水平过高，导致视网膜血管和色素上皮受损。糖尿病视网膜病变是一种严重的眼部疾病，如果不及时治疗，可能会导致失明。']\n"
     ]
    }
   ],
   "source": [
    "model, processor = load_qwen_vl_med()\n",
    "\n",
    "for batch in dl:\n",
    "    images = batch[\"image\"]\n",
    "    # 假设 batch size 为 1，如果为多张图片可以遍历\n",
    "    for img in images:\n",
    "        # Qwen-VL-Med 输入需要 PIL.Image，如果 img 是 tensor，需要还原\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # 反归一化可以根据你的预处理决定是否需要\n",
    "            image_np = img.permute(1, 2, 0).cpu().numpy()\n",
    "            # 如果像素值是0~1，乘255再转为uint8\n",
    "            if image_np.max() <= 1.0:\n",
    "                image_np = (image_np * 255).astype(\"uint8\")\n",
    "            from PIL import Image\n",
    "\n",
    "            image_pil = Image.fromarray(image_np)\n",
    "        else:\n",
    "            image_pil = img\n",
    "        # 输入模型，假设你的 prompt 是询问对该视网膜图片的诊断\n",
    "        prompt = \"请根据这张视网膜图片进行医学诊断。\"\n",
    "        output_text = qwen_vl_med_chat(\n",
    "            model, processor, new_text=prompt, new_image=image_pil\n",
    "        )\n",
    "        print(\"模型诊断输出：\", output_text)\n",
    "    break  # 只测试一批\n",
    "\n",
    "# 输出\n",
    "# 这张视网膜图片显示了典型的糖尿病视网膜病变特征。糖尿病视网膜病变是一种由糖尿病引起的视网膜疾病，其主要表现为视网膜血管病变和视网膜色素上皮病变。图片中可以看到明显的视网膜血管病变，包括视网膜血管的增生和硬化，以及视网膜色素上皮的病变。这些病变可以导致视力下降和视网膜脱落等严重并发症。糖尿病视网膜病变的诊断需要结合患者的病史、血糖水平、眼底检查结果等多方面的信息。建议患者及时就医，接受规范的治疗和管理。\n",
    "# 这张视网膜图片显示了典型的糖尿病视网膜病变特征。糖尿病视网膜病变是一种由糖尿病引起的视网膜疾病，其主要表现为视网膜血管病变和视网膜色素上皮病变。图片中可以看到明显的视网膜血管病变，包括视网膜血管的增生和硬化，以及视网膜血管的阻塞和萎缩。此外，还可见到视网膜色素上皮病变，表现为视网膜色素上皮的增生和色素沉着。这些病变的出现是由于糖尿病患者体内血糖水平过高，导致视网膜血管和色素上皮受损。这种病变如果不及时治疗，可能会导致严重的眼部并发症，如视网膜脱落、视力丧失等。因此，对于糖尿病患者来说，定期进行眼部检查和治疗是非常重要的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000449bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.54s/it]\n",
      "Some weights of the model checkpoint at microsoft/llava-med-v1.5-mistral-7b were not used when initializing LlavaMistralForCausalLM: ['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight']\n",
      "- This IS expected if you are initializing LlavaMistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlavaMistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 97.94 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 23.15 GiB is allocated by PyTorch, and 121.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, tokenizer, processor \u001b[38;5;241m=\u001b[39m \u001b[43mload_llava_med\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dl:\n\u001b[1;32m      4\u001b[0m     images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m, in \u001b[0;36mload_llava_med\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_pretrained_model\n\u001b[1;32m     38\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/llava-med-v1.5-mistral-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 39\u001b[0m tokenizer, model, image_processor, context_len \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllava-med-v1.5-mistral-7b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer, image_processor\n",
      "File \u001b[0;32m~/workspace/sxy/lab/NeuroTrain/llava/model/builder.py:75\u001b[0m, in \u001b[0;36mload_pretrained_model\u001b[0;34m(model_path, model_base, model_name, load_8bit, load_4bit, device_map, device)\u001b[0m\n\u001b[1;32m     73\u001b[0m     vision_tower\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m     74\u001b[0m     model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmm_projector\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     image_processor \u001b[38;5;241m=\u001b[39m vision_tower\u001b[38;5;241m.\u001b[39mimage_processor\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/transformers/modeling_utils.py:4343\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   4339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4340\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4341\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4342\u001b[0m         )\n\u001b[0;32m-> 4343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 97.94 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 23.15 GiB is allocated by PyTorch, and 121.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model, tokenizer, processor = load_llava_med()\n",
    "\n",
    "for batch in dl:\n",
    "    images = batch[\"image\"]\n",
    "    # 假设 batch size 为 1，如果为多张图片可以遍历\n",
    "    for img in images:\n",
    "        # Qwen-VL-Med 输入需要 PIL.Image，如果 img 是 tensor，需要还原\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # 反归一化可以根据你的预处理决定是否需要\n",
    "            image_np = img.permute(1, 2, 0).cpu().numpy()\n",
    "            # 如果像素值是0~1，乘255再转为uint8\n",
    "            if image_np.max() <= 1.0:\n",
    "                image_np = (image_np * 255).astype(\"uint8\")\n",
    "            from PIL import Image\n",
    "\n",
    "            image_pil = Image.fromarray(image_np)\n",
    "        else:\n",
    "            image_pil = img\n",
    "        # 输入模型，假设你的 prompt 是询问对该视网膜图片的诊断\n",
    "        prompt = \"请根据这张视网膜图片进行医学诊断。\"\n",
    "        output_text = llava_med_chat(\n",
    "            model, tokenizer, processor, new_text=prompt, new_image=image_pil\n",
    "        )\n",
    "        print(\"模型诊断输出：\", output_text)\n",
    "    break  # 只测试一批\n",
    "\n",
    "# 输出\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
