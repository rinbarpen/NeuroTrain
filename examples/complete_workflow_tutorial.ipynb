{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NeuroTrain Complete Workflow Tutorial\n",
        "\n",
        "This notebook demonstrates a complete end-to-end deep learning workflow using NeuroTrain, including:\n",
        "\n",
        "1. Environment Setup\n",
        "2. Data Loading and Exploration\n",
        "3. Model Selection and Configuration\n",
        "4. Training Process\n",
        "5. Model Evaluation\n",
        "6. Results Visualization\n",
        "7. Model Export and Deployment\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "```bash\n",
        "conda activate ntrain\n",
        "uv pip install -e '.[cu128]'\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Import Libraries and Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from src.dataset import get_train_valid_test_dataloader\n",
        "from src.models import get_model\n",
        "from src.metrics import accuracy, dice, iou_seg\n",
        "from src.utils.criterion import get_criterion\n",
        "from src.utils import EarlyStopping\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Loading and Exploration\n",
        "\n",
        "We'll use CIFAR-10 as an example dataset for this tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure dataset\n",
        "config = {\n",
        "    \"dataset\": {\n",
        "        \"name\": \"cifar10\",\n",
        "        \"root_dir\": \"../data/cifar10\",\n",
        "        \"train\": True,\n",
        "        \"download\": True,\n",
        "    },\n",
        "    \"training\": {\"batch_size\": 128, \"num_workers\": 2},\n",
        "}\n",
        "\n",
        "# Load data\n",
        "print(\"Loading datasets...\")\n",
        "train_loader, valid_loader, test_loader = get_train_valid_test_dataloader(config)\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(valid_loader) if valid_loader else 0}\")\n",
        "print(f\"Test batches: {len(test_loader) if test_loader else 0}\")\n",
        "\n",
        "# CIFAR-10 classes\n",
        "classes = [\n",
        "    \"airplane\",\n",
        "    \"automobile\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"deer\",\n",
        "    \"dog\",\n",
        "    \"frog\",\n",
        "    \"horse\",\n",
        "    \"ship\",\n",
        "    \"truck\",\n",
        "]\n",
        "\n",
        "\n",
        "# Visualize some samples\n",
        "def show_batch(loader, num_images=16):\n",
        "    dataiter = iter(loader)\n",
        "    images, labels = next(dataiter)\n",
        "\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < num_images:\n",
        "            img = images[i].permute(1, 2, 0).numpy()\n",
        "            img = (img - img.min()) / (img.max() - img.min())\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(f\"{classes[labels[i]]}\")\n",
        "            ax.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "show_batch(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Model Creation\n",
        "\n",
        "We'll use a ResNet18 model pretrained on ImageNet and fine-tune it for CIFAR-10.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "model_config = {\n",
        "    \"arch\": \"resnet18\",\n",
        "    \"pretrained\": True,\n",
        "    \"n_classes\": 10,\n",
        "    \"n_channels\": 3,\n",
        "}\n",
        "\n",
        "# Create model\n",
        "print(\"Creating model...\")\n",
        "model = get_model(\"torchvision\", model_config)\n",
        "model = model.to(device)\n",
        "\n",
        "# Model summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel: {model_config['arch']}\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB (float32)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Training Configuration\n",
        "\n",
        "Define loss function, optimizer, and learning rate scheduler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(f\"Loss function: {criterion.__class__.__name__}\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "print(f\"Optimizer: {optimizer.__class__.__name__}, LR: {learning_rate}\")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "print(f\"Scheduler: {scheduler.__class__.__name__}\")\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=1e-4, mode=\"min\")\n",
        "print(f\"Early stopping: patience={early_stopping.patience}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Training Loop\n",
        "\n",
        "Now let's implement the training loop with validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "    for images, labels in train_pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Update progress bar\n",
        "        train_pbar.set_postfix(\n",
        "            {\n",
        "                \"loss\": f\"{loss.item():.4f}\",\n",
        "                \"acc\": f\"{100.*train_correct/train_total:.2f}%\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_acc = 100.0 * train_correct / train_total\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Validation phase\n",
        "    if valid_loader:\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in valid_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(valid_loader)\n",
        "        val_acc = 100.0 * val_correct / val_total\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
        "        print(f\"  Train - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Valid - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
        "        print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            print(f\"  âœ“ New best model!\")\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopping(avg_val_loss):\n",
        "            print(f\"\\nâš  Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Training Complete!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss curves\n",
        "ax1.plot(train_losses, label=\"Training Loss\", marker=\"o\", linewidth=2)\n",
        "if val_losses:\n",
        "    ax1.plot(val_losses, label=\"Validation Loss\", marker=\"s\", linewidth=2)\n",
        "ax1.set_xlabel(\"Epoch\", fontsize=12)\n",
        "ax1.set_ylabel(\"Loss\", fontsize=12)\n",
        "ax1.set_title(\"Training and Validation Loss\", fontsize=14, fontweight=\"bold\")\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "ax2.plot(train_accs, label=\"Training Accuracy\", marker=\"o\", linewidth=2)\n",
        "if val_accs:\n",
        "    ax2.plot(val_accs, label=\"Validation Accuracy\", marker=\"s\", linewidth=2)\n",
        "ax2.set_xlabel(\"Epoch\", fontsize=12)\n",
        "ax2.set_ylabel(\"Accuracy (%)\", fontsize=12)\n",
        "ax2.set_title(\"Training and Validation Accuracy\", fontsize=14, fontweight=\"bold\")\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final statistics\n",
        "print(f\"\\nFinal Training Accuracy: {train_accs[-1]:.2f}%\")\n",
        "if val_accs:\n",
        "    print(f\"Final Validation Accuracy: {val_accs[-1]:.2f}%\")\n",
        "print(f\"Best Validation Loss: {best_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Model Evaluation on Test Set\n",
        "\n",
        "Load the best model and evaluate on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "if \"best_model_state\" in locals():\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(\"Loaded best model for testing\")\n",
        "\n",
        "# Test evaluation\n",
        "if test_loader:\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "\n",
        "    # For confusion matrix\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    test_acc = 100.0 * test_correct / test_total\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Test Results\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "    print(f\"Correct: {test_correct} / {test_total}\")\n",
        "else:\n",
        "    print(\"No test loader available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Confusion Matrix and Per-Class Metrics\n",
        "\n",
        "Visualize the confusion matrix and compute per-class metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "if test_loader and \"all_predictions\" in locals():\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=classes,\n",
        "        yticklabels=classes,\n",
        "        cbar_kws={\"label\": \"Count\"},\n",
        "    )\n",
        "    plt.xlabel(\"Predicted Label\", fontsize=12)\n",
        "    plt.ylabel(\"True Label\", fontsize=12)\n",
        "    plt.title(\"Confusion Matrix\", fontsize=14, fontweight=\"bold\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nPer-Class Metrics:\")\n",
        "    print(\n",
        "        classification_report(\n",
        "            all_labels, all_predictions, target_names=classes, digits=4\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Per-class accuracy\n",
        "    print(\"\\nPer-Class Accuracy:\")\n",
        "    for i, class_name in enumerate(classes):\n",
        "        class_correct = cm[i, i]\n",
        "        class_total = cm[i].sum()\n",
        "        class_acc = 100.0 * class_correct / class_total if class_total > 0 else 0\n",
        "        print(f\"  {class_name:12s}: {class_acc:6.2f}% ({class_correct}/{class_total})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Sample Predictions Visualization\n",
        "\n",
        "Let's visualize some predictions to see how the model performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if test_loader:\n",
        "    # Get a batch of test images\n",
        "    dataiter = iter(test_loader)\n",
        "    images, labels = next(dataiter)\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Get predictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "    # Move to CPU for visualization\n",
        "    images = images.cpu()\n",
        "    labels = labels.cpu()\n",
        "    predicted = predicted.cpu()\n",
        "\n",
        "    # Visualize predictions\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < len(images):\n",
        "            img = images[i].permute(1, 2, 0).numpy()\n",
        "            img = (img - img.min()) / (img.max() - img.min())\n",
        "\n",
        "            ax.imshow(img)\n",
        "            true_label = classes[labels[i]]\n",
        "            pred_label = classes[predicted[i]]\n",
        "\n",
        "            # Color: green if correct, red if wrong\n",
        "            color = \"green\" if labels[i] == predicted[i] else \"red\"\n",
        "            ax.set_title(\n",
        "                f\"True: {true_label}\\nPred: {pred_label}\",\n",
        "                color=color,\n",
        "                fontsize=10,\n",
        "                fontweight=\"bold\",\n",
        "            )\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "    plt.suptitle(\n",
        "        \"Sample Predictions (Green=Correct, Red=Wrong)\", fontsize=16, fontweight=\"bold\"\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Model Export and Deployment\n",
        "\n",
        "Save the model for future use and optionally export to ONNX format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = Path(\"../runs/tutorial_example\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save PyTorch model\n",
        "model_path = output_dir / \"best_model.pth\"\n",
        "torch.save(\n",
        "    {\n",
        "        \"epoch\": len(train_losses),\n",
        "        \"model_state_dict\": (\n",
        "            best_model_state if \"best_model_state\" in locals() else model.state_dict()\n",
        "        ),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_losses\": val_losses,\n",
        "        \"train_accs\": train_accs,\n",
        "        \"val_accs\": val_accs,\n",
        "        \"test_acc\": test_acc if \"test_acc\" in locals() else None,\n",
        "        \"config\": {\n",
        "            \"model\": model_config,\n",
        "            \"training\": {\"num_epochs\": num_epochs, \"learning_rate\": learning_rate},\n",
        "        },\n",
        "    },\n",
        "    model_path,\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Model saved to: {model_path}\")\n",
        "print(f\"  File size: {model_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
        "\n",
        "# Optional: Export to ONNX (uncomment to use)\n",
        "# try:\n",
        "#     onnx_path = output_dir / 'model.onnx'\n",
        "#     dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "#     torch.onnx.export(\n",
        "#         model,\n",
        "#         dummy_input,\n",
        "#         onnx_path,\n",
        "#         input_names=['input'],\n",
        "#         output_names=['output'],\n",
        "#         dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
        "#         opset_version=11\n",
        "#     )\n",
        "#     print(f\"âœ“ ONNX model saved to: {onnx_path}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"âš  ONNX export failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You've completed a full deep learning workflow with NeuroTrain. Here's what we covered:\n",
        "\n",
        "### âœ… What We Accomplished\n",
        "\n",
        "1. **Environment Setup** - Configured PyTorch and NeuroTrain\n",
        "2. **Data Loading** - Loaded and visualized CIFAR-10 dataset\n",
        "3. **Model Creation** - Built a ResNet18 model with pretrained weights\n",
        "4. **Training Configuration** - Set up loss, optimizer, and scheduler\n",
        "5. **Model Training** - Trained the model with validation\n",
        "6. **Visualization** - Plotted training curves and metrics\n",
        "7. **Evaluation** - Tested model on test set\n",
        "8. **Analysis** - Generated confusion matrix and per-class metrics\n",
        "9. **Prediction Visualization** - Visualized sample predictions\n",
        "10. **Model Export** - Saved model for deployment\n",
        "\n",
        "### ðŸ“Š Key Results\n",
        "\n",
        "- Training completed successfully\n",
        "- Model saved and ready for deployment\n",
        "- Comprehensive evaluation metrics computed\n",
        "- Visualization of model performance\n",
        "\n",
        "### ðŸš€ Next Steps\n",
        "\n",
        "1. **Experiment with different models**: Try VGG, EfficientNet, or Vision Transformers\n",
        "2. **Try different datasets**: Medical images, COCO, ImageNet\n",
        "3. **Tune hyperparameters**: Learning rate, batch size, augmentation\n",
        "4. **Advanced features**: Mixed precision training, distributed training\n",
        "5. **Deploy your model**: Export to ONNX, quantization, TorchScript\n",
        "\n",
        "### ðŸ“š Additional Resources\n",
        "\n",
        "- [Dataset Module Documentation](../docs/DATASET_MODULE.md)\n",
        "- [Models Module Documentation](../docs/MODELS_MODULE.md)\n",
        "- [Engine Module Documentation](../docs/ENGINE_MODULE.md)\n",
        "- [Project Architecture](../docs/ARCHITECTURE.md)\n",
        "- [More Examples](../examples/)\n",
        "\n",
        "### ðŸ’¡ Tips for Better Results\n",
        "\n",
        "1. **Use data augmentation** to improve generalization\n",
        "2. **Monitor validation metrics** to detect overfitting\n",
        "3. **Use learning rate scheduling** for better convergence\n",
        "4. **Save checkpoints regularly** to resume interrupted training\n",
        "5. **Visualize results** to understand model behavior\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Training with NeuroTrain! ðŸŽ‰**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
