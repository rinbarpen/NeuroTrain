{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NeuroTrain Dataset模块完整教程\n",
        "\n",
        "本Notebook提供NeuroTrain Dataset模块的完整使用教程，包括：\n",
        "\n",
        "1. 基础数据集加载\n",
        "2. 混合数据集使用\n",
        "3. 数据增强\n",
        "4. DataLoader配置\n",
        "5. 自定义数据集\n",
        "6. 高级功能\n",
        "\n",
        "## 环境准备\n",
        "\n",
        "首先确保已安装所有依赖：\n",
        "\n",
        "```bash\n",
        "conda activate ntrain\n",
        "uv pip install -e '.[cu128]'\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from src.dataset import (\n",
        "    get_dataset,\n",
        "    get_train_dataset,\n",
        "    get_test_dataset,\n",
        "    get_train_valid_test_dataloader,\n",
        "    random_sample,\n",
        ")\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 基础数据集加载\n",
        "\n",
        "### 1.1 CIFAR-10数据集\n",
        "\n",
        "CIFAR-10是一个经典的图像分类数据集，包含10个类别的60000张32x32彩色图像。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure CIFAR-10 dataset\n",
        "config_cifar = {\n",
        "    \"dataset\": {\n",
        "        \"name\": \"cifar10\",\n",
        "        \"root_dir\": \"../data/cifar10\",\n",
        "        \"train\": True,\n",
        "        \"download\": True,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Load dataset\n",
        "cifar_dataset = get_dataset(config_cifar)\n",
        "\n",
        "print(f\"Dataset size: {len(cifar_dataset)}\")\n",
        "print(\n",
        "    f\"Classes: {cifar_dataset.classes if hasattr(cifar_dataset, 'classes') else 'N/A'}\"\n",
        ")\n",
        "\n",
        "# Visualize samples\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    image, label = cifar_dataset[i]\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.permute(1, 2, 0).numpy()\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(f\"Class: {cifar_dataset.classes[label]}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.suptitle(\"CIFAR-10 Sample Images\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 医学图像数据集 (DRIVE)\n",
        "\n",
        "DRIVE数据集用于视网膜血管分割任务。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure DRIVE dataset\n",
        "config_drive = {\n",
        "    \"dataset\": {\n",
        "        \"name\": \"drive\",\n",
        "        \"root_dir\": \"../data/drive\",\n",
        "        \"is_rgb\": True,\n",
        "        \"train_split\": 0.8,\n",
        "        \"image_size\": [512, 512],\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Load training and test datasets\n",
        "    train_dataset = get_train_dataset(config_drive)\n",
        "    test_dataset = get_test_dataset(config_drive)\n",
        "\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "    # Visualize a sample\n",
        "    if len(train_dataset) > 0:\n",
        "        image, mask = train_dataset[0]\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "        # Original image\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            img_np = image.permute(1, 2, 0).numpy()\n",
        "        axes[0].imshow(img_np)\n",
        "        axes[0].set_title(\"Retinal Image\", fontsize=14)\n",
        "        axes[0].axis(\"off\")\n",
        "\n",
        "        # Mask\n",
        "        if isinstance(mask, torch.Tensor):\n",
        "            mask_np = mask.squeeze().numpy()\n",
        "        axes[1].imshow(mask_np, cmap=\"gray\")\n",
        "        axes[1].set_title(\"Vessel Mask\", fontsize=14)\n",
        "        axes[1].axis(\"off\")\n",
        "\n",
        "        # Overlay\n",
        "        axes[2].imshow(img_np)\n",
        "        axes[2].imshow(mask_np, alpha=0.5, cmap=\"Reds\")\n",
        "        axes[2].set_title(\"Overlay\", fontsize=14)\n",
        "        axes[2].axis(\"off\")\n",
        "\n",
        "        plt.suptitle(\"DRIVE Dataset Sample\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"DRIVE dataset not available: {e}\")\n",
        "    print(\"Please ensure the DRIVE dataset is in the correct directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure dataset with augmentation\n",
        "config_aug = {\n",
        "    \"dataset\": {\n",
        "        \"name\": \"cifar10\",\n",
        "        \"root_dir\": \"../data/cifar10\",\n",
        "        \"train\": True,\n",
        "        \"download\": True,\n",
        "        \"augmentation\": {\n",
        "            \"random_flip\": True,\n",
        "            \"random_rotation\": True,\n",
        "            \"rotation_range\": 15,\n",
        "            \"brightness_range\": [0.8, 1.2],\n",
        "            \"color_jitter\": True,\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "dataset_aug = get_dataset(config_aug)\n",
        "\n",
        "# Get the same sample multiple times to see different augmentations\n",
        "sample_idx = 0\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "\n",
        "for i in range(8):\n",
        "    image, label = dataset_aug[sample_idx]\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "    row, col = i // 4, i % 4\n",
        "    axes[row, col].imshow(image)\n",
        "    axes[row, col].set_title(f\"Augmentation {i+1}\")\n",
        "    axes[row, col].axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Data Augmentation Examples (Same Original Image)\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. DataLoader使用\n",
        "\n",
        "DataLoader提供批量数据加载、并行处理和数据混洗功能。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training parameters\n",
        "config_loader = {\n",
        "    \"dataset\": {\n",
        "        \"name\": \"cifar10\",\n",
        "        \"root_dir\": \"../data/cifar10\",\n",
        "        \"train\": True,\n",
        "        \"download\": True,\n",
        "    },\n",
        "    \"training\": {\"batch_size\": 64, \"num_workers\": 2},\n",
        "}\n",
        "\n",
        "# Get DataLoaders\n",
        "train_loader, valid_loader, test_loader = get_train_valid_test_dataloader(config_loader)\n",
        "\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "if valid_loader:\n",
        "    print(f\"Number of validation batches: {len(valid_loader)}\")\n",
        "if test_loader:\n",
        "    print(f\"Number of test batches: {len(test_loader)}\")\n",
        "\n",
        "# Inspect a batch\n",
        "for images, labels in train_loader:\n",
        "    print(f\"\\nBatch information:\")\n",
        "    print(f\"  Images shape: {images.shape}\")\n",
        "    print(f\"  Labels shape: {labels.shape}\")\n",
        "    print(f\"  Image dtype: {images.dtype}\")\n",
        "    print(f\"  Image value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
        "\n",
        "    # Visualize a batch\n",
        "    fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < len(images):\n",
        "            img = images[i].permute(1, 2, 0).numpy()\n",
        "            # Normalize for display\n",
        "            img = (img - img.min()) / (img.max() - img.min())\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(f\"Label: {labels[i].item()}\", fontsize=8)\n",
        "            ax.axis(\"off\")\n",
        "    plt.suptitle(\"Batch Visualization\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    break  # Only show first batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 混合数据集\n",
        "\n",
        "混合数据集允许同时使用多个数据集进行训练，支持不同的采样策略。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure hybrid dataset\n",
        "config_hybrid = {\n",
        "    \"dataset\": {\n",
        "        \"name\": \"enhanced_hybrid\",\n",
        "        \"datasets\": [\"drive\", \"medical/chasedb1\"],\n",
        "        \"sampling_strategy\": \"weighted\",\n",
        "        \"ratios\": [0.6, 0.4],\n",
        "        \"weights\": [1.0, 1.2],\n",
        "        \"drive\": {\"root_dir\": \"../data/drive\", \"is_rgb\": True},\n",
        "        \"medical/chasedb1\": {\"root_dir\": \"../data/chasedb1\", \"is_rgb\": True},\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    hybrid_dataset = get_dataset(config_hybrid)\n",
        "    print(f\"Hybrid dataset total samples: {len(hybrid_dataset)}\")\n",
        "    print(f\"Sampling strategy: {config_hybrid['dataset']['sampling_strategy']}\")\n",
        "    print(f\"Dataset ratios: {config_hybrid['dataset']['ratios']}\")\n",
        "    print(f\"Sample weights: {config_hybrid['dataset']['weights']}\")\n",
        "\n",
        "    # Sample from different sources\n",
        "    print(\"\\nSampling examples:\")\n",
        "    for i in range(min(5, len(hybrid_dataset))):\n",
        "        image, mask = hybrid_dataset[i]\n",
        "        source = (\n",
        "            hybrid_dataset.get_source(i)\n",
        "            if hasattr(hybrid_dataset, \"get_source\")\n",
        "            else \"unknown\"\n",
        "        )\n",
        "        print(f\"  Sample {i}: source={source}, image shape={image.shape}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Hybrid dataset not available: {e}\")\n",
        "    print(\"Please ensure all component datasets are available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 数据集统计分析\n",
        "\n",
        "了解数据集的统计特性对于模型训练很重要。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze CIFAR-10 dataset statistics\n",
        "def analyze_dataset(dataset, num_samples=1000):\n",
        "    \"\"\"Analyze dataset statistics\"\"\"\n",
        "    print(f\"Analyzing dataset (using {num_samples} samples)...\")\n",
        "\n",
        "    # Sample randomly\n",
        "    indices = np.random.choice(\n",
        "        len(dataset), min(num_samples, len(dataset)), replace=False\n",
        "    )\n",
        "\n",
        "    # Collect statistics\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for idx in indices:\n",
        "        image, label = dataset[idx]\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            images.append(image)\n",
        "        labels.append(label)\n",
        "\n",
        "    images = torch.stack(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Calculate statistics\n",
        "    mean = images.mean(dim=[0, 2, 3])\n",
        "    std = images.std(dim=[0, 2, 3])\n",
        "\n",
        "    print(f\"\\nImage Statistics:\")\n",
        "    print(f\"  Mean (RGB): [{mean[0]:.4f}, {mean[1]:.4f}, {mean[2]:.4f}]\")\n",
        "    print(f\"  Std (RGB):  [{std[0]:.4f}, {std[1]:.4f}, {std[2]:.4f}]\")\n",
        "    print(f\"  Value range: [{images.min():.4f}, {images.max():.4f}]\")\n",
        "\n",
        "    # Class distribution\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    print(f\"\\nClass Distribution:\")\n",
        "    for cls, count in zip(unique, counts):\n",
        "        print(f\"  Class {cls}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "    # Visualize class distribution\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(unique, counts, color=\"steelblue\")\n",
        "    plt.xlabel(\"Class\", fontsize=12)\n",
        "    plt.ylabel(\"Number of Samples\", fontsize=12)\n",
        "    plt.title(\"Class Distribution\", fontsize=14)\n",
        "    plt.xticks(unique)\n",
        "    plt.grid(axis=\"y\", alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "# Analyze CIFAR-10\n",
        "mean, std = analyze_dataset(cifar_dataset, num_samples=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 总结\n",
        "\n",
        "本教程介绍了NeuroTrain Dataset模块的主要功能：\n",
        "\n",
        "1. ✅ 加载标准和医学图像数据集\n",
        "2. ✅ 配置数据增强\n",
        "3. ✅ 使用DataLoader进行批量加载\n",
        "4. ✅ 创建混合数据集\n",
        "5. ✅ 分析数据集统计信息\n",
        "\n",
        "### 下一步\n",
        "\n",
        "- 查看其他教程了解模型训练\n",
        "- 阅读API文档了解更多选项\n",
        "- 尝试自定义数据集类\n",
        "\n",
        "### 参考资料\n",
        "\n",
        "- [Dataset模块文档](../docs/DATASET_MODULE.md)\n",
        "- [项目架构文档](../docs/ARCHITECTURE.md)\n",
        "- [API参考](../docs/API_REFERENCE.md)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
