{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a3cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dict loading result:\n",
      "  Missing keys: ['region_encoder.backbone.cls_token', 'region_encoder.backbone.pos_embed', 'region_encoder.backbone.patch_embed.proj.weight', 'region_encoder.backbone.patch_embed.proj.bias', 'region_encoder.backbone.blocks.0.norm1.weight', 'region_encoder.backbone.blocks.0.norm1.bias', 'region_encoder.backbone.blocks.0.attn.qkv.weight', 'region_encoder.backbone.blocks.0.attn.qkv.bias', 'region_encoder.backbone.blocks.0.attn.proj.weight', 'region_encoder.backbone.blocks.0.attn.proj.bias', 'region_encoder.backbone.blocks.0.norm2.weight', 'region_encoder.backbone.blocks.0.norm2.bias', 'region_encoder.backbone.blocks.0.mlp.fc1.weight', 'region_encoder.backbone.blocks.0.mlp.fc1.bias', 'region_encoder.backbone.blocks.0.mlp.fc2.weight', 'region_encoder.backbone.blocks.0.mlp.fc2.bias', 'region_encoder.backbone.blocks.1.norm1.weight', 'region_encoder.backbone.blocks.1.norm1.bias', 'region_encoder.backbone.blocks.1.attn.qkv.weight', 'region_encoder.backbone.blocks.1.attn.qkv.bias', 'region_encoder.backbone.blocks.1.attn.proj.weight', 'region_encoder.backbone.blocks.1.attn.proj.bias', 'region_encoder.backbone.blocks.1.norm2.weight', 'region_encoder.backbone.blocks.1.norm2.bias', 'region_encoder.backbone.blocks.1.mlp.fc1.weight', 'region_encoder.backbone.blocks.1.mlp.fc1.bias', 'region_encoder.backbone.blocks.1.mlp.fc2.weight', 'region_encoder.backbone.blocks.1.mlp.fc2.bias', 'region_encoder.backbone.blocks.2.norm1.weight', 'region_encoder.backbone.blocks.2.norm1.bias', 'region_encoder.backbone.blocks.2.attn.qkv.weight', 'region_encoder.backbone.blocks.2.attn.qkv.bias', 'region_encoder.backbone.blocks.2.attn.proj.weight', 'region_encoder.backbone.blocks.2.attn.proj.bias', 'region_encoder.backbone.blocks.2.norm2.weight', 'region_encoder.backbone.blocks.2.norm2.bias', 'region_encoder.backbone.blocks.2.mlp.fc1.weight', 'region_encoder.backbone.blocks.2.mlp.fc1.bias', 'region_encoder.backbone.blocks.2.mlp.fc2.weight', 'region_encoder.backbone.blocks.2.mlp.fc2.bias', 'region_encoder.backbone.blocks.3.norm1.weight', 'region_encoder.backbone.blocks.3.norm1.bias', 'region_encoder.backbone.blocks.3.attn.qkv.weight', 'region_encoder.backbone.blocks.3.attn.qkv.bias', 'region_encoder.backbone.blocks.3.attn.proj.weight', 'region_encoder.backbone.blocks.3.attn.proj.bias', 'region_encoder.backbone.blocks.3.norm2.weight', 'region_encoder.backbone.blocks.3.norm2.bias', 'region_encoder.backbone.blocks.3.mlp.fc1.weight', 'region_encoder.backbone.blocks.3.mlp.fc1.bias', 'region_encoder.backbone.blocks.3.mlp.fc2.weight', 'region_encoder.backbone.blocks.3.mlp.fc2.bias', 'region_encoder.backbone.blocks.4.norm1.weight', 'region_encoder.backbone.blocks.4.norm1.bias', 'region_encoder.backbone.blocks.4.attn.qkv.weight', 'region_encoder.backbone.blocks.4.attn.qkv.bias', 'region_encoder.backbone.blocks.4.attn.proj.weight', 'region_encoder.backbone.blocks.4.attn.proj.bias', 'region_encoder.backbone.blocks.4.norm2.weight', 'region_encoder.backbone.blocks.4.norm2.bias', 'region_encoder.backbone.blocks.4.mlp.fc1.weight', 'region_encoder.backbone.blocks.4.mlp.fc1.bias', 'region_encoder.backbone.blocks.4.mlp.fc2.weight', 'region_encoder.backbone.blocks.4.mlp.fc2.bias', 'region_encoder.backbone.blocks.5.norm1.weight', 'region_encoder.backbone.blocks.5.norm1.bias', 'region_encoder.backbone.blocks.5.attn.qkv.weight', 'region_encoder.backbone.blocks.5.attn.qkv.bias', 'region_encoder.backbone.blocks.5.attn.proj.weight', 'region_encoder.backbone.blocks.5.attn.proj.bias', 'region_encoder.backbone.blocks.5.norm2.weight', 'region_encoder.backbone.blocks.5.norm2.bias', 'region_encoder.backbone.blocks.5.mlp.fc1.weight', 'region_encoder.backbone.blocks.5.mlp.fc1.bias', 'region_encoder.backbone.blocks.5.mlp.fc2.weight', 'region_encoder.backbone.blocks.5.mlp.fc2.bias', 'region_encoder.backbone.blocks.6.norm1.weight', 'region_encoder.backbone.blocks.6.norm1.bias', 'region_encoder.backbone.blocks.6.attn.qkv.weight', 'region_encoder.backbone.blocks.6.attn.qkv.bias', 'region_encoder.backbone.blocks.6.attn.proj.weight', 'region_encoder.backbone.blocks.6.attn.proj.bias', 'region_encoder.backbone.blocks.6.norm2.weight', 'region_encoder.backbone.blocks.6.norm2.bias', 'region_encoder.backbone.blocks.6.mlp.fc1.weight', 'region_encoder.backbone.blocks.6.mlp.fc1.bias', 'region_encoder.backbone.blocks.6.mlp.fc2.weight', 'region_encoder.backbone.blocks.6.mlp.fc2.bias', 'region_encoder.backbone.blocks.7.norm1.weight', 'region_encoder.backbone.blocks.7.norm1.bias', 'region_encoder.backbone.blocks.7.attn.qkv.weight', 'region_encoder.backbone.blocks.7.attn.qkv.bias', 'region_encoder.backbone.blocks.7.attn.proj.weight', 'region_encoder.backbone.blocks.7.attn.proj.bias', 'region_encoder.backbone.blocks.7.norm2.weight', 'region_encoder.backbone.blocks.7.norm2.bias', 'region_encoder.backbone.blocks.7.mlp.fc1.weight', 'region_encoder.backbone.blocks.7.mlp.fc1.bias', 'region_encoder.backbone.blocks.7.mlp.fc2.weight', 'region_encoder.backbone.blocks.7.mlp.fc2.bias', 'region_encoder.backbone.blocks.8.norm1.weight', 'region_encoder.backbone.blocks.8.norm1.bias', 'region_encoder.backbone.blocks.8.attn.qkv.weight', 'region_encoder.backbone.blocks.8.attn.qkv.bias', 'region_encoder.backbone.blocks.8.attn.proj.weight', 'region_encoder.backbone.blocks.8.attn.proj.bias', 'region_encoder.backbone.blocks.8.norm2.weight', 'region_encoder.backbone.blocks.8.norm2.bias', 'region_encoder.backbone.blocks.8.mlp.fc1.weight', 'region_encoder.backbone.blocks.8.mlp.fc1.bias', 'region_encoder.backbone.blocks.8.mlp.fc2.weight', 'region_encoder.backbone.blocks.8.mlp.fc2.bias', 'region_encoder.backbone.blocks.9.norm1.weight', 'region_encoder.backbone.blocks.9.norm1.bias', 'region_encoder.backbone.blocks.9.attn.qkv.weight', 'region_encoder.backbone.blocks.9.attn.qkv.bias', 'region_encoder.backbone.blocks.9.attn.proj.weight', 'region_encoder.backbone.blocks.9.attn.proj.bias', 'region_encoder.backbone.blocks.9.norm2.weight', 'region_encoder.backbone.blocks.9.norm2.bias', 'region_encoder.backbone.blocks.9.mlp.fc1.weight', 'region_encoder.backbone.blocks.9.mlp.fc1.bias', 'region_encoder.backbone.blocks.9.mlp.fc2.weight', 'region_encoder.backbone.blocks.9.mlp.fc2.bias', 'region_encoder.backbone.blocks.10.norm1.weight', 'region_encoder.backbone.blocks.10.norm1.bias', 'region_encoder.backbone.blocks.10.attn.qkv.weight', 'region_encoder.backbone.blocks.10.attn.qkv.bias', 'region_encoder.backbone.blocks.10.attn.proj.weight', 'region_encoder.backbone.blocks.10.attn.proj.bias', 'region_encoder.backbone.blocks.10.norm2.weight', 'region_encoder.backbone.blocks.10.norm2.bias', 'region_encoder.backbone.blocks.10.mlp.fc1.weight', 'region_encoder.backbone.blocks.10.mlp.fc1.bias', 'region_encoder.backbone.blocks.10.mlp.fc2.weight', 'region_encoder.backbone.blocks.10.mlp.fc2.bias', 'region_encoder.backbone.blocks.11.norm1.weight', 'region_encoder.backbone.blocks.11.norm1.bias', 'region_encoder.backbone.blocks.11.attn.qkv.weight', 'region_encoder.backbone.blocks.11.attn.qkv.bias', 'region_encoder.backbone.blocks.11.attn.proj.weight', 'region_encoder.backbone.blocks.11.attn.proj.bias', 'region_encoder.backbone.blocks.11.norm2.weight', 'region_encoder.backbone.blocks.11.norm2.bias', 'region_encoder.backbone.blocks.11.mlp.fc1.weight', 'region_encoder.backbone.blocks.11.mlp.fc1.bias', 'region_encoder.backbone.blocks.11.mlp.fc2.weight', 'region_encoder.backbone.blocks.11.mlp.fc2.bias', 'region_encoder.backbone.norm.weight', 'region_encoder.backbone.norm.bias', 'region_encoder.norm1.weight', 'region_encoder.norm1.bias', 'region_encoder.norm2.weight', 'region_encoder.norm2.bias', 'region_encoder.attn.qkv.weight', 'region_encoder.attn.proj.weight', 'region_encoder.attn.proj.bias', 'region_encoder.moe.experts.0.0.weight', 'region_encoder.moe.experts.0.0.bias', 'region_encoder.moe.experts.0.3.weight', 'region_encoder.moe.experts.0.3.bias', 'region_encoder.moe.experts.1.0.weight', 'region_encoder.moe.experts.1.0.bias', 'region_encoder.moe.experts.1.3.weight', 'region_encoder.moe.experts.1.3.bias', 'region_encoder.moe.experts.2.0.weight', 'region_encoder.moe.experts.2.0.bias', 'region_encoder.moe.experts.2.3.weight', 'region_encoder.moe.experts.2.3.bias', 'region_encoder.moe.experts.3.0.weight', 'region_encoder.moe.experts.3.0.bias', 'region_encoder.moe.experts.3.3.weight', 'region_encoder.moe.experts.3.3.bias', 'region_encoder.moe.router.weight', 'region_encoder.moe.router.bias', 'text_encoder.text_model.embeddings.token_embedding.weight', 'text_encoder.text_model.embeddings.position_embedding.weight', 'text_encoder.text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_encoder.text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_encoder.text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_encoder.text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_encoder.text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_encoder.text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_encoder.text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_encoder.text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_encoder.text_model.encoder.layers.0.layer_norm1.weight', 'text_encoder.text_model.encoder.layers.0.layer_norm1.bias', 'text_encoder.text_model.encoder.layers.0.mlp.fc1.weight', 'text_encoder.text_model.encoder.layers.0.mlp.fc1.bias', 'text_encoder.text_model.encoder.layers.0.mlp.fc2.weight', 'text_encoder.text_model.encoder.layers.0.mlp.fc2.bias', 'text_encoder.text_model.encoder.layers.0.layer_norm2.weight', 'text_encoder.text_model.encoder.layers.0.layer_norm2.bias', 'text_encoder.text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_encoder.text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_encoder.text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_encoder.text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_encoder.text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_encoder.text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_encoder.text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_encoder.text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_encoder.text_model.encoder.layers.1.layer_norm1.weight', 'text_encoder.text_model.encoder.layers.1.layer_norm1.bias', 'text_encoder.text_model.encoder.layers.1.mlp.fc1.weight', 'text_encoder.text_model.encoder.layers.1.mlp.fc1.bias', 'text_encoder.text_model.encoder.layers.1.mlp.fc2.weight', 'text_encoder.text_model.encoder.layers.1.mlp.fc2.bias', 'text_encoder.text_model.encoder.layers.1.layer_norm2.weight', 'text_encoder.text_model.encoder.layers.1.layer_norm2.bias', 'text_encoder.text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_encoder.text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_encoder.text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_encoder.text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_encoder.text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_encoder.text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_encoder.text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_encoder.text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_encoder.text_model.encoder.layers.2.layer_norm1.weight', 'text_encoder.text_model.encoder.layers.2.layer_norm1.bias', 'text_encoder.text_model.encoder.layers.2.mlp.fc1.weight', 'text_encoder.text_model.encoder.layers.2.mlp.fc1.bias', 'text_encoder.text_model.encoder.layers.2.mlp.fc2.weight', 'text_encoder.text_model.encoder.layers.2.mlp.fc2.bias', 'text_encoder.text_model.encoder.layers.2.layer_norm2.weight', 'text_encoder.text_model.encoder.layers.2.layer_norm2.bias', 'text_encoder.text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_encoder.text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_encoder.text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_encoder.text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_encoder.text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_encoder.text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_encoder.text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_encoder.text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_encoder.text_model.encoder.layers.3.layer_norm1.weight', 'text_encoder.text_model.encoder.layers.3.layer_norm1.bias', 'text_encoder.text_model.encoder.layers.3.mlp.fc1.weight', 'text_encoder.text_model.encoder.layers.3.mlp.fc1.bias', 'text_encoder.text_model.encoder.layers.3.mlp.fc2.weight', 'text_encoder.text_model.encoder.layers.3.mlp.fc2.bias', 'text_encoder.text_model.encoder.layers.3.layer_norm2.weight', 'text_encoder.text_model.encoder.layers.3.layer_norm2.bias', 'text_encoder.text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_encoder.text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_encoder.text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_encoder.text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_encoder.text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_encoder.text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_encoder.text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_encoder.text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_encoder.text_model.encoder.layers.4.layer_norm1.weight', 'text_encoder.text_model.encoder.layers.4.layer_norm1.bias', 'text_encoder.text_model.encoder.layers.4.mlp.fc1.weight', 'text_encoder.text_model.encoder.layers.4.mlp.fc1.bias', 'text_encoder.text_model.encoder.layers.4.mlp.fc2.weight', 'text_encoder.text_model.encoder.layers.4.mlp.fc2.bias', 'text_encoder.text_model.encoder.layers.4.layer_norm2.weight', 'text_encoder.text_model.encoder.layers.4.layer_norm2.bias', 'text_encoder.text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_encoder.text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_encoder.text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_encoder.text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_encoder.text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_encoder.text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_encoder.text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_encoder.text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_encoder.text_model.encoder.layers.5.layer_norm1.weight', 'text_encoder.text_model.encoder.layers.5.layer_norm1.bias', 'text_encoder.text_model.encoder.layers.5.mlp.fc1.weight', 'text_encoder.text_model.encoder.layers.5.mlp.fc1.bias', 'text_encoder.text_model.encoder.layers.5.mlp.fc2.weight', 'text_encoder.text_model.encoder.layers.5.mlp.fc2.bias', 'text_encoder.text_model.encoder.layers.5.layer_norm2.weight', 'text_encoder.text_model.encoder.layers.5.layer_norm2.bias', 'text_encoder.text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_encoder.text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_encoder.text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_encoder.text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_encoder.text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_encoder.text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_encoder.text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_encoder.text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_encoder.text_model.encoder.layers.6.layer_norm1.weight', 'text_encoder.text_model.encoder.layers.6.layer_norm1.bias', 'text_encoder.text_model.encoder.layers.6.mlp.fc1.weight', 'text_encoder.text_model.encoder.layers.6.mlp.fc1.bias', 'text_encoder.text_model.encoder.layers.6.mlp.fc2.weight', 'text_encoder.text_model.encoder.layers.6.mlp.fc2.bias', 'text_encoder.text_model.encoder.layers.6.layer_norm2.weight', 'text_encoder.text_model.encoder.layers.6.layer_norm2.bias', 'text_encoder.text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_encoder.text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_encoder.text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_encoder.text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_encoder.text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_encoder.text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_encoder.text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_encoder.text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_encoder.text_model.encoder.layers.7.layer_norm1.weight', 'text_encoder.text_model.encoder.layers.7.layer_norm1.bias', 'text_encoder.text_model.encoder.layers.7.mlp.fc1.weight', 'text_encoder.text_model.encoder.layers.7.mlp.fc1.bias', 'text_encoder.text_model.encoder.layers.7.mlp.fc2.weight', 'text_encoder.text_model.encoder.layers.7.mlp.fc2.bias', 'text_encoder.text_model.encoder.layers.7.layer_norm2.weight', 'text_encoder.text_model.encoder.layers.7.layer_norm2.bias', 'text_encoder.text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_encoder.text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_encoder.text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_encoder.text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_encoder.text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_encoder.text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_encoder.text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_encoder.text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_encoder.text_model.encoder.layers.8.layer_norm1.weight', 'text_encoder.text_model.encoder.layers.8.layer_norm1.bias', 'text_encoder.text_model.encoder.layers.8.mlp.fc1.weight', 'text_encoder.text_model.encoder.layers.8.mlp.fc1.bias', 'text_encoder.text_model.encoder.layers.8.mlp.fc2.weight', 'text_encoder.text_model.encoder.layers.8.mlp.fc2.bias', 'text_encoder.text_model.encoder.layers.8.layer_norm2.weight', 'text_encoder.text_model.encoder.layers.8.layer_norm2.bias', 'text_encoder.text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_encoder.text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_encoder.text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_encoder.text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_encoder.text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_encoder.text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_encoder.text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_encoder.text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_encoder.text_model.encoder.layers.9.layer_norm1.weight', 'text_encoder.text_model.encoder.layers.9.layer_norm1.bias', 'text_encoder.text_model.encoder.layers.9.mlp.fc1.weight', 'text_encoder.text_model.encoder.layers.9.mlp.fc1.bias', 'text_encoder.text_model.encoder.layers.9.mlp.fc2.weight', 'text_encoder.text_model.encoder.layers.9.mlp.fc2.bias', 'text_encoder.text_model.encoder.layers.9.layer_norm2.weight', 'text_encoder.text_model.encoder.layers.9.layer_norm2.bias', 'text_encoder.text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_encoder.text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_encoder.text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_encoder.text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_encoder.text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_encoder.text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_encoder.text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_encoder.text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_encoder.text_model.encoder.layers.10.layer_norm1.weight', 'text_encoder.text_model.encoder.layers.10.layer_norm1.bias', 'text_encoder.text_model.encoder.layers.10.mlp.fc1.weight', 'text_encoder.text_model.encoder.layers.10.mlp.fc1.bias', 'text_encoder.text_model.encoder.layers.10.mlp.fc2.weight', 'text_encoder.text_model.encoder.layers.10.mlp.fc2.bias', 'text_encoder.text_model.encoder.layers.10.layer_norm2.weight', 'text_encoder.text_model.encoder.layers.10.layer_norm2.bias', 'text_encoder.text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_encoder.text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_encoder.text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_encoder.text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_encoder.text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_encoder.text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_encoder.text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_encoder.text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_encoder.text_model.encoder.layers.11.layer_norm1.weight', 'text_encoder.text_model.encoder.layers.11.layer_norm1.bias', 'text_encoder.text_model.encoder.layers.11.mlp.fc1.weight', 'text_encoder.text_model.encoder.layers.11.mlp.fc1.bias', 'text_encoder.text_model.encoder.layers.11.mlp.fc2.weight', 'text_encoder.text_model.encoder.layers.11.mlp.fc2.bias', 'text_encoder.text_model.encoder.layers.11.layer_norm2.weight', 'text_encoder.text_model.encoder.layers.11.layer_norm2.bias', 'text_encoder.text_model.final_layer_norm.weight', 'text_encoder.text_model.final_layer_norm.bias', 'region_proj.weight', 'region_proj.bias', 'text_proj.weight', 'text_proj.bias']\n",
      "  Unexpected keys: ['model.region_encoder.backbone.cls_token', 'model.region_encoder.backbone.pos_embed', 'model.region_encoder.backbone.patch_embed.proj.weight', 'model.region_encoder.backbone.patch_embed.proj.bias', 'model.region_encoder.backbone.blocks.0.norm1.weight', 'model.region_encoder.backbone.blocks.0.norm1.bias', 'model.region_encoder.backbone.blocks.0.attn.qkv.weight', 'model.region_encoder.backbone.blocks.0.attn.qkv.bias', 'model.region_encoder.backbone.blocks.0.attn.proj.weight', 'model.region_encoder.backbone.blocks.0.attn.proj.bias', 'model.region_encoder.backbone.blocks.0.norm2.weight', 'model.region_encoder.backbone.blocks.0.norm2.bias', 'model.region_encoder.backbone.blocks.0.mlp.fc1.weight', 'model.region_encoder.backbone.blocks.0.mlp.fc1.bias', 'model.region_encoder.backbone.blocks.0.mlp.fc2.weight', 'model.region_encoder.backbone.blocks.0.mlp.fc2.bias', 'model.region_encoder.backbone.blocks.1.norm1.weight', 'model.region_encoder.backbone.blocks.1.norm1.bias', 'model.region_encoder.backbone.blocks.1.attn.qkv.weight', 'model.region_encoder.backbone.blocks.1.attn.qkv.bias', 'model.region_encoder.backbone.blocks.1.attn.proj.weight', 'model.region_encoder.backbone.blocks.1.attn.proj.bias', 'model.region_encoder.backbone.blocks.1.norm2.weight', 'model.region_encoder.backbone.blocks.1.norm2.bias', 'model.region_encoder.backbone.blocks.1.mlp.fc1.weight', 'model.region_encoder.backbone.blocks.1.mlp.fc1.bias', 'model.region_encoder.backbone.blocks.1.mlp.fc2.weight', 'model.region_encoder.backbone.blocks.1.mlp.fc2.bias', 'model.region_encoder.backbone.blocks.2.norm1.weight', 'model.region_encoder.backbone.blocks.2.norm1.bias', 'model.region_encoder.backbone.blocks.2.attn.qkv.weight', 'model.region_encoder.backbone.blocks.2.attn.qkv.bias', 'model.region_encoder.backbone.blocks.2.attn.proj.weight', 'model.region_encoder.backbone.blocks.2.attn.proj.bias', 'model.region_encoder.backbone.blocks.2.norm2.weight', 'model.region_encoder.backbone.blocks.2.norm2.bias', 'model.region_encoder.backbone.blocks.2.mlp.fc1.weight', 'model.region_encoder.backbone.blocks.2.mlp.fc1.bias', 'model.region_encoder.backbone.blocks.2.mlp.fc2.weight', 'model.region_encoder.backbone.blocks.2.mlp.fc2.bias', 'model.region_encoder.backbone.blocks.3.norm1.weight', 'model.region_encoder.backbone.blocks.3.norm1.bias', 'model.region_encoder.backbone.blocks.3.attn.qkv.weight', 'model.region_encoder.backbone.blocks.3.attn.qkv.bias', 'model.region_encoder.backbone.blocks.3.attn.proj.weight', 'model.region_encoder.backbone.blocks.3.attn.proj.bias', 'model.region_encoder.backbone.blocks.3.norm2.weight', 'model.region_encoder.backbone.blocks.3.norm2.bias', 'model.region_encoder.backbone.blocks.3.mlp.fc1.weight', 'model.region_encoder.backbone.blocks.3.mlp.fc1.bias', 'model.region_encoder.backbone.blocks.3.mlp.fc2.weight', 'model.region_encoder.backbone.blocks.3.mlp.fc2.bias', 'model.region_encoder.backbone.blocks.4.norm1.weight', 'model.region_encoder.backbone.blocks.4.norm1.bias', 'model.region_encoder.backbone.blocks.4.attn.qkv.weight', 'model.region_encoder.backbone.blocks.4.attn.qkv.bias', 'model.region_encoder.backbone.blocks.4.attn.proj.weight', 'model.region_encoder.backbone.blocks.4.attn.proj.bias', 'model.region_encoder.backbone.blocks.4.norm2.weight', 'model.region_encoder.backbone.blocks.4.norm2.bias', 'model.region_encoder.backbone.blocks.4.mlp.fc1.weight', 'model.region_encoder.backbone.blocks.4.mlp.fc1.bias', 'model.region_encoder.backbone.blocks.4.mlp.fc2.weight', 'model.region_encoder.backbone.blocks.4.mlp.fc2.bias', 'model.region_encoder.backbone.blocks.5.norm1.weight', 'model.region_encoder.backbone.blocks.5.norm1.bias', 'model.region_encoder.backbone.blocks.5.attn.qkv.weight', 'model.region_encoder.backbone.blocks.5.attn.qkv.bias', 'model.region_encoder.backbone.blocks.5.attn.proj.weight', 'model.region_encoder.backbone.blocks.5.attn.proj.bias', 'model.region_encoder.backbone.blocks.5.norm2.weight', 'model.region_encoder.backbone.blocks.5.norm2.bias', 'model.region_encoder.backbone.blocks.5.mlp.fc1.weight', 'model.region_encoder.backbone.blocks.5.mlp.fc1.bias', 'model.region_encoder.backbone.blocks.5.mlp.fc2.weight', 'model.region_encoder.backbone.blocks.5.mlp.fc2.bias', 'model.region_encoder.backbone.blocks.6.norm1.weight', 'model.region_encoder.backbone.blocks.6.norm1.bias', 'model.region_encoder.backbone.blocks.6.attn.qkv.weight', 'model.region_encoder.backbone.blocks.6.attn.qkv.bias', 'model.region_encoder.backbone.blocks.6.attn.proj.weight', 'model.region_encoder.backbone.blocks.6.attn.proj.bias', 'model.region_encoder.backbone.blocks.6.norm2.weight', 'model.region_encoder.backbone.blocks.6.norm2.bias', 'model.region_encoder.backbone.blocks.6.mlp.fc1.weight', 'model.region_encoder.backbone.blocks.6.mlp.fc1.bias', 'model.region_encoder.backbone.blocks.6.mlp.fc2.weight', 'model.region_encoder.backbone.blocks.6.mlp.fc2.bias', 'model.region_encoder.backbone.blocks.7.norm1.weight', 'model.region_encoder.backbone.blocks.7.norm1.bias', 'model.region_encoder.backbone.blocks.7.attn.qkv.weight', 'model.region_encoder.backbone.blocks.7.attn.qkv.bias', 'model.region_encoder.backbone.blocks.7.attn.proj.weight', 'model.region_encoder.backbone.blocks.7.attn.proj.bias', 'model.region_encoder.backbone.blocks.7.norm2.weight', 'model.region_encoder.backbone.blocks.7.norm2.bias', 'model.region_encoder.backbone.blocks.7.mlp.fc1.weight', 'model.region_encoder.backbone.blocks.7.mlp.fc1.bias', 'model.region_encoder.backbone.blocks.7.mlp.fc2.weight', 'model.region_encoder.backbone.blocks.7.mlp.fc2.bias', 'model.region_encoder.backbone.blocks.8.norm1.weight', 'model.region_encoder.backbone.blocks.8.norm1.bias', 'model.region_encoder.backbone.blocks.8.attn.qkv.weight', 'model.region_encoder.backbone.blocks.8.attn.qkv.bias', 'model.region_encoder.backbone.blocks.8.attn.proj.weight', 'model.region_encoder.backbone.blocks.8.attn.proj.bias', 'model.region_encoder.backbone.blocks.8.norm2.weight', 'model.region_encoder.backbone.blocks.8.norm2.bias', 'model.region_encoder.backbone.blocks.8.mlp.fc1.weight', 'model.region_encoder.backbone.blocks.8.mlp.fc1.bias', 'model.region_encoder.backbone.blocks.8.mlp.fc2.weight', 'model.region_encoder.backbone.blocks.8.mlp.fc2.bias', 'model.region_encoder.backbone.blocks.9.norm1.weight', 'model.region_encoder.backbone.blocks.9.norm1.bias', 'model.region_encoder.backbone.blocks.9.attn.qkv.weight', 'model.region_encoder.backbone.blocks.9.attn.qkv.bias', 'model.region_encoder.backbone.blocks.9.attn.proj.weight', 'model.region_encoder.backbone.blocks.9.attn.proj.bias', 'model.region_encoder.backbone.blocks.9.norm2.weight', 'model.region_encoder.backbone.blocks.9.norm2.bias', 'model.region_encoder.backbone.blocks.9.mlp.fc1.weight', 'model.region_encoder.backbone.blocks.9.mlp.fc1.bias', 'model.region_encoder.backbone.blocks.9.mlp.fc2.weight', 'model.region_encoder.backbone.blocks.9.mlp.fc2.bias', 'model.region_encoder.backbone.blocks.10.norm1.weight', 'model.region_encoder.backbone.blocks.10.norm1.bias', 'model.region_encoder.backbone.blocks.10.attn.qkv.weight', 'model.region_encoder.backbone.blocks.10.attn.qkv.bias', 'model.region_encoder.backbone.blocks.10.attn.proj.weight', 'model.region_encoder.backbone.blocks.10.attn.proj.bias', 'model.region_encoder.backbone.blocks.10.norm2.weight', 'model.region_encoder.backbone.blocks.10.norm2.bias', 'model.region_encoder.backbone.blocks.10.mlp.fc1.weight', 'model.region_encoder.backbone.blocks.10.mlp.fc1.bias', 'model.region_encoder.backbone.blocks.10.mlp.fc2.weight', 'model.region_encoder.backbone.blocks.10.mlp.fc2.bias', 'model.region_encoder.backbone.blocks.11.norm1.weight', 'model.region_encoder.backbone.blocks.11.norm1.bias', 'model.region_encoder.backbone.blocks.11.attn.qkv.weight', 'model.region_encoder.backbone.blocks.11.attn.qkv.bias', 'model.region_encoder.backbone.blocks.11.attn.proj.weight', 'model.region_encoder.backbone.blocks.11.attn.proj.bias', 'model.region_encoder.backbone.blocks.11.norm2.weight', 'model.region_encoder.backbone.blocks.11.norm2.bias', 'model.region_encoder.backbone.blocks.11.mlp.fc1.weight', 'model.region_encoder.backbone.blocks.11.mlp.fc1.bias', 'model.region_encoder.backbone.blocks.11.mlp.fc2.weight', 'model.region_encoder.backbone.blocks.11.mlp.fc2.bias', 'model.region_encoder.backbone.norm.weight', 'model.region_encoder.backbone.norm.bias', 'model.region_encoder.norm1.weight', 'model.region_encoder.norm1.bias', 'model.region_encoder.norm2.weight', 'model.region_encoder.norm2.bias', 'model.region_encoder.attn.qkv.weight', 'model.region_encoder.attn.proj.weight', 'model.region_encoder.attn.proj.bias', 'model.region_encoder.moe.experts.0.0.weight', 'model.region_encoder.moe.experts.0.0.bias', 'model.region_encoder.moe.experts.0.3.weight', 'model.region_encoder.moe.experts.0.3.bias', 'model.region_encoder.moe.experts.1.0.weight', 'model.region_encoder.moe.experts.1.0.bias', 'model.region_encoder.moe.experts.1.3.weight', 'model.region_encoder.moe.experts.1.3.bias', 'model.region_encoder.moe.experts.2.0.weight', 'model.region_encoder.moe.experts.2.0.bias', 'model.region_encoder.moe.experts.2.3.weight', 'model.region_encoder.moe.experts.2.3.bias', 'model.region_encoder.moe.experts.3.0.weight', 'model.region_encoder.moe.experts.3.0.bias', 'model.region_encoder.moe.experts.3.3.weight', 'model.region_encoder.moe.experts.3.3.bias', 'model.region_encoder.moe.router.weight', 'model.region_encoder.moe.router.bias', 'model.text_encoder.text_model.embeddings.token_embedding.weight', 'model.text_encoder.text_model.embeddings.position_embedding.weight', 'model.text_encoder.text_model.encoder.layers.0.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.0.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.0.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.0.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.0.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.0.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.0.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.0.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.0.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.0.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.0.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.0.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.0.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.0.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.0.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.0.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.1.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.1.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.1.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.1.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.1.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.1.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.1.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.1.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.1.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.1.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.1.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.1.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.1.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.1.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.1.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.1.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.2.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.2.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.2.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.2.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.2.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.2.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.2.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.2.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.2.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.2.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.2.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.2.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.2.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.2.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.2.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.2.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.3.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.3.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.3.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.3.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.3.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.3.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.3.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.3.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.3.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.3.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.3.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.3.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.3.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.3.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.3.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.3.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.4.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.4.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.4.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.4.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.4.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.4.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.4.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.4.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.4.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.4.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.4.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.4.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.4.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.4.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.4.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.4.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.5.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.5.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.5.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.5.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.5.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.5.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.5.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.5.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.5.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.5.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.5.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.5.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.5.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.5.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.5.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.5.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.6.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.6.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.6.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.6.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.6.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.6.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.6.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.6.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.6.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.6.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.6.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.6.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.6.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.6.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.6.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.6.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.7.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.7.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.7.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.7.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.7.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.7.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.7.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.7.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.7.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.7.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.7.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.7.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.7.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.7.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.7.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.7.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.8.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.8.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.8.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.8.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.8.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.8.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.8.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.8.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.8.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.8.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.8.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.8.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.8.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.8.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.8.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.8.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.9.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.9.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.9.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.9.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.9.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.9.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.9.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.9.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.9.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.9.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.9.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.9.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.9.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.9.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.9.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.9.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.10.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.10.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.10.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.10.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.10.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.10.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.10.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.10.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.10.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.10.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.10.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.10.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.10.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.10.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.10.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.10.layer_norm2.bias', 'model.text_encoder.text_model.encoder.layers.11.self_attn.k_proj.weight', 'model.text_encoder.text_model.encoder.layers.11.self_attn.k_proj.bias', 'model.text_encoder.text_model.encoder.layers.11.self_attn.v_proj.weight', 'model.text_encoder.text_model.encoder.layers.11.self_attn.v_proj.bias', 'model.text_encoder.text_model.encoder.layers.11.self_attn.q_proj.weight', 'model.text_encoder.text_model.encoder.layers.11.self_attn.q_proj.bias', 'model.text_encoder.text_model.encoder.layers.11.self_attn.out_proj.weight', 'model.text_encoder.text_model.encoder.layers.11.self_attn.out_proj.bias', 'model.text_encoder.text_model.encoder.layers.11.layer_norm1.weight', 'model.text_encoder.text_model.encoder.layers.11.layer_norm1.bias', 'model.text_encoder.text_model.encoder.layers.11.mlp.fc1.weight', 'model.text_encoder.text_model.encoder.layers.11.mlp.fc1.bias', 'model.text_encoder.text_model.encoder.layers.11.mlp.fc2.weight', 'model.text_encoder.text_model.encoder.layers.11.mlp.fc2.bias', 'model.text_encoder.text_model.encoder.layers.11.layer_norm2.weight', 'model.text_encoder.text_model.encoder.layers.11.layer_norm2.bias', 'model.text_encoder.text_model.final_layer_norm.weight', 'model.text_encoder.text_model.final_layer_norm.bias', 'model.region_proj.weight', 'model.region_proj.bias', 'model.text_proj.weight', 'model.text_proj.bias']\n",
      "模型输出： {'region_features': tensor([[[ 0.0029, -0.0302,  0.0042,  ..., -0.0076, -0.0304,  0.0544],\n",
      "         [-0.0029, -0.0324,  0.0104,  ..., -0.0111, -0.0305,  0.0538],\n",
      "         [ 0.0054, -0.0252,  0.0034,  ..., -0.0154, -0.0310,  0.0646],\n",
      "         [ 0.0030, -0.0261,  0.0051,  ...,  0.0028, -0.0312,  0.0606]],\n",
      "\n",
      "        [[ 0.0084, -0.0285,  0.0061,  ..., -0.0107, -0.0297,  0.0568],\n",
      "         [-0.0055, -0.0308,  0.0049,  ...,  0.0037, -0.0409,  0.0568],\n",
      "         [-0.0085, -0.0343,  0.0084,  ..., -0.0010, -0.0289,  0.0570],\n",
      "         [ 0.0035, -0.0325,  0.0032,  ..., -0.0064, -0.0291,  0.0486]]],\n",
      "       device='cuda:0'), 'text_features': tensor([[[-0.0017,  0.0720,  0.0706,  ...,  0.0827, -0.0476, -0.0614],\n",
      "         [-0.0081,  0.0357,  0.0583,  ...,  0.1018, -0.0165, -0.0395],\n",
      "         [ 0.0197,  0.0482,  0.0585,  ...,  0.1099, -0.0912, -0.0330],\n",
      "         [-0.0044,  0.0382,  0.0702,  ...,  0.0916, -0.0660, -0.0505]],\n",
      "\n",
      "        [[ 0.0131, -0.0072,  0.0768,  ...,  0.0844, -0.0595, -0.0488],\n",
      "         [-0.0181,  0.0514,  0.0249,  ...,  0.1528, -0.0554, -0.0500],\n",
      "         [ 0.0116,  0.0570,  0.0561,  ...,  0.1123, -0.0556, -0.0391],\n",
      "         [ 0.0251,  0.0544,  0.0638,  ...,  0.0981, -0.1267, -0.0119]]],\n",
      "       device='cuda:0'), 'global_region_features': tensor([[ 0.0106, -0.0327,  0.0125,  ...,  0.0102, -0.0265,  0.0689],\n",
      "        [-0.0053, -0.0306,  0.0032,  ..., -0.0038, -0.0391,  0.0505]],\n",
      "       device='cuda:0'), 'logits_per_text': tensor([[[ 0.3144,  0.2479,  0.1144,  0.3765],\n",
      "         [ 0.3723,  0.3640,  0.2398,  0.3881],\n",
      "         [ 0.3238,  0.3066,  0.2130,  0.3292],\n",
      "         [ 0.2176,  0.1867,  0.0119,  0.1773]],\n",
      "\n",
      "        [[ 0.2343,  0.3231,  0.3047,  0.2621],\n",
      "         [ 0.2499,  0.5084,  0.4454,  0.3257],\n",
      "         [-0.1528, -0.0145, -0.0366, -0.1026],\n",
      "         [ 0.5615,  0.7271,  0.6845,  0.6111]]], device='cuda:0'), 'logits_per_region': tensor([[[ 0.3144,  0.3723,  0.3238,  0.2176],\n",
      "         [ 0.2479,  0.3640,  0.3066,  0.1867],\n",
      "         [ 0.1144,  0.2398,  0.2130,  0.0119],\n",
      "         [ 0.3765,  0.3881,  0.3292,  0.1773]],\n",
      "\n",
      "        [[ 0.2343,  0.2499, -0.1528,  0.5615],\n",
      "         [ 0.3231,  0.5084, -0.0145,  0.7271],\n",
      "         [ 0.3047,  0.4454, -0.0366,  0.6845],\n",
      "         [ 0.2621,  0.3257, -0.1026,  0.6111]]], device='cuda:0'), 'loss_text': tensor(1.3754, device='cuda:0'), 'loss_region': tensor(1.3831, device='cuda:0'), 'total_loss': tensor(1.3831, device='cuda:0')}\n",
      "模型 EMOE_RefCOCO-10of100.pt 已准备好进行测试。\n"
     ]
    }
   ],
   "source": [
    "# 测试EMOE_RefCOCO-10of100.pt模型的能力，允许加载state_dict时忽略key不匹配\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.models.like.emoe.refcoco_model import EMOE_RefCOCO\n",
    "\n",
    "model_path = \"EMOE_RefCOCO-10of100.pt\"\n",
    "\n",
    "# device 设置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 创建模型实例\n",
    "model = EMOE_RefCOCO()\n",
    "\n",
    "# 加载模型参数，同时允许跳过部分缺失或多余的key\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "# 检查键\n",
    "result = model.load_state_dict(state_dict, strict=False)\n",
    "print(\"State dict loading result:\")\n",
    "print(\"  Missing keys:\", result.missing_keys)\n",
    "print(\"  Unexpected keys:\", result.unexpected_keys)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 构造一个refcoco-like的batch输入，维度符合模型期望\n",
    "B = 2  # batch size\n",
    "N_OBJ = 5  # 每张图片的区域数（含背景）\n",
    "C, H, W = 3, 224, 224  # 图像区域通道和尺寸\n",
    "\n",
    "max_regions = N_OBJ - 1  # 不含背景的region数\n",
    "max_seq_len = 16  # 一个region的文本描述最大长度\n",
    "\n",
    "regions = torch.randn(B, N_OBJ, C, H, W).to(device)\n",
    "texts = torch.randint(0, 49408, (B, max_regions, max_seq_len)).to(\n",
    "    device\n",
    ")  # 假设clip tokenizer词表大小49408\n",
    "text_attn_mask = torch.ones(B, max_regions, max_seq_len, dtype=torch.long).to(device)\n",
    "\n",
    "# 推理与输出\n",
    "with torch.no_grad():\n",
    "    output = model(regions=regions, texts=texts, text_attn_mask=text_attn_mask)\n",
    "    print(\"模型输出：\", output)\n",
    "\n",
    "print(\"模型 {} 已准备好进行测试。\".format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c8c139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本0的余弦相似度矩阵:\n",
      "tensor([[0.0220, 0.0174, 0.0080, 0.0264],\n",
      "        [0.0261, 0.0255, 0.0168, 0.0272],\n",
      "        [0.0227, 0.0215, 0.0149, 0.0230],\n",
      "        [0.0152, 0.0131, 0.0008, 0.0124]], device='cuda:0')\n",
      "手动计算的logits与模型输出最大差异: 0.000000e+00\n",
      "每个文本-区域对的对角相似度:\n",
      "tensor([[ 0.0220,  0.0255,  0.0149,  0.0124],\n",
      "        [ 0.0164,  0.0356, -0.0026,  0.0428]], device='cuda:0')\n",
      "text_features范数范围: [1.0000, 1.0000]\n",
      "region_features范数范围: [1.0000, 1.0000]\n"
     ]
    }
   ],
   "source": [
    "# 测试 text_features 与 region_features 的关系\n",
    "text_features = output[\"text_features\"]\n",
    "region_features = output[\"region_features\"]\n",
    "\n",
    "# 计算归一化后的余弦相似度矩阵\n",
    "similarity = torch.bmm(text_features, region_features.transpose(1, 2))\n",
    "print(\"样本0的余弦相似度矩阵:\")\n",
    "print(similarity[0])\n",
    "\n",
    "# 对比模型内部计算的logits\n",
    "logits_manual = similarity / model.temperature\n",
    "diff = (logits_manual - output[\"logits_per_text\"]).abs().max().item()\n",
    "print(f\"手动计算的logits与模型输出最大差异: {diff:.6e}\")\n",
    "\n",
    "# 查看每个文本与对应区域的对角相似度\n",
    "diag_similarity = similarity.diagonal(dim1=-2, dim2=-1)\n",
    "print(\"每个文本-区域对的对角相似度:\")\n",
    "print(diag_similarity)\n",
    "\n",
    "# 检查特征是否经过L2归一化\n",
    "text_norms = text_features.norm(dim=-1)\n",
    "region_norms = region_features.norm(dim=-1)\n",
    "print(\n",
    "    f\"text_features范数范围: [{text_norms.min().item():.4f}, {text_norms.max().item():.4f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"region_features范数范围: [{region_norms.min().item():.4f}, {region_norms.max().item():.4f}]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3d20d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从COCO样本 0 获得 4 个区域\n"
     ]
    }
   ],
   "source": [
    "# 使用COCO2017真实数据测试\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from src.dataset.alignment.coco2017_alignment_dataset import COCO2017RegionAlignment\n",
    "\n",
    "coco_root = Path(\"/home/rczx/workspace/sxy/lab/NeuroTrain/data/coco2017\")\n",
    "coco_dataset = COCO2017RegionAlignment(root_dir=coco_root, split=\"val\")\n",
    "\n",
    "sample_idx = 0\n",
    "sample = coco_dataset[sample_idx]\n",
    "sample_row = coco_dataset.samples.iloc[sample_idx]\n",
    "\n",
    "regions_real = sample[\"inputs\"].unsqueeze(0).to(device)\n",
    "texts_real = sample[\"text_ids\"].unsqueeze(0).to(device)\n",
    "attn_real = sample[\"text_attn_mask\"]\n",
    "if attn_real is not None:\n",
    "    attn_real = attn_real.unsqueeze(0).to(device)\n",
    "\n",
    "region_bboxes = sample_row[\"bboxes\"]\n",
    "region_labels = sample_row[\"labels\"]\n",
    "region_texts = [coco_dataset.OBJECT_TEXT.format(label=label) for label in region_labels]\n",
    "\n",
    "image_path = coco_dataset.img_dir / sample_row[\"file_name\"]\n",
    "base_image = Image.open(image_path).convert(\"RGB\")\n",
    "region_crops = []\n",
    "for box in region_bboxes:\n",
    "    x, y, w, h = box\n",
    "    region_crops.append(base_image.crop((x, y, x + w, y + h)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_real = model(\n",
    "        regions=regions_real, texts=texts_real, text_attn_mask=attn_real\n",
    "    )\n",
    "\n",
    "print(f\"从COCO样本 {sample_idx} 获得 {len(region_labels)} 个区域\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59bbd32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算的logits与模型输出最大差异: 0.000000e+00\n",
      "真实数据每个文本-区域对的对角相似度:\n",
      "tensor([[ 0.0509, -0.0407, -0.0031, -0.0757]], device='cuda:0')\n",
      "真实数据余弦相似度均值: -0.0172\n"
     ]
    }
   ],
   "source": [
    "# 计算真实数据上 text-region 余弦相似度\n",
    "text_features_real = output_real[\"text_features\"]\n",
    "region_features_real = output_real[\"region_features\"]\n",
    "\n",
    "similarity_real = torch.bmm(text_features_real, region_features_real.transpose(1, 2))\n",
    "logits_manual_real = similarity_real / model.temperature\n",
    "diff_real = (logits_manual_real - output_real[\"logits_per_text\"]).abs().max().item()\n",
    "\n",
    "diag_similarity_real = similarity_real.diagonal(dim1=-2, dim2=-1)\n",
    "\n",
    "print(f\"手动计算的logits与模型输出最大差异: {diff_real:.6e}\")\n",
    "print(\"真实数据每个文本-区域对的对角相似度:\")\n",
    "print(diag_similarity_real)\n",
    "print(f\"真实数据余弦相似度均值: {diag_similarity_real.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a345f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: padding, truncation, text.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify input_ids",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m clip_random\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 24\u001b[0m     clip_outputs_random \u001b[38;5;241m=\u001b[39m \u001b[43mclip_random\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclip_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m text_embeds_random \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(clip_outputs_random\u001b[38;5;241m.\u001b[39mtext_embeds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m image_embeds_random \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(clip_outputs_random\u001b[38;5;241m.\u001b[39mimage_embeds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:989\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    978\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    979\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    980\u001b[0m )\n\u001b[1;32m    982\u001b[0m vision_outputs: BaseModelOutputWithPooling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model(\n\u001b[1;32m    983\u001b[0m     pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[1;32m    984\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    985\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    986\u001b[0m     interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding,\n\u001b[1;32m    987\u001b[0m )\n\u001b[0;32m--> 989\u001b[0m text_outputs: BaseModelOutputWithPooling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m vision_outputs\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[1;32m    998\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_projection(image_embeds)\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ntrain/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:598\u001b[0m, in \u001b[0;36mCLIPTextTransformer.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    593\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    594\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    595\u001b[0m )\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    600\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    601\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify input_ids"
     ]
    }
   ],
   "source": [
    "# 与未训练的 CLIP 模型对比\n",
    "from transformers import CLIPModel, CLIPConfig, CLIPImageProcessor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "clip_ckpt = \"openai/clip-vit-base-patch32\"\n",
    "clip_processor = CLIPImageProcessor.from_pretrained(\n",
    "    clip_ckpt, cache_dir=model.cache_dir\n",
    ")\n",
    "\n",
    "clip_inputs = clip_processor(\n",
    "    text=region_texts,\n",
    "    images=region_crops,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "clip_inputs = {k: v.to(device) for k, v in clip_inputs.items()}\n",
    "\n",
    "clip_config = CLIPConfig.from_pretrained(clip_ckpt)\n",
    "clip_random = CLIPModel(clip_config).to(device)\n",
    "clip_random.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    clip_outputs_random = clip_random(**clip_inputs)\n",
    "\n",
    "text_embeds_random = F.normalize(clip_outputs_random.text_embeds, dim=-1)\n",
    "image_embeds_random = F.normalize(clip_outputs_random.image_embeds, dim=-1)\n",
    "similarity_clip_random = text_embeds_random @ image_embeds_random.T\n",
    "diag_clip_random = similarity_clip_random.diagonal()\n",
    "\n",
    "print(\"未训练 CLIP 对角余弦相似度:\")\n",
    "print(diag_clip_random)\n",
    "print(f\"未训练 CLIP 平均对角相似度: {diag_clip_random.mean().item():.4f}\")\n",
    "\n",
    "# 方便对比，再输出 EMOE 结果\n",
    "print(\"EMOE 对角余弦相似度:\")\n",
    "print(diag_similarity_real.squeeze(0))\n",
    "print(f\"EMOE 平均对角相似度: {diag_similarity_real.mean().item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
