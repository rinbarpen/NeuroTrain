# Qwen 模型训练示例配置
# 支持 Qwen-Med 和 Qwen-VL-Med 的 SFT 和 DPO 训练

task_name = "qwen_med_sft_dpo"
seed = 42
base_output_dir = "runs/qwen"
use_wandb = false
wandb_project = "neurotrain-qwen"

# ==================== 模型配置 ====================
[model]
# Qwen-Med 纯文本模型
model_name_or_path = "Echelon-AI/Med-Qwen2-7B"
# 或使用 Qwen-VL-Med 视觉语言模型
# model_name_or_path = "AdaptLLM/biomed-Qwen2-VL-2B-Instruct"

model_type = "llm"  # llm 或 vlm (Qwen-VL-Med 使用 vlm)
device = "auto"
dtype = "bfloat16"  # float16, bfloat16, float32
trust_remote_code = true

# LoRA 配置
use_lora = true
[model.lora_config]
r = 16
lora_alpha = 32
# Qwen2 模型的 target_modules
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
lora_dropout = 0.05
bias = "none"
task_type = "CAUSAL_LM"

# 量化配置(可选，节省显存)
load_in_8bit = false
load_in_4bit = true
[model.bnb_config]
use_double_quant = true
quant_type = "nf4"

# 其他模型参数
gradient_checkpointing = true
use_flash_attention = false

# ==================== 数据集配置 ====================

# Stage 1: SFT 数据集
[datasets.sft_stage]
# 支持多种格式：
# 1. JSONL 格式: dataset_path = "data/qwen_sft_dataset.jsonl"
# 2. TXT 格式(自定义分隔符): dataset_path = "data/qwen_sft_dataset.txt"
#    需要配置 txt_delimiter 和 txt_field_names
dataset_path = "data/qwen_sft_dataset.jsonl"  # 或使用 HF: dataset_name = "your_dataset"
dataset_split = "train"
max_length = 2048
prompt_field = "instruction"  # 或 "prompt"
response_field = "output"    # 或 "response"
text_field = "text"
preprocessing_num_workers = 4
num_samples = 10000  # 限制样本数量(可选)

# TXT 文件格式配置(可选，仅当使用txt文件时)
# txt_delimiter = "|"  # 分隔符，如 "|", "\t", ","
# txt_field_names = ["instruction", "output"]  # 字段名称列表，顺序需与文件中的列对应

# Stage 2: DPO 数据集
[datasets.dpo_stage]
# 支持多种格式：
# 1. JSONL 格式: dataset_path = "data/qwen_dpo_dataset.jsonl"
# 2. TXT 格式(自定义分隔符): dataset_path = "data/qwen_dpo_dataset.txt"
#    需要配置 txt_delimiter 和 txt_field_names
dataset_path = "data/qwen_dpo_dataset.jsonl"  # 或使用 HF: dataset_name = "your_dataset"
dataset_split = "train"
max_length = 2048
prompt_field = "prompt"
chosen_field = "chosen"
rejected_field = "rejected"
preprocessing_num_workers = 4

# TXT 文件格式配置(可选，仅当使用txt文件时)
# txt_delimiter = "|"  # 分隔符，如 "|", "\t", ","
# txt_field_names = ["prompt", "chosen", "rejected"]  # 字段名称列表，顺序需与文件中的列对应

# ==================== 训练阶段配置 ====================

# Stage 1: SFT (监督微调)
[[stages]]
stage_type = "sft"
stage_name = "sft_stage"
load_from = ""  # 留空则从 model.model_name_or_path 加载
output_dir = ""  # 留空则自动生成

# 训练参数
num_train_epochs = 3
per_device_train_batch_size = 4
per_device_eval_batch_size = 4
gradient_accumulation_steps = 4
learning_rate = 2e-5
warmup_steps = 100
logging_steps = 10
save_steps = 500
eval_steps = 500
max_steps = -1

# 优化器
optimizer = "adamw_torch"
weight_decay = 0.01
max_grad_norm = 1.0
lr_scheduler_type = "cosine"

# 混合精度
fp16 = false
bf16 = true

# 评估
do_eval = true
eval_strategy = "steps"

# 保存
save_strategy = "steps"
save_total_limit = 3

# DeepSpeed(可选，多卡训练时使用)
# deepspeed = "configs/deepspeed/ds_config_zero2.json"

# 阶段特定参数
[stages.stage_kwargs]
packing = false  # 是否使用 packing 优化


# Stage 2: DPO (直接偏好优化)
[[stages]]
stage_type = "dpo"
stage_name = "dpo_stage"
load_from = ""  # 留空则从上一阶段(sft_stage)加载
output_dir = ""  # 留空则自动生成

# 训练参数
num_train_epochs = 2
per_device_train_batch_size = 2
per_device_eval_batch_size = 2
gradient_accumulation_steps = 8
learning_rate = 5e-6
warmup_steps = 50
logging_steps = 10
save_steps = 500
eval_steps = 500

optimizer = "adamw_torch"
weight_decay = 0.01
max_grad_norm = 1.0
lr_scheduler_type = "cosine"

fp16 = false
bf16 = true

do_eval = true
eval_strategy = "steps"
save_strategy = "steps"
save_total_limit = 2

# DPO 专用参数
dpo_beta = 0.1  # DPO 温度参数，控制 KL 散度惩罚强度
reference_model_path = ""  # 留空则使用初始模型作为 reference

